
The following have been reloaded with a version change:
  1) FFTW/3.3.8-gompic-2019b => FFTW/3.3.8-gompi-2019b
  2) OpenMPI/3.1.4-gcccuda-2019b => OpenMPI/3.1.4-GCC-8.3.0
  3) ScaLAPACK/2.0.2-gompic-2019b => ScaLAPACK/2.0.2-gompi-2019b
  4) SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4 => SciPy-bundle/2019.10-foss-2019b-Python-3.7.4

2020-03-10 12:03:51.631619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-10 12:04:05.618725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-03-10 12:04:05.625336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:05.625855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GRID V100D-32Q computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.87GiB deviceMemoryBandwidth: 836.37GiB/s
2020-03-10 12:04:05.625921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-10 12:04:05.630457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-10 12:04:05.633873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-10 12:04:05.635286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-10 12:04:05.638969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-10 12:04:05.641232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-10 12:04:05.647316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-10 12:04:05.647496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:05.648083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:05.648484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-03-10 12:04:05.651214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:05.651649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GRID V100D-32Q computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.87GiB deviceMemoryBandwidth: 836.37GiB/s
2020-03-10 12:04:05.651694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-10 12:04:05.651729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-10 12:04:05.651750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-10 12:04:05.651784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-10 12:04:05.651805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-10 12:04:05.651825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-10 12:04:05.651846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-10 12:04:05.651945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:05.652411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:05.652803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-03-10 12:04:05.652855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-10 12:04:06.430474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-10 12:04:06.430557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-03-10 12:04:06.430573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-03-10 12:04:06.430862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:06.431503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:06.432024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-10 12:04:06.432457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28652 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-03-10 12:04:06.432826: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
2020-03-10 12:04:11.489665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-10 12:04:11.844871: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-10 12:04:16.616647: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.
2020-03-10 12:04:16.616752: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1259] Profiler found 1 GPUs
2020-03-10 12:04:16.619432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1
2020-03-10 12:04:16.719987: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_VIRTUALIZED_DEVICE_NOT_SUPPORTED
2020-03-10 12:04:16.720530: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_VIRTUALIZED_DEVICE_NOT_SUPPORTED
2020-03-10 12:04:16.922223: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER
2020-03-10 12:04:16.922317: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.
Random seed for replication: 841
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 32, num_classes = 5, epochs_1 = 120,
        epochts_2 = 300, test_img_idx = 1175,
        train_test_split = 0.8, to_shuffle = True, augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463],
        label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 1e-05,
        train_all_layers = True, weights_to_use = None,
        es_patience_1 = 40, es_patience_2 = 100, train_val_split = 0.9
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            
__________________________________________________________________________________________________
block1_conv1 (Conv2D)           (None, 256, 256, 64) 1792        input_1[0][0]                    
__________________________________________________________________________________________________
block1_conv2 (Conv2D)           (None, 256, 256, 64) 36928       block1_conv1[1][0]               
__________________________________________________________________________________________________
block1_pool (MaxPooling2D)      (None, 128, 128, 64) 0           block1_conv2[1][0]               
__________________________________________________________________________________________________
block2_conv1 (Conv2D)           (None, 128, 128, 128 73856       block1_pool[1][0]                
__________________________________________________________________________________________________
block2_conv2 (Conv2D)           (None, 128, 128, 128 147584      block2_conv1[1][0]               
__________________________________________________________________________________________________
block2_pool (MaxPooling2D)      (None, 64, 64, 128)  0           block2_conv2[1][0]               
__________________________________________________________________________________________________
block3_conv1 (Conv2D)           (None, 64, 64, 256)  295168      block2_pool[1][0]                
__________________________________________________________________________________________________
block3_conv2 (Conv2D)           (None, 64, 64, 256)  590080      block3_conv1[1][0]               
__________________________________________________________________________________________________
block3_conv3 (Conv2D)           (None, 64, 64, 256)  590080      block3_conv2[1][0]               
__________________________________________________________________________________________________
block3_pool (MaxPooling2D)      (None, 32, 32, 256)  0           block3_conv3[1][0]               
__________________________________________________________________________________________________
block4_conv1 (Conv2D)           (None, 32, 32, 512)  1180160     block3_pool[1][0]                
__________________________________________________________________________________________________
block4_conv2 (Conv2D)           (None, 32, 32, 512)  2359808     block4_conv1[1][0]               
__________________________________________________________________________________________________
block4_conv3 (Conv2D)           (None, 32, 32, 512)  2359808     block4_conv2[1][0]               
__________________________________________________________________________________________________
block4_pool (MaxPooling2D)      (None, 16, 16, 512)  0           block4_conv3[1][0]               
__________________________________________________________________________________________________
block5_conv1 (Conv2D)           (None, 16, 16, 512)  2359808     block4_pool[1][0]                
__________________________________________________________________________________________________
block5_conv2 (Conv2D)           (None, 16, 16, 512)  2359808     block5_conv1[1][0]               
__________________________________________________________________________________________________
block5_conv3 (Conv2D)           (None, 16, 16, 512)  2359808     block5_conv2[1][0]               
__________________________________________________________________________________________________
block5_pool (MaxPooling2D)      (None, 8, 8, 512)    0           block5_conv3[1][0]               
__________________________________________________________________________________________________
flatten (Flatten)               (None, 32768)        0           block5_pool[1][0]                
__________________________________________________________________________________________________
fc1 (Dense)                     (None, 4096)         134221824   flatten[0][0]                    
__________________________________________________________________________________________________
fc2 (Dense)                     (None, 4096)         16781312    fc1[0][0]                        
__________________________________________________________________________________________________
dense (Dense)                   (None, 5)            20485       fc2[0][0]                        
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 5)            20485       fc2[0][0]                        
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 10)           0           dense[0][0]                      
                                                                 dense_1[0][0]                    
==================================================================================================
Total params: 165,758,794
Trainable params: 165,758,794
Non-trainable params: 0
__________________________________________________________________________________________________
Start fitting
Train for 158 steps, validate for 18 steps
Epoch 1/120
158/158 - 41s - loss: 1.5823 - acc: 0.2392 - val_loss: 1.4705 - val_acc: 0.3232
Epoch 2/120
158/158 - 33s - loss: 1.4496 - acc: 0.3437 - val_loss: 1.4209 - val_acc: 0.3714
Epoch 3/120
158/158 - 33s - loss: 1.4149 - acc: 0.3592 - val_loss: 1.4063 - val_acc: 0.3679
Epoch 4/120
158/158 - 33s - loss: 1.3951 - acc: 0.3807 - val_loss: 1.3596 - val_acc: 0.4196
Epoch 5/120
158/158 - 33s - loss: 1.3696 - acc: 0.3916 - val_loss: 1.3939 - val_acc: 0.3982
Epoch 6/120
158/158 - 33s - loss: 1.3541 - acc: 0.3998 - val_loss: 1.3156 - val_acc: 0.4268
Epoch 7/120
158/158 - 33s - loss: 1.3121 - acc: 0.4280 - val_loss: 1.2748 - val_acc: 0.4393
Epoch 8/120
158/158 - 33s - loss: 1.2367 - acc: 0.4623 - val_loss: 1.2276 - val_acc: 0.4518
Epoch 9/120
158/158 - 33s - loss: 1.1750 - acc: 0.5057 - val_loss: 1.1746 - val_acc: 0.5161
Epoch 10/120
158/158 - 33s - loss: 1.1180 - acc: 0.5456 - val_loss: 1.1611 - val_acc: 0.5107
Epoch 11/120
158/158 - 33s - loss: 1.0347 - acc: 0.5806 - val_loss: 1.0737 - val_acc: 0.5554
Epoch 12/120
158/158 - 33s - loss: 0.9512 - acc: 0.6257 - val_loss: 0.9914 - val_acc: 0.5946
Epoch 13/120
158/158 - 33s - loss: 0.8437 - acc: 0.6684 - val_loss: 0.9628 - val_acc: 0.6089
Epoch 14/120
158/158 - 33s - loss: 0.7299 - acc: 0.7097 - val_loss: 1.0025 - val_acc: 0.5786
Epoch 15/120
158/158 - 33s - loss: 0.5894 - acc: 0.7546 - val_loss: 1.0115 - val_acc: 0.6179
Epoch 16/120
158/158 - 33s - loss: 0.5059 - acc: 0.7697 - val_loss: 0.9380 - val_acc: 0.6500
Epoch 17/120
158/158 - 33s - loss: 0.3948 - acc: 0.7955 - val_loss: 1.0385 - val_acc: 0.5893
Epoch 18/120
158/158 - 33s - loss: 0.3386 - acc: 0.7804 - val_loss: 0.9561 - val_acc: 0.5875
Epoch 19/120
158/158 - 33s - loss: 0.2571 - acc: 0.7920 - val_loss: 1.0449 - val_acc: 0.5643
Epoch 20/120
158/158 - 33s - loss: 0.2181 - acc: 0.7431 - val_loss: 1.1836 - val_acc: 0.5375
Epoch 21/120
158/158 - 33s - loss: 0.1860 - acc: 0.7826 - val_loss: 1.1339 - val_acc: 0.5411
Epoch 22/120
158/158 - 33s - loss: 0.1432 - acc: 0.7151 - val_loss: 1.3892 - val_acc: 0.4607
Epoch 23/120
158/158 - 33s - loss: 0.1215 - acc: 0.7067 - val_loss: 1.3475 - val_acc: 0.4679
Epoch 24/120
158/158 - 33s - loss: 0.1008 - acc: 0.6755 - val_loss: 1.2826 - val_acc: 0.4804
Epoch 25/120
158/158 - 33s - loss: 0.1431 - acc: 0.6718 - val_loss: 1.6299 - val_acc: 0.4339
Epoch 26/120
158/158 - 33s - loss: 0.1544 - acc: 0.7155 - val_loss: 1.2714 - val_acc: 0.5143
Epoch 27/120
158/158 - 33s - loss: 0.0685 - acc: 0.6747 - val_loss: 1.4107 - val_acc: 0.4071
Epoch 28/120
158/158 - 33s - loss: 0.0694 - acc: 0.6273 - val_loss: 1.3263 - val_acc: 0.4696
Epoch 29/120
158/158 - 33s - loss: 0.0830 - acc: 0.6527 - val_loss: 1.4695 - val_acc: 0.4411
Epoch 30/120
158/158 - 33s - loss: 0.0671 - acc: 0.6505 - val_loss: 1.2961 - val_acc: 0.4089
Epoch 31/120
158/158 - 33s - loss: 0.0408 - acc: 0.6076 - val_loss: 1.5849 - val_acc: 0.3946
Epoch 32/120
158/158 - 33s - loss: 0.0504 - acc: 0.6066 - val_loss: 1.4037 - val_acc: 0.3946
Epoch 33/120
158/158 - 33s - loss: 0.0487 - acc: 0.5879 - val_loss: 1.3634 - val_acc: 0.4036
Epoch 34/120
158/158 - 33s - loss: 0.0525 - acc: 0.5812 - val_loss: 1.4720 - val_acc: 0.4036
Epoch 35/120
158/158 - 33s - loss: 0.0585 - acc: 0.6263 - val_loss: 1.4992 - val_acc: 0.3839
Epoch 36/120
158/158 - 33s - loss: 0.1086 - acc: 0.5996 - val_loss: 1.4054 - val_acc: 0.4429
Epoch 37/120
158/158 - 33s - loss: 0.0554 - acc: 0.6227 - val_loss: 1.5211 - val_acc: 0.3750
Epoch 38/120
158/158 - 33s - loss: 0.0405 - acc: 0.5764 - val_loss: 1.4521 - val_acc: 0.4125
Epoch 39/120
158/158 - 33s - loss: 0.0376 - acc: 0.5400 - val_loss: 1.5206 - val_acc: 0.3625
Epoch 40/120
158/158 - 33s - loss: 0.0367 - acc: 0.5637 - val_loss: 1.3856 - val_acc: 0.3875
Epoch 41/120
158/158 - 33s - loss: 0.0368 - acc: 0.5082 - val_loss: 1.3647 - val_acc: 0.4000
Epoch 42/120
158/158 - 33s - loss: 0.0327 - acc: 0.5830 - val_loss: 1.4597 - val_acc: 0.3875
Epoch 43/120
158/158 - 33s - loss: 0.0374 - acc: 0.5712 - val_loss: 1.5004 - val_acc: 0.4161
Epoch 44/120
158/158 - 33s - loss: 0.0480 - acc: 0.5782 - val_loss: 1.4720 - val_acc: 0.4268
Epoch 45/120
158/158 - 33s - loss: 0.0352 - acc: 0.5913 - val_loss: 1.3839 - val_acc: 0.4589
Epoch 46/120
158/158 - 33s - loss: 0.0335 - acc: 0.6479 - val_loss: 1.3730 - val_acc: 0.4518
Epoch 47/120
158/158 - 33s - loss: 0.0300 - acc: 0.6491 - val_loss: 1.4017 - val_acc: 0.4536
Epoch 48/120
158/158 - 33s - loss: 0.0280 - acc: 0.6205 - val_loss: 1.3444 - val_acc: 0.4375
Epoch 49/120
158/158 - 33s - loss: 0.0290 - acc: 0.5887 - val_loss: 1.4246 - val_acc: 0.4268
Epoch 50/120
158/158 - 33s - loss: 0.0384 - acc: 0.6261 - val_loss: 1.4132 - val_acc: 0.4821
Epoch 51/120
158/158 - 33s - loss: 0.0677 - acc: 0.6243 - val_loss: 1.3997 - val_acc: 0.4643
Epoch 52/120
158/158 - 33s - loss: 0.1278 - acc: 0.6787 - val_loss: 1.5531 - val_acc: 0.3929
Epoch 53/120
158/158 - 33s - loss: 0.0413 - acc: 0.5408 - val_loss: 1.4487 - val_acc: 0.3768
Epoch 54/120
158/158 - 33s - loss: 0.0325 - acc: 0.5563 - val_loss: 1.3963 - val_acc: 0.4071
Epoch 55/120
158/158 - 33s - loss: 0.0725 - acc: 0.6384 - val_loss: 1.5807 - val_acc: 0.4268
Epoch 56/120
158/158 - 33s - loss: 0.0437 - acc: 0.6072 - val_loss: 1.5188 - val_acc: 0.4196
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
2020-03-10 12:34:47.312144: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.
2020-03-10 12:34:47.312302: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_NOT_INITIALIZED
2020-03-10 12:34:47.312345: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_NOT_INITIALIZED
2020-03-10 12:34:47.515190: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER
2020-03-10 12:34:47.515292: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.
Epoch 00056: early stopping
Train for 158 steps, validate for 18 steps
Epoch 1/300
158/158 - 34s - loss: 0.2067 - acc: 0.9543 - val_loss: 0.9025 - val_acc: 0.7357
Epoch 2/300
158/158 - 33s - loss: 0.0912 - acc: 0.9839 - val_loss: 0.8606 - val_acc: 0.7446
Epoch 3/300
158/158 - 33s - loss: 0.0616 - acc: 0.9895 - val_loss: 0.9218 - val_acc: 0.7429
Epoch 4/300
158/158 - 33s - loss: 0.0486 - acc: 0.9895 - val_loss: 0.9614 - val_acc: 0.7286
Epoch 5/300
158/158 - 33s - loss: 0.0423 - acc: 0.9907 - val_loss: 0.9582 - val_acc: 0.7357
Epoch 6/300
158/158 - 33s - loss: 0.0404 - acc: 0.9913 - val_loss: 0.9171 - val_acc: 0.7339
Epoch 7/300
158/158 - 33s - loss: 0.0395 - acc: 0.9907 - val_loss: 0.9421 - val_acc: 0.7464
Epoch 8/300
158/158 - 33s - loss: 0.0372 - acc: 0.9913 - val_loss: 0.9812 - val_acc: 0.7375
Epoch 9/300
158/158 - 33s - loss: 0.0346 - acc: 0.9901 - val_loss: 0.9592 - val_acc: 0.7464
Epoch 10/300
158/158 - 33s - loss: 0.0334 - acc: 0.9899 - val_loss: 0.9823 - val_acc: 0.7339
Epoch 11/300
158/158 - 33s - loss: 0.0327 - acc: 0.9909 - val_loss: 0.9733 - val_acc: 0.7375
Epoch 12/300
158/158 - 33s - loss: 0.0332 - acc: 0.9897 - val_loss: 0.9400 - val_acc: 0.7357
Epoch 13/300
158/158 - 33s - loss: 0.0329 - acc: 0.9905 - val_loss: 0.9346 - val_acc: 0.7554
Epoch 14/300
158/158 - 33s - loss: 0.0324 - acc: 0.9907 - val_loss: 0.9925 - val_acc: 0.7446
Epoch 15/300
158/158 - 33s - loss: 0.0309 - acc: 0.9921 - val_loss: 1.0330 - val_acc: 0.7393
Epoch 16/300
158/158 - 33s - loss: 0.0313 - acc: 0.9911 - val_loss: 0.9715 - val_acc: 0.7518
Epoch 17/300
158/158 - 33s - loss: 0.0304 - acc: 0.9926 - val_loss: 1.0108 - val_acc: 0.7375
Epoch 18/300
158/158 - 33s - loss: 0.0312 - acc: 0.9911 - val_loss: 0.9880 - val_acc: 0.7518
Epoch 19/300
158/158 - 33s - loss: 0.0298 - acc: 0.9926 - val_loss: 1.0267 - val_acc: 0.7357
Epoch 20/300
158/158 - 33s - loss: 0.0295 - acc: 0.9921 - val_loss: 0.9954 - val_acc: 0.7446
Epoch 21/300
158/158 - 33s - loss: 0.0293 - acc: 0.9913 - val_loss: 1.0427 - val_acc: 0.7321
Epoch 22/300
158/158 - 33s - loss: 0.0264 - acc: 0.9917 - val_loss: 1.0129 - val_acc: 0.7536
Epoch 23/300
158/158 - 33s - loss: 0.0275 - acc: 0.9915 - val_loss: 0.9713 - val_acc: 0.7518
Epoch 24/300
158/158 - 33s - loss: 0.0273 - acc: 0.9924 - val_loss: 0.9415 - val_acc: 0.7411
Epoch 25/300
158/158 - 33s - loss: 0.0276 - acc: 0.9921 - val_loss: 1.0132 - val_acc: 0.7429
Epoch 26/300
158/158 - 33s - loss: 0.0291 - acc: 0.9901 - val_loss: 1.0211 - val_acc: 0.7518
Epoch 27/300
158/158 - 33s - loss: 0.0258 - acc: 0.9924 - val_loss: 0.9973 - val_acc: 0.7500
Epoch 28/300
158/158 - 33s - loss: 0.0260 - acc: 0.9923 - val_loss: 0.9747 - val_acc: 0.7571
Epoch 29/300
158/158 - 33s - loss: 0.0239 - acc: 0.9924 - val_loss: 0.9799 - val_acc: 0.7500
Epoch 30/300
158/158 - 33s - loss: 0.0255 - acc: 0.9919 - val_loss: 0.9776 - val_acc: 0.7589
Epoch 31/300
158/158 - 33s - loss: 0.0246 - acc: 0.9919 - val_loss: 0.9882 - val_acc: 0.7393
Epoch 32/300
158/158 - 33s - loss: 0.0922 - acc: 0.9742 - val_loss: 1.2937 - val_acc: 0.7071
Epoch 33/300
158/158 - 33s - loss: 0.0411 - acc: 0.9899 - val_loss: 1.0771 - val_acc: 0.7554
Epoch 34/300
158/158 - 33s - loss: 0.0242 - acc: 0.9917 - val_loss: 1.0435 - val_acc: 0.7446
Epoch 35/300
158/158 - 33s - loss: 0.0187 - acc: 0.9932 - val_loss: 1.0628 - val_acc: 0.7464
Epoch 36/300
158/158 - 33s - loss: 0.0187 - acc: 0.9913 - val_loss: 1.0513 - val_acc: 0.7518
Epoch 37/300
158/158 - 33s - loss: 0.0173 - acc: 0.9909 - val_loss: 1.0345 - val_acc: 0.7464
Epoch 38/300
158/158 - 33s - loss: 0.0169 - acc: 0.9915 - val_loss: 1.0692 - val_acc: 0.7411
Epoch 39/300
158/158 - 33s - loss: 0.0170 - acc: 0.9923 - val_loss: 0.9898 - val_acc: 0.7661
Epoch 40/300
158/158 - 33s - loss: 0.0178 - acc: 0.9915 - val_loss: 1.0118 - val_acc: 0.7625
Epoch 41/300
158/158 - 33s - loss: 0.0173 - acc: 0.9913 - val_loss: 1.0344 - val_acc: 0.7518
Epoch 42/300
158/158 - 33s - loss: 0.0166 - acc: 0.9909 - val_loss: 1.0563 - val_acc: 0.7464
Epoch 43/300
158/158 - 33s - loss: 0.0175 - acc: 0.9919 - val_loss: 1.0311 - val_acc: 0.7554
Epoch 44/300
158/158 - 33s - loss: 0.0167 - acc: 0.9930 - val_loss: 1.0163 - val_acc: 0.7625
Epoch 45/300
158/158 - 33s - loss: 0.0171 - acc: 0.9913 - val_loss: 1.0100 - val_acc: 0.7482
Epoch 46/300
158/158 - 33s - loss: 0.0181 - acc: 0.9915 - val_loss: 1.0212 - val_acc: 0.7464
Epoch 47/300
158/158 - 33s - loss: 0.0175 - acc: 0.9926 - val_loss: 1.0554 - val_acc: 0.7482
Epoch 48/300
158/158 - 33s - loss: 0.0184 - acc: 0.9917 - val_loss: 1.0154 - val_acc: 0.7411
Epoch 49/300
158/158 - 33s - loss: 0.0380 - acc: 0.9877 - val_loss: 1.4892 - val_acc: 0.6589
Epoch 50/300
158/158 - 33s - loss: 0.1438 - acc: 0.9553 - val_loss: 1.1978 - val_acc: 0.7268
Epoch 51/300
158/158 - 33s - loss: 0.0310 - acc: 0.9915 - val_loss: 1.0539 - val_acc: 0.7286
Epoch 52/300
158/158 - 33s - loss: 0.0173 - acc: 0.9923 - val_loss: 1.0780 - val_acc: 0.7286
Epoch 53/300
158/158 - 33s - loss: 0.0154 - acc: 0.9911 - val_loss: 1.0539 - val_acc: 0.7393
Epoch 54/300
158/158 - 33s - loss: 0.0139 - acc: 0.9917 - val_loss: 1.1154 - val_acc: 0.7375
Epoch 55/300
158/158 - 33s - loss: 0.0133 - acc: 0.9921 - val_loss: 1.0871 - val_acc: 0.7429
Epoch 56/300
158/158 - 33s - loss: 0.0131 - acc: 0.9919 - val_loss: 1.0834 - val_acc: 0.7393
Epoch 57/300
158/158 - 33s - loss: 0.0137 - acc: 0.9915 - val_loss: 1.0637 - val_acc: 0.7375
Epoch 58/300
158/158 - 33s - loss: 0.0127 - acc: 0.9915 - val_loss: 1.1008 - val_acc: 0.7393
Epoch 59/300
158/158 - 33s - loss: 0.0137 - acc: 0.9919 - val_loss: 1.0427 - val_acc: 0.7536
Epoch 60/300
158/158 - 33s - loss: 0.0135 - acc: 0.9915 - val_loss: 1.0544 - val_acc: 0.7393
Epoch 61/300
158/158 - 33s - loss: 0.0139 - acc: 0.9928 - val_loss: 1.0151 - val_acc: 0.7518
Epoch 62/300
158/158 - 33s - loss: 0.0140 - acc: 0.9915 - val_loss: 1.0237 - val_acc: 0.7464
Epoch 63/300
158/158 - 33s - loss: 0.0138 - acc: 0.9913 - val_loss: 1.0199 - val_acc: 0.7482
Epoch 64/300
158/158 - 33s - loss: 0.0136 - acc: 0.9909 - val_loss: 1.0058 - val_acc: 0.7554
Epoch 65/300
158/158 - 33s - loss: 0.0137 - acc: 0.9915 - val_loss: 0.9892 - val_acc: 0.7554
Epoch 66/300
158/158 - 33s - loss: 0.0134 - acc: 0.9911 - val_loss: 0.9851 - val_acc: 0.7714
Epoch 67/300
158/158 - 33s - loss: 0.0141 - acc: 0.9913 - val_loss: 1.0086 - val_acc: 0.7536
Epoch 68/300
158/158 - 33s - loss: 0.0145 - acc: 0.9913 - val_loss: 1.0183 - val_acc: 0.7554
Epoch 69/300
158/158 - 33s - loss: 0.0146 - acc: 0.9928 - val_loss: 0.9956 - val_acc: 0.7464
Epoch 70/300
158/158 - 33s - loss: 0.0151 - acc: 0.9923 - val_loss: 0.9851 - val_acc: 0.7536
Epoch 71/300
158/158 - 33s - loss: 0.0153 - acc: 0.9919 - val_loss: 0.9575 - val_acc: 0.7661
Epoch 72/300
158/158 - 33s - loss: 0.0152 - acc: 0.9917 - val_loss: 0.9725 - val_acc: 0.7554
Epoch 73/300
158/158 - 33s - loss: 0.0150 - acc: 0.9917 - val_loss: 0.9457 - val_acc: 0.7536
Epoch 74/300
158/158 - 33s - loss: 0.0147 - acc: 0.9915 - val_loss: 0.9890 - val_acc: 0.7554
Epoch 75/300
158/158 - 33s - loss: 0.0143 - acc: 0.9917 - val_loss: 0.9760 - val_acc: 0.7464
Epoch 76/300
158/158 - 33s - loss: 0.0141 - acc: 0.9917 - val_loss: 1.0098 - val_acc: 0.7446
Epoch 77/300
158/158 - 33s - loss: 0.0142 - acc: 0.9909 - val_loss: 0.9521 - val_acc: 0.7536
Epoch 78/300
158/158 - 33s - loss: 0.0138 - acc: 0.9917 - val_loss: 0.9588 - val_acc: 0.7500
Epoch 79/300
158/158 - 33s - loss: 0.0132 - acc: 0.9917 - val_loss: 0.9897 - val_acc: 0.7625
Epoch 80/300
158/158 - 33s - loss: 0.0141 - acc: 0.9911 - val_loss: 0.9743 - val_acc: 0.7643
Epoch 81/300
158/158 - 33s - loss: 0.0139 - acc: 0.9921 - val_loss: 0.9395 - val_acc: 0.7536
Epoch 82/300
158/158 - 33s - loss: 0.0129 - acc: 0.9921 - val_loss: 0.9697 - val_acc: 0.7554
Epoch 83/300
158/158 - 33s - loss: 0.0139 - acc: 0.9913 - val_loss: 0.9924 - val_acc: 0.7500
Epoch 84/300
158/158 - 33s - loss: 0.0134 - acc: 0.9911 - val_loss: 0.9757 - val_acc: 0.7518
Epoch 85/300
158/158 - 33s - loss: 0.0129 - acc: 0.9913 - val_loss: 0.9170 - val_acc: 0.7696
Epoch 86/300
158/158 - 33s - loss: 0.0133 - acc: 0.9923 - val_loss: 0.9532 - val_acc: 0.7429
Epoch 87/300
158/158 - 33s - loss: 0.0137 - acc: 0.9921 - val_loss: 0.9596 - val_acc: 0.7500
Epoch 88/300
158/158 - 33s - loss: 0.0132 - acc: 0.9921 - val_loss: 0.9475 - val_acc: 0.7750
Epoch 89/300
158/158 - 33s - loss: 0.0129 - acc: 0.9913 - val_loss: 0.9460 - val_acc: 0.7643
Epoch 90/300
158/158 - 33s - loss: 0.0127 - acc: 0.9923 - val_loss: 0.9974 - val_acc: 0.7518
Epoch 91/300
158/158 - 33s - loss: 0.0128 - acc: 0.9919 - val_loss: 0.9609 - val_acc: 0.7571
Epoch 92/300
158/158 - 33s - loss: 0.0267 - acc: 0.9899 - val_loss: 1.3430 - val_acc: 0.6929
Epoch 93/300
158/158 - 33s - loss: 0.1125 - acc: 0.9642 - val_loss: 1.0856 - val_acc: 0.7339
Epoch 94/300
158/158 - 33s - loss: 0.0164 - acc: 0.9919 - val_loss: 1.0654 - val_acc: 0.7464
Epoch 95/300
158/158 - 33s - loss: 0.0122 - acc: 0.9928 - val_loss: 1.0539 - val_acc: 0.7518
Epoch 96/300
158/158 - 33s - loss: 0.0109 - acc: 0.9924 - val_loss: 1.0888 - val_acc: 0.7518
Epoch 97/300
158/158 - 33s - loss: 0.0104 - acc: 0.9924 - val_loss: 1.0933 - val_acc: 0.7589
Epoch 98/300
158/158 - 33s - loss: 0.0101 - acc: 0.9919 - val_loss: 1.1161 - val_acc: 0.7554
Epoch 99/300
158/158 - 33s - loss: 0.0099 - acc: 0.9921 - val_loss: 1.1072 - val_acc: 0.7554
Epoch 100/300
158/158 - 33s - loss: 0.0099 - acc: 0.9915 - val_loss: 1.0981 - val_acc: 0.7571
Epoch 101/300
158/158 - 33s - loss: 0.0097 - acc: 0.9926 - val_loss: 1.1014 - val_acc: 0.7589
Epoch 102/300
158/158 - 33s - loss: 0.0101 - acc: 0.9907 - val_loss: 1.0835 - val_acc: 0.7607
Epoch 00102: early stopping
Correct: 989, wrong: 410, accuracy: 70.69335239456754%
Varcorrect: 273, varwrong: 1126, accuracy: 19.513938527519656%
Supercorrect: 189, superwrong: 328, accuracy: 13.509649749821302%
match: 271, notmatch: 1128, accuracy: 19.37097927090779%


###############################################################################
Peregrine Cluster
Job 10027049 for user 's2934833'
Finished at: Tue Mar 10 13:30:36 CET 2020

Job details:
============

Name                : varianceoutput
User                : s2934833
Partition           : gpu
Nodes               : pg-gpu23
Cores               : 12
State               : COMPLETED
Submit              : 2020-03-10T11:58:15
Start               : 2020-03-10T12:03:46
End                 : 2020-03-10T13:30:35
Reserved walltime   : 02:00:00
Used walltime       : 01:26:49
Used CPU time       : 01:12:55 (efficiency:  7.00%)
% User (Computation): 70.53%
% System (I/O)      : 29.48%
Mem reserved        : 32000M/node
Max Mem used        : 8.64G (pg-gpu23)
Max Disk Write      : 153.60K (pg-gpu23)
Max Disk Read       : 5.35M (pg-gpu23)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
