Random seed for replication: 734
Random seed for replication: 7
dataset_name = /Polar_PNG_256.hdf5, batch_size = 16, num_classes = 3, epochs = 500,
        MCDO_PREDICTIONS = 50, MCDO_BATCH_SIZE = 16, test_img_idx = 36,
        train_test_split = 0.8, to_shuffle = True, augmentation = False, train_label_count = [42, 22, 120],
        test_label_count = [14, 7, 33], label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 0.01,
        add_dropout_inside = True, train_all_layers = True, weights_to_use = imagenet,
        es_patience = 50, train_val_split = 0.9, MIN_DELTA = 0.005, Early_monitor = val_accuracy
x_train shape: (184, 256, 256, 3)
184 train samples
54 test samples
Model: "model_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 256, 256, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
dropout (Dropout)            (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
dropout_5 (Dropout)          (None, 4096)              0         
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
dropout_6 (Dropout)          (None, 4096)              0         
_________________________________________________________________
predictions (Dense)          (None, 3)                 12291     
=================================================================
Total params: 165,730,115
Trainable params: 165,730,115
Non-trainable params: 0
_________________________________________________________________
Start fitting monte carlo dropout model
Train for 11 steps, validate for 2 steps
Epoch 1/500
11/11 - 7s - loss: 123635626795.5049 - accuracy: 0.4970 - val_loss: 115921.2109 - val_accuracy: 0.7895
Epoch 2/500
11/11 - 3s - loss: 878409.4912 - accuracy: 0.5152 - val_loss: 18943.8486 - val_accuracy: 0.7895
Epoch 3/500
11/11 - 3s - loss: 350641.4264 - accuracy: 0.5030 - val_loss: 141304.7617 - val_accuracy: 0.7895
Epoch 4/500
11/11 - 3s - loss: 38372.3402 - accuracy: 0.4848 - val_loss: 3483.1870 - val_accuracy: 0.7895
Epoch 5/500
11/11 - 3s - loss: 26312.7199 - accuracy: 0.5394 - val_loss: 14777.4121 - val_accuracy: 0.2105
Epoch 6/500
11/11 - 3s - loss: 10382.7971 - accuracy: 0.4788 - val_loss: 5512.9792 - val_accuracy: 0.5789
Epoch 7/500
11/11 - 3s - loss: 2602.4423 - accuracy: 0.4364 - val_loss: 407.2951 - val_accuracy: 0.5789
Epoch 8/500
11/11 - 3s - loss: 1227.2910 - accuracy: 0.5515 - val_loss: 2933.7358 - val_accuracy: 0.4737
Epoch 9/500
11/11 - 3s - loss: 89590.1539 - accuracy: 0.5273 - val_loss: 6217.9792 - val_accuracy: 0.5263
Epoch 10/500
11/11 - 3s - loss: 58757.5211 - accuracy: 0.4606 - val_loss: 21879.8108 - val_accuracy: 0.6842
Epoch 11/500
11/11 - 3s - loss: 23367.6202 - accuracy: 0.4727 - val_loss: 18522.1904 - val_accuracy: 0.3158
Epoch 12/500
11/11 - 3s - loss: 8322.5643 - accuracy: 0.3455 - val_loss: 687.6620 - val_accuracy: 0.6316
Epoch 13/500
11/11 - 3s - loss: 750.6133 - accuracy: 0.5455 - val_loss: 706.6506 - val_accuracy: 0.6316
Epoch 14/500
11/11 - 3s - loss: 596.9410 - accuracy: 0.4545 - val_loss: 101.7022 - val_accuracy: 0.2632
Epoch 15/500
11/11 - 3s - loss: 59.4616 - accuracy: 0.4545 - val_loss: 3.4534 - val_accuracy: 0.7895
Epoch 16/500
11/11 - 3s - loss: 2290.0412 - accuracy: 0.4303 - val_loss: 203.7971 - val_accuracy: 0.1579
Epoch 17/500
11/11 - 3s - loss: 454.1239 - accuracy: 0.4545 - val_loss: 1.5145 - val_accuracy: 0.7368
Epoch 18/500
11/11 - 3s - loss: 13978.5766 - accuracy: 0.4788 - val_loss: 58.8374 - val_accuracy: 0.1579
Epoch 19/500
11/11 - 3s - loss: 31.1672 - accuracy: 0.4909 - val_loss: 17894.9487 - val_accuracy: 0.7895
Epoch 20/500
11/11 - 3s - loss: 4594.9468 - accuracy: 0.5030 - val_loss: 12.4376 - val_accuracy: 0.5789
Epoch 21/500
11/11 - 3s - loss: 12.1393 - accuracy: 0.4970 - val_loss: 3.4678 - val_accuracy: 0.6316
Epoch 22/500
11/11 - 3s - loss: 4.1520 - accuracy: 0.4788 - val_loss: 2.5549 - val_accuracy: 0.4737
Epoch 23/500
11/11 - 3s - loss: 3.6635 - accuracy: 0.4788 - val_loss: 2.9254 - val_accuracy: 0.4211
Epoch 24/500
11/11 - 3s - loss: 3.0813 - accuracy: 0.4848 - val_loss: 2.5513 - val_accuracy: 0.5263
Epoch 25/500
11/11 - 3s - loss: 2.6395 - accuracy: 0.3879 - val_loss: 2.0854 - val_accuracy: 0.5263
Epoch 26/500
11/11 - 3s - loss: 2.0181 - accuracy: 0.5273 - val_loss: 3.8355 - val_accuracy: 0.2105
Epoch 27/500
11/11 - 3s - loss: 2.2595 - accuracy: 0.3455 - val_loss: 1.9773 - val_accuracy: 0.6842
Epoch 28/500
11/11 - 3s - loss: 1.3516 - accuracy: 0.6061 - val_loss: 1.4715 - val_accuracy: 0.4737
Epoch 29/500
11/11 - 3s - loss: 1.3965 - accuracy: 0.4970 - val_loss: 1.4280 - val_accuracy: 0.5789
Epoch 30/500
11/11 - 3s - loss: 1.5745 - accuracy: 0.4485 - val_loss: 0.9787 - val_accuracy: 0.6316
Epoch 31/500
11/11 - 3s - loss: 1.3368 - accuracy: 0.4485 - val_loss: 1.0184 - val_accuracy: 0.6842
Epoch 32/500
11/11 - 3s - loss: 1.3653 - accuracy: 0.5394 - val_loss: 1.0334 - val_accuracy: 0.7895
Epoch 33/500
11/11 - 3s - loss: 1.2624 - accuracy: 0.5455 - val_loss: 1.3279 - val_accuracy: 0.6316
Epoch 34/500
11/11 - 3s - loss: 1.4059 - accuracy: 0.4848 - val_loss: 1.2803 - val_accuracy: 0.5789
Epoch 35/500
11/11 - 3s - loss: 1.2294 - accuracy: 0.5212 - val_loss: 1.1260 - val_accuracy: 0.5789
Epoch 36/500
11/11 - 3s - loss: 1.2010 - accuracy: 0.5212 - val_loss: 1.2314 - val_accuracy: 0.5263
Epoch 37/500
11/11 - 3s - loss: 1.1900 - accuracy: 0.4970 - val_loss: 1.1246 - val_accuracy: 0.7368
Epoch 38/500
11/11 - 3s - loss: 1.1980 - accuracy: 0.5394 - val_loss: 1.2586 - val_accuracy: 0.7368
Epoch 39/500
11/11 - 3s - loss: 1.0326 - accuracy: 0.5939 - val_loss: 1.1222 - val_accuracy: 0.6316
Epoch 40/500
11/11 - 3s - loss: 1.0939 - accuracy: 0.4970 - val_loss: 1.1600 - val_accuracy: 0.4737
Epoch 41/500
11/11 - 3s - loss: 1.1054 - accuracy: 0.5333 - val_loss: 1.1702 - val_accuracy: 0.7368
Epoch 42/500
11/11 - 3s - loss: 1.1284 - accuracy: 0.5636 - val_loss: 0.9232 - val_accuracy: 0.6316
Epoch 43/500
11/11 - 3s - loss: 1.0724 - accuracy: 0.5939 - val_loss: 1.1381 - val_accuracy: 0.6842
Epoch 44/500
11/11 - 3s - loss: 1.0555 - accuracy: 0.5212 - val_loss: 1.0313 - val_accuracy: 0.7368
Epoch 45/500
11/11 - 3s - loss: 1.0308 - accuracy: 0.5939 - val_loss: 1.3743 - val_accuracy: 0.6316
Epoch 46/500
11/11 - 3s - loss: 1.0265 - accuracy: 0.5939 - val_loss: 1.2121 - val_accuracy: 0.7895
Epoch 47/500
11/11 - 3s - loss: 0.9827 - accuracy: 0.6121 - val_loss: 1.0485 - val_accuracy: 0.7368
Epoch 48/500
11/11 - 3s - loss: 1.0532 - accuracy: 0.5818 - val_loss: 1.2662 - val_accuracy: 0.7368
Epoch 49/500
11/11 - 3s - loss: 0.9559 - accuracy: 0.5939 - val_loss: 1.0744 - val_accuracy: 0.7368
Epoch 50/500
11/11 - 3s - loss: 1.0190 - accuracy: 0.6000 - val_loss: 1.0176 - val_accuracy: 0.7895
Epoch 51/500
11/11 - 3s - loss: 1.0251 - accuracy: 0.5939 - val_loss: 1.0840 - val_accuracy: 0.7368
Epoch 00051: early stopping
 0/50 [..............................] - ETA: 0s15/50 [========>.....................] - ETA: 11s32/50 [==================>...........] - ETA: 5s 49/50 [============================>.] - ETA: 0sMCDO accuracy: 58.5%
MCDO-ensemble accuracy: 61.1%
tf.Tensor(
[[ 0  0 14]
 [ 0  0  7]
 [ 0  0 33]], shape=(3, 3), dtype=int32)
posterior mean: 2
true label: 2

class: 0; proba: 25.1%; var: 8.55% 
class: 1; proba: 13.9%; var: 7.42% 
class: 2; proba: 61.0%; var: 11.12% 
