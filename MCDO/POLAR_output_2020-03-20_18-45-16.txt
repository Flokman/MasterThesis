Random seed for replication: 968
Random seed for replication: 863
dataset_name = /Polar_PNG_256.hdf5, batch_size = 16, num_classes = 3, epochs = 500,
        MCDO_PREDICTIONS = 50, MCDO_BATCH_SIZE = 16, test_img_idx = 16,
        train_test_split = 0.8, to_shuffle = True, augmentation = False, train_label_count = [42, 22, 120],
        test_label_count = [14, 7, 33], label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 0.01,
        add_dropout_inside = True, train_all_layers = True, weights_to_use = imagenet,
        es_patience = 50, train_val_split = 0.9
x_train shape: (184, 256, 256, 3)
184 train samples
54 test samples
Model: "model_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 256, 256, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
dropout (Dropout)            (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
dropout_5 (Dropout)          (None, 4096)              0         
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
dropout_6 (Dropout)          (None, 4096)              0         
_________________________________________________________________
predictions (Dense)          (None, 3)                 12291     
=================================================================
Total params: 165,730,115
Trainable params: 165,730,115
Non-trainable params: 0
_________________________________________________________________
Start fitting monte carlo dropout model
Train for 11 steps, validate for 2 steps
Epoch 1/500
11/11 - 7s - loss: 36598691721.8161 - accuracy: 0.4061 - val_loss: 406437.8750 - val_accuracy: 0.8947
Epoch 2/500
11/11 - 3s - loss: 8260457.2452 - accuracy: 0.3394 - val_loss: 9436.5303 - val_accuracy: 0.8947
Epoch 3/500
11/11 - 3s - loss: 36810.5310 - accuracy: 0.5636 - val_loss: 8608.8248 - val_accuracy: 0.4737
Epoch 4/500
11/11 - 3s - loss: 76999.2144 - accuracy: 0.4242 - val_loss: 3173.2603 - val_accuracy: 0.8421
Epoch 5/500
11/11 - 3s - loss: 3467.5454 - accuracy: 0.4909 - val_loss: 20.4138 - val_accuracy: 0.8947
Epoch 6/500
11/11 - 3s - loss: 351.5780 - accuracy: 0.3939 - val_loss: 5.5424 - val_accuracy: 0.7368
Epoch 7/500
11/11 - 3s - loss: 91.5687 - accuracy: 0.4364 - val_loss: 0.7380 - val_accuracy: 0.7895
Epoch 8/500
11/11 - 3s - loss: 1.3305 - accuracy: 0.4909 - val_loss: 1.3619 - val_accuracy: 0.3684
Epoch 9/500
11/11 - 3s - loss: 1.3722 - accuracy: 0.4424 - val_loss: 1.8859 - val_accuracy: 0.4211
Epoch 10/500
11/11 - 3s - loss: 39.9468 - accuracy: 0.4545 - val_loss: 0.4346 - val_accuracy: 0.7895
Epoch 11/500
11/11 - 3s - loss: 2498.2752 - accuracy: 0.4606 - val_loss: 0.3071 - val_accuracy: 0.8947
Epoch 12/500
11/11 - 3s - loss: 2.2608 - accuracy: 0.5152 - val_loss: 0.5928 - val_accuracy: 0.6316
Epoch 13/500
11/11 - 3s - loss: 1.3609 - accuracy: 0.5394 - val_loss: 2.1994 - val_accuracy: 0.2632
Epoch 14/500
11/11 - 3s - loss: 1.4871 - accuracy: 0.4848 - val_loss: 0.4566 - val_accuracy: 0.8421
Epoch 15/500
11/11 - 3s - loss: 1.2744 - accuracy: 0.4485 - val_loss: 0.5314 - val_accuracy: 0.7895
Epoch 16/500
11/11 - 3s - loss: 1.2733 - accuracy: 0.4848 - val_loss: 0.6033 - val_accuracy: 0.6842
Epoch 17/500
11/11 - 3s - loss: 1.1443 - accuracy: 0.4909 - val_loss: 0.6504 - val_accuracy: 0.5263
Epoch 18/500
11/11 - 3s - loss: 1.1887 - accuracy: 0.4788 - val_loss: 0.5775 - val_accuracy: 0.8421
Epoch 19/500
11/11 - 3s - loss: 1.0815 - accuracy: 0.5576 - val_loss: 0.5110 - val_accuracy: 0.7895
Epoch 20/500
11/11 - 3s - loss: 1.1100 - accuracy: 0.5576 - val_loss: 0.8338 - val_accuracy: 0.7368
Epoch 21/500
11/11 - 3s - loss: 1.1009 - accuracy: 0.4909 - val_loss: 0.7174 - val_accuracy: 0.6842
Epoch 22/500
11/11 - 3s - loss: 1.0194 - accuracy: 0.5273 - val_loss: 0.4835 - val_accuracy: 0.8947
Epoch 23/500
11/11 - 3s - loss: 0.9605 - accuracy: 0.6303 - val_loss: 0.6038 - val_accuracy: 0.6842
Epoch 24/500
11/11 - 3s - loss: 1.0572 - accuracy: 0.5758 - val_loss: 0.4586 - val_accuracy: 0.8947
Epoch 25/500
11/11 - 3s - loss: 1.0220 - accuracy: 0.5091 - val_loss: 0.7799 - val_accuracy: 0.7368
Epoch 26/500
11/11 - 3s - loss: 0.9792 - accuracy: 0.6000 - val_loss: 0.5978 - val_accuracy: 0.8947
Epoch 27/500
11/11 - 3s - loss: 0.9759 - accuracy: 0.5818 - val_loss: 0.7194 - val_accuracy: 0.7895
Epoch 28/500
11/11 - 3s - loss: 0.9675 - accuracy: 0.6000 - val_loss: 0.6675 - val_accuracy: 0.7895
Epoch 29/500
11/11 - 3s - loss: 0.9664 - accuracy: 0.5818 - val_loss: 0.6635 - val_accuracy: 0.8947
Epoch 30/500
11/11 - 3s - loss: 0.9626 - accuracy: 0.6061 - val_loss: 0.5095 - val_accuracy: 0.8947
Epoch 31/500
11/11 - 3s - loss: 1.0313 - accuracy: 0.5515 - val_loss: 0.7024 - val_accuracy: 0.6842
Epoch 32/500
11/11 - 3s - loss: 0.8967 - accuracy: 0.6000 - val_loss: 0.5310 - val_accuracy: 0.8947
Epoch 33/500
11/11 - 3s - loss: 0.9556 - accuracy: 0.6242 - val_loss: 0.6079 - val_accuracy: 0.8947
Epoch 34/500
11/11 - 3s - loss: 0.9517 - accuracy: 0.6000 - val_loss: 0.7436 - val_accuracy: 0.8421
Epoch 35/500
11/11 - 3s - loss: 0.9529 - accuracy: 0.6364 - val_loss: 0.6971 - val_accuracy: 0.8421
Epoch 36/500
11/11 - 3s - loss: 0.9579 - accuracy: 0.6303 - val_loss: 0.5374 - val_accuracy: 0.8947
Epoch 37/500
11/11 - 3s - loss: 0.9899 - accuracy: 0.5697 - val_loss: 0.7065 - val_accuracy: 0.7895
Epoch 38/500
11/11 - 3s - loss: 0.9592 - accuracy: 0.6303 - val_loss: 0.4709 - val_accuracy: 0.8947
Epoch 39/500
11/11 - 3s - loss: 0.9668 - accuracy: 0.6303 - val_loss: 0.5903 - val_accuracy: 0.8947
Epoch 40/500
11/11 - 3s - loss: 0.9539 - accuracy: 0.6182 - val_loss: 0.7181 - val_accuracy: 0.8947
Epoch 41/500
11/11 - 3s - loss: 0.9264 - accuracy: 0.6182 - val_loss: 0.6552 - val_accuracy: 0.9474
Epoch 42/500
11/11 - 3s - loss: 0.9466 - accuracy: 0.6242 - val_loss: 0.5088 - val_accuracy: 0.8947
Epoch 43/500
11/11 - 3s - loss: 0.9597 - accuracy: 0.6242 - val_loss: 0.6065 - val_accuracy: 0.8947
Epoch 44/500
11/11 - 3s - loss: 0.9358 - accuracy: 0.6182 - val_loss: 0.6384 - val_accuracy: 0.8947
Epoch 45/500
11/11 - 3s - loss: 0.9411 - accuracy: 0.6242 - val_loss: 0.5917 - val_accuracy: 0.8947
Epoch 46/500
11/11 - 3s - loss: 0.9461 - accuracy: 0.6182 - val_loss: 0.5376 - val_accuracy: 0.8947
Epoch 47/500
11/11 - 3s - loss: 0.9305 - accuracy: 0.6242 - val_loss: 0.5876 - val_accuracy: 0.8947
Epoch 48/500
11/11 - 3s - loss: 0.9494 - accuracy: 0.6242 - val_loss: 0.5973 - val_accuracy: 0.8947
Epoch 49/500
11/11 - 3s - loss: 0.9307 - accuracy: 0.6182 - val_loss: 0.6144 - val_accuracy: 0.8947
Epoch 50/500
11/11 - 3s - loss: 0.9443 - accuracy: 0.6242 - val_loss: 0.5455 - val_accuracy: 0.8947
Epoch 51/500
11/11 - 3s - loss: 0.9502 - accuracy: 0.6242 - val_loss: 0.6034 - val_accuracy: 0.8947
Epoch 52/500
11/11 - 3s - loss: 0.9393 - accuracy: 0.6242 - val_loss: 0.5321 - val_accuracy: 0.8947
Epoch 53/500
11/11 - 3s - loss: 0.9335 - accuracy: 0.6242 - val_loss: 0.5999 - val_accuracy: 0.8947
Epoch 54/500
11/11 - 3s - loss: 0.9501 - accuracy: 0.6242 - val_loss: 0.6655 - val_accuracy: 0.8947
Epoch 55/500
11/11 - 3s - loss: 0.9489 - accuracy: 0.6242 - val_loss: 0.5050 - val_accuracy: 0.8947
Epoch 56/500
11/11 - 3s - loss: 0.9341 - accuracy: 0.6242 - val_loss: 0.5656 - val_accuracy: 0.8947
Epoch 57/500
11/11 - 3s - loss: 0.9359 - accuracy: 0.6242 - val_loss: 0.6218 - val_accuracy: 0.8947
Epoch 58/500
11/11 - 3s - loss: 0.9246 - accuracy: 0.6242 - val_loss: 0.5223 - val_accuracy: 0.8947
Epoch 59/500
11/11 - 3s - loss: 0.9413 - accuracy: 0.6242 - val_loss: 0.6128 - val_accuracy: 0.8947
Epoch 60/500
11/11 - 3s - loss: 0.9498 - accuracy: 0.6242 - val_loss: 0.6222 - val_accuracy: 0.8947
Epoch 61/500
11/11 - 3s - loss: 0.9517 - accuracy: 0.6242 - val_loss: 0.5661 - val_accuracy: 0.8947
Epoch 00061: early stopping
 0/50 [..............................] - ETA: 0s15/50 [========>.....................] - ETA: 11s32/50 [==================>...........] - ETA: 5s 49/50 [============================>.] - ETA: 0sMCDO accuracy: 61.1%
MCDO-ensemble accuracy: 61.1%
tf.Tensor(
[[ 0  0 14]
 [ 0  0  7]
 [ 0  0 33]], shape=(3, 3), dtype=int32)
posterior mean: 2
true label: 2

class: 0; proba: 23.6%; var: 3.27% 
class: 1; proba: 12.2%; var: 2.29% 
class: 2; proba: 64.2%; var: 4.10% 
