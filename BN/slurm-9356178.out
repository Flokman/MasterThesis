
The following have been reloaded with a version change:
  1) FFTW/3.3.7-gompic-2018a => FFTW/3.3.7-gompi-2018a
  2) OpenMPI/2.1.2-gcccuda-2018a => OpenMPI/2.1.2-GCC-6.4.0-2.28
  3) Python/3.6.4-fosscuda-2018a => Python/3.6.4-foss-2018a
  4) ScaLAPACK/2.0.2-gompic-2018a-OpenBLAS-0.2.20 => ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20

2020-01-22 12:46:58.769631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-22 12:46:58.770204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 31.87GiB freeMemory: 29.56GiB
2020-01-22 12:46:58.770258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-22 12:46:59.307873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-22 12:46:59.307966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-22 12:46:59.307984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-22 12:46:59.308121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28669 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-01-22 12:46:59.309422: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 32, num_classes = 5, epochs = 100, MCBN_amount_of_predictions = 500, MCBN_batch_size = 250, test_img_idx = 647, train_test_split = 0.8, to_shuffle = True, augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463], label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 0.001, add_batch_normalization_inside = True, train_all_layers = True, weights_to_use = None
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
block1_conv1
1
block1_conv2
3
block2_conv1
6
block2_conv2
8
block3_conv1
11
block3_conv2
13
block3_conv3
15
block4_conv1
18
block4_conv2
20
block4_conv3
22
block5_conv1
25
block5_conv2
27
block5_conv3
29
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
batch_normalization (BatchNo (None, 256, 256, 64)      256       
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
batch_normalization_1 (Batch (None, 256, 256, 64)      256       
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
batch_normalization_2 (Batch (None, 128, 128, 128)     512       
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
batch_normalization_3 (Batch (None, 128, 128, 128)     512       
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
batch_normalization_4 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
batch_normalization_5 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
batch_normalization_6 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
batch_normalization_7 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
batch_normalization_8 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
batch_normalization_9 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_10 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_11 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
batch_normalization_13 (Batc (None, 4096)              16384     
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
batch_normalization_14 (Batc (None, 4096)              16384     
_________________________________________________________________
predictions (Dense)          (None, 5)                 20485     
=================================================================
Total params: 165,787,973
Trainable params: 165,763,141
Non-trainable params: 24,832
_________________________________________________________________
Start fitting monte carlo batch_normalization model
Train on 5593 samples, validate on 1399 samples
Epoch 1/100
 - 192s - loss: 2.7025 - acc: 0.2793 - val_loss: 2.2172 - val_acc: 0.2580
Epoch 2/100
 - 176s - loss: 1.8798 - acc: 0.3234 - val_loss: 1.8922 - val_acc: 0.2566
Epoch 3/100
 - 176s - loss: 1.7820 - acc: 0.3436 - val_loss: 2.2100 - val_acc: 0.3045
Epoch 4/100
 - 176s - loss: 1.8922 - acc: 0.3290 - val_loss: 1.9355 - val_acc: 0.3045
Epoch 5/100
 - 176s - loss: 1.6746 - acc: 0.3478 - val_loss: 1.9200 - val_acc: 0.2845
Epoch 6/100
 - 176s - loss: 1.6709 - acc: 0.3662 - val_loss: 2.2816 - val_acc: 0.3574
Epoch 7/100
 - 176s - loss: 1.7030 - acc: 0.3790 - val_loss: 2.1015 - val_acc: 0.3624
Epoch 8/100
 - 176s - loss: 1.7053 - acc: 0.3876 - val_loss: 1.8801 - val_acc: 0.3903
Epoch 9/100
 - 176s - loss: 1.7962 - acc: 0.3765 - val_loss: 2.3320 - val_acc: 0.3038
Epoch 10/100
 - 176s - loss: 1.6737 - acc: 0.3615 - val_loss: 2.1476 - val_acc: 0.2659
Epoch 11/100
 - 176s - loss: 1.7523 - acc: 0.3633 - val_loss: 3.0353 - val_acc: 0.2938
Epoch 12/100
 - 177s - loss: 1.5974 - acc: 0.3817 - val_loss: 1.6325 - val_acc: 0.3774
Epoch 13/100
 - 177s - loss: 1.5560 - acc: 0.3987 - val_loss: 1.5249 - val_acc: 0.3874
Epoch 14/100
 - 177s - loss: 1.4896 - acc: 0.4119 - val_loss: 1.7106 - val_acc: 0.3724
Epoch 15/100
 - 177s - loss: 1.5030 - acc: 0.4096 - val_loss: 1.4253 - val_acc: 0.4267
Epoch 16/100
 - 176s - loss: 1.5365 - acc: 0.4068 - val_loss: 1.5009 - val_acc: 0.3631
Epoch 17/100
 - 176s - loss: 1.4440 - acc: 0.3903 - val_loss: 1.6615 - val_acc: 0.3445
Epoch 18/100
 - 177s - loss: 1.4067 - acc: 0.4041 - val_loss: 1.3943 - val_acc: 0.4353
Epoch 19/100
 - 176s - loss: 1.3319 - acc: 0.4443 - val_loss: 1.3662 - val_acc: 0.4532
Epoch 20/100
 - 176s - loss: 1.3007 - acc: 0.4541 - val_loss: 1.4768 - val_acc: 0.4339
Epoch 21/100
 - 176s - loss: 1.2735 - acc: 0.4679 - val_loss: 1.3717 - val_acc: 0.4389
Epoch 22/100
 - 176s - loss: 1.3055 - acc: 0.4491 - val_loss: 1.4841 - val_acc: 0.4110
Epoch 23/100
 - 176s - loss: 1.2796 - acc: 0.4631 - val_loss: 1.4016 - val_acc: 0.4525
Epoch 24/100
 - 176s - loss: 1.2354 - acc: 0.4783 - val_loss: 1.3945 - val_acc: 0.4074
Epoch 25/100
 - 176s - loss: 1.1952 - acc: 0.5060 - val_loss: 1.2636 - val_acc: 0.4689
Epoch 26/100
 - 176s - loss: 1.1920 - acc: 0.5062 - val_loss: 1.3104 - val_acc: 0.4568
Epoch 27/100
 - 176s - loss: 1.1559 - acc: 0.5240 - val_loss: 1.2461 - val_acc: 0.4911
Epoch 28/100
 - 176s - loss: 1.1801 - acc: 0.5083 - val_loss: 1.4109 - val_acc: 0.3831
Epoch 29/100
 - 176s - loss: 1.1792 - acc: 0.5080 - val_loss: 1.4130 - val_acc: 0.3881
Epoch 30/100
 - 177s - loss: 1.1480 - acc: 0.5217 - val_loss: 1.2480 - val_acc: 0.4853
Epoch 31/100
 - 176s - loss: 1.1642 - acc: 0.5276 - val_loss: 1.3921 - val_acc: 0.4739
Epoch 32/100
 - 177s - loss: 1.1080 - acc: 0.5469 - val_loss: 1.3679 - val_acc: 0.4475
Epoch 33/100
 - 176s - loss: 1.0789 - acc: 0.5507 - val_loss: 1.2624 - val_acc: 0.4846
Epoch 34/100
 - 176s - loss: 1.0259 - acc: 0.5816 - val_loss: 1.7814 - val_acc: 0.3481
Epoch 35/100
 - 176s - loss: 0.9900 - acc: 0.5916 - val_loss: 1.1527 - val_acc: 0.5218
Epoch 36/100
 - 176s - loss: 0.9667 - acc: 0.6070 - val_loss: 1.2264 - val_acc: 0.4961
Epoch 37/100
 - 176s - loss: 0.9838 - acc: 0.5979 - val_loss: 1.1316 - val_acc: 0.5361
Epoch 38/100
 - 176s - loss: 0.8844 - acc: 0.6526 - val_loss: 1.1483 - val_acc: 0.5432
Epoch 39/100
 - 176s - loss: 0.8912 - acc: 0.6417 - val_loss: 1.3524 - val_acc: 0.4896
