
The following have been reloaded with a version change:
  1) FFTW/3.3.7-gompic-2018a => FFTW/3.3.7-gompi-2018a
  2) OpenMPI/2.1.2-gcccuda-2018a => OpenMPI/2.1.2-GCC-6.4.0-2.28
  3) Python/3.6.4-fosscuda-2018a => Python/3.6.4-foss-2018a
  4) ScaLAPACK/2.0.2-gompic-2018a-OpenBLAS-0.2.20 => ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20

2020-02-11 18:20:55.611307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-11 18:20:55.611948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 31.88GiB freeMemory: 29.56GiB
2020-02-11 18:20:55.612023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-02-11 18:20:56.100672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-11 18:20:56.100808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-02-11 18:20:56.100826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-02-11 18:20:56.100983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28671 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-02-11 18:20:56.102579: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Random seed for replication: 656
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 32, num_classes = 5, epochs = 500,
        MCBN_amount_of_predictions = 50, MCBN_batch_size = 250, test_img_idx = 368,
        train_test_split = 0.8, to_shuffle = True, augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463],
        label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 1e-05,
        add_mcbn_inside = True, train_all_layers = True, weights_to_use = None,
        mcbn = True, es_patience = 30, train_val_split = 0.9
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
block1_conv1
1
block1_conv2
3
block2_conv1
6
block2_conv2
8
block3_conv1
11
block3_conv2
13
block3_conv3
15
block4_conv1
18
block4_conv2
20
block4_conv3
22
block5_conv1
25
block5_conv2
27
block5_conv3
29
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
MCBatchNorm_1 (MCBatchNorm)  (None, 256, 256, 64)      0         
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
MCBatchNorm_2 (MCBatchNorm)  (None, 256, 256, 64)      0         
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
MCBatchNorm_3 (MCBatchNorm)  (None, 128, 128, 128)     0         
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
MCBatchNorm_4 (MCBatchNorm)  (None, 128, 128, 128)     0         
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
MCBatchNorm_5 (MCBatchNorm)  (None, 64, 64, 256)       0         
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
MCBatchNorm_6 (MCBatchNorm)  (None, 64, 64, 256)       0         
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
MCBatchNorm_7 (MCBatchNorm)  (None, 64, 64, 256)       0         
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
MCBatchNorm_8 (MCBatchNorm)  (None, 32, 32, 512)       0         
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
MCBatchNorm_9 (MCBatchNorm)  (None, 32, 32, 512)       0         
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
MCBatchNorm_10 (MCBatchNorm) (None, 32, 32, 512)       0         
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
MCBatchNorm_11 (MCBatchNorm) (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
MCBatchNorm_12 (MCBatchNorm) (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
MCBatchNorm_13 (MCBatchNorm) (None, 16, 16, 512)       0         
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
MCBatchNorm_14 (MCBatchNorm) (None, 4096)              0         
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
MCBatchNorm_15 (MCBatchNorm) (None, 4096)              0         
_________________________________________________________________
predictions (Dense)          (None, 5)                 20485     
=================================================================
Total params: 165,738,309
Trainable params: 165,738,309
Non-trainable params: 0
_________________________________________________________________
Start fitting monte carlo mcbn model
Epoch 1/500
 - 90s - loss: 1.7812 - acc: 0.3744 - val_loss: 1.6094 - val_acc: 0.1982
Epoch 2/500
 - 78s - loss: 1.5327 - acc: 0.4522 - val_loss: 1.6093 - val_acc: 0.2107
Epoch 3/500
 - 78s - loss: 1.3730 - acc: 0.4990 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 4/500
 - 78s - loss: 1.3511 - acc: 0.5081 - val_loss: 1.6093 - val_acc: 0.1929
Epoch 5/500
 - 78s - loss: 1.2875 - acc: 0.5265 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 6/500
 - 78s - loss: 1.2565 - acc: 0.5324 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 7/500
 - 78s - loss: 1.1716 - acc: 0.5576 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 8/500
 - 78s - loss: 1.1204 - acc: 0.5747 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 9/500
 - 78s - loss: 1.0856 - acc: 0.5883 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 10/500
 - 80s - loss: 1.0385 - acc: 0.6102 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 11/500
 - 79s - loss: 1.0155 - acc: 0.6210 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 12/500
 - 79s - loss: 0.9730 - acc: 0.6290 - val_loss: 1.6092 - val_acc: 0.1946
Epoch 13/500
 - 79s - loss: 0.9257 - acc: 0.6563 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 14/500
 - 79s - loss: 0.8978 - acc: 0.6538 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 15/500
 - 79s - loss: 0.8872 - acc: 0.6621 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 16/500
 - 79s - loss: 0.8598 - acc: 0.6730 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 17/500
 - 79s - loss: 0.8212 - acc: 0.6892 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 18/500
 - 79s - loss: 0.7838 - acc: 0.7010 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 19/500
 - 79s - loss: 0.7879 - acc: 0.6996 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 20/500
 - 79s - loss: 0.7614 - acc: 0.7153 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 21/500
 - 79s - loss: 0.7670 - acc: 0.7037 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 22/500
 - 79s - loss: 0.7185 - acc: 0.7249 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 23/500
 - 79s - loss: 0.7357 - acc: 0.7218 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 24/500
 - 79s - loss: 0.7198 - acc: 0.7230 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 25/500
 - 79s - loss: 0.7136 - acc: 0.7308 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 26/500
 - 79s - loss: 0.6955 - acc: 0.7353 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 27/500
 - 79s - loss: 0.6812 - acc: 0.7433 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 28/500
 - 79s - loss: 0.6914 - acc: 0.7372 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 29/500
 - 79s - loss: 0.6896 - acc: 0.7439 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 30/500
 - 79s - loss: 0.6726 - acc: 0.7432 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 31/500
 - 79s - loss: 0.6656 - acc: 0.7514 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 32/500
 - 79s - loss: 0.6647 - acc: 0.7514 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 33/500
 - 79s - loss: 0.6304 - acc: 0.7636 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 34/500
 - 79s - loss: 0.6096 - acc: 0.7748 - val_loss: 1.6093 - val_acc: 0.1946
Epoch 35/500
 - 79s - loss: 0.5989 - acc: 0.7729 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 36/500
 - 79s - loss: 0.5889 - acc: 0.7793 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 37/500
 - 79s - loss: 0.5851 - acc: 0.7740 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 38/500
 - 79s - loss: 0.5853 - acc: 0.7830 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 39/500
 - 79s - loss: 0.5641 - acc: 0.7855 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 40/500
 - 79s - loss: 0.5573 - acc: 0.7960 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 41/500
 - 79s - loss: 0.5324 - acc: 0.8009 - val_loss: 1.6095 - val_acc: 0.1946
Epoch 42/500
 - 79s - loss: 0.5243 - acc: 0.8133 - val_loss: 1.6094 - val_acc: 0.1946
Epoch 00042: early stopping
Saved mcbn_model to disk
 0/50 [..............................] - ETA: 0s 1/50 [..............................] - ETA: 13:29 2/50 [>.............................] - ETA: 9:28  3/50 [>.............................] - ETA: 8:01 4/50 [=>............................] - ETA: 7:15 5/50 [==>...........................] - ETA: 6:44 6/50 [==>...........................] - ETA: 6:21 7/50 [===>..........................] - ETA: 6:02 8/50 [===>..........................] - ETA: 5:47 9/50 [====>.........................] - ETA: 5:3310/50 [=====>........................] - ETA: 5:2111/50 [=====>........................] - ETA: 5:0912/50 [======>.......................] - ETA: 4:5913/50 [======>.......................] - ETA: 4:4814/50 [=======>......................] - ETA: 4:3915/50 [========>.....................] - ETA: 4:2916/50 [========>.....................] - ETA: 4:2017/50 [=========>....................] - ETA: 4:1118/50 [=========>....................] - ETA: 4:0319/50 [==========>...................] - ETA: 3:5420/50 [===========>..................] - ETA: 3:4621/50 [===========>..................] - ETA: 3:3822/50 [============>.................] - ETA: 3:3023/50 [============>.................] - ETA: 3:2224/50 [=============>................] - ETA: 3:1425/50 [==============>...............] - ETA: 3:0626/50 [==============>...............] - ETA: 2:5827/50 [===============>..............] - ETA: 2:5028/50 [===============>..............] - ETA: 2:4329/50 [================>.............] - ETA: 2:3530/50 [=================>............] - ETA: 2:2731/50 [=================>............] - ETA: 2:2032/50 [==================>...........] - ETA: 2:1233/50 [==================>...........] - ETA: 2:0534/50 [===================>..........] - ETA: 1:5735/50 [====================>.........] - ETA: 1:5036/50 [====================>.........] - ETA: 1:4237/50 [=====================>........] - ETA: 1:3538/50 [=====================>........] - ETA: 1:2739/50 [======================>.......] - ETA: 1:2040/50 [=======================>......] - ETA: 1:1341/50 [=======================>......] - ETA: 1:0542/50 [========================>.....] - ETA: 58s 43/50 [========================>.....] - ETA: 51s44/50 [=========================>....] - ETA: 43s45/50 [==========================>...] - ETA: 36s46/50 [==========================>...] - ETA: 29s47/50 [===========================>..] - ETA: 21s48/50 [===========================>..] - ETA: 14s49/50 [============================>.] - ETA: 7s 2020-02-11 19:22:47.765482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-02-11 19:22:47.765604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-11 19:22:47.765624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-02-11 19:22:47.765639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-02-11 19:22:47.765753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28671 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
/software/software/h5py/2.7.1-fosscuda-2018a-Python-3.6.4/lib/python3.6/site-packages/h5py-2.7.1-py3.6-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
MCBN accuracy: 19.5%
MCBN-ensemble accuracy: 19.5%
[[  0   0 243   2   0]
 [  0   0 283   2   0]
 [  0   0 270   1   0]
 [  0   0 302   3   0]
 [  0   0 286   7   0]]
posterior mean: 2
true label: 0

class: 0; proba: 6.9%; var: 0.00% 
class: 1; proba: 14.5%; var: 0.00% 
class: 2; proba: 40.6%; var: 0.00% 
class: 3; proba: 27.3%; var: 0.00% 
class: 4; proba: 10.7%; var: 0.00% 


###############################################################################
Peregrine Cluster
Job 9512308 for user 's2934833'
Finished at: Tue Feb 11 19:22:51 CET 2020

Job details:
============

Name                : mcbn_astro
User                : s2934833
Partition           : gpu
Nodes               : pg-gpu26
Cores               : 12
State               : COMPLETED
Submit              : 2020-02-11T13:32:46
Start               : 2020-02-11T18:20:28
End                 : 2020-02-11T19:22:51
Reserved walltime   : 04:00:00
Used walltime       : 01:02:23
Used CPU time       : 00:52:59 (efficiency:  7.08%)
% User (Computation): 70.22%
% System (I/O)      : 29.78%
Mem reserved        : 62.50G/node
Max Mem used        : 10.38G (pg-gpu26)
Max Disk Write      : 184.32K (pg-gpu26)
Max Disk Read       : 5.47M (pg-gpu26)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
