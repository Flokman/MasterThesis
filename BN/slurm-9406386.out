
The following have been reloaded with a version change:
  1) FFTW/3.3.7-gompic-2018a => FFTW/3.3.7-gompi-2018a
  2) OpenMPI/2.1.2-gcccuda-2018a => OpenMPI/2.1.2-GCC-6.4.0-2.28
  3) Python/3.6.4-fosscuda-2018a => Python/3.6.4-foss-2018a
  4) ScaLAPACK/2.0.2-gompic-2018a-OpenBLAS-0.2.20 => ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20

2020-01-29 14:12:02.318619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-29 14:12:02.319166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 31.87GiB freeMemory: 29.56GiB
2020-01-29 14:12:02.319213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-29 14:12:02.823241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-29 14:12:02.823329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-29 14:12:02.823346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-29 14:12:02.823453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28669 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-01-29 14:12:02.824750: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Random seed for replication: 627
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 64, num_classes = 5, epochs = 150, amount_of_predictions = 10, MCBN_batch_size = 64, test_img_idx = 722, train_test_split = 0.8, to_shuffle = True, augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463], label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 0.0001, add_bn_inside = False, train_all_layers = False, weights_to_use = imagenet
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7ff9b135e518> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff9b0ded748> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ffa1470c0b8> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff9b0deda58> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff9b0dd1fd0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff9b0da5b70> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff9b0d3ecc0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff9b0d3e3c8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff9b0d8bcc0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff9b0cdbef0> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff9b0cea908> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff96eb57080> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff96eb69fd0> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff96eb8eb00> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff96eb1bcf8> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff96eb1bf28> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff96eb47828> False
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff96eb56978> False
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff96eaf9e80> False
<tensorflow.python.keras.layers.core.Flatten object at 0x7ff96eaad828> True
<tensorflow.python.keras.layers.core.Dense object at 0x7ff96c1f5128> True
<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7ff96ea8a7b8> True
<tensorflow.python.keras.layers.core.Dense object at 0x7ff96ea8ab70> True
<tensorflow.python.keras.layers.normalization.BatchNormalization object at 0x7ff96ea26630> True
<tensorflow.python.keras.layers.core.Dense object at 0x7ff96ea26d68> True
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
batch_normalization (BatchNo (None, 4096)              16384     
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
batch_normalization_1 (Batch (None, 4096)              16384     
_________________________________________________________________
predictions (Dense)          (None, 5)                 20485     
=================================================================
Total params: 165,771,077
Trainable params: 151,040,005
Non-trainable params: 14,731,072
_________________________________________________________________
Start fitting monte carlo batch_normalization model
Train on 5593 samples, validate on 1399 samples
Epoch 1/150
 - 28s - loss: 2.1156 - acc: 0.4055 - val_loss: 1.4667 - val_acc: 0.4446
Epoch 2/150
 - 21s - loss: 1.1978 - acc: 0.5437 - val_loss: 1.3915 - val_acc: 0.4582
Epoch 3/150
 - 21s - loss: 0.9756 - acc: 0.6224 - val_loss: 1.3224 - val_acc: 0.5254
Epoch 4/150
 - 21s - loss: 0.8655 - acc: 0.6716 - val_loss: 1.2451 - val_acc: 0.5611
Epoch 5/150
 - 21s - loss: 0.7306 - acc: 0.7306 - val_loss: 1.4670 - val_acc: 0.5218
Epoch 6/150
 - 21s - loss: 0.6519 - acc: 0.7533 - val_loss: 1.2354 - val_acc: 0.5740
Epoch 7/150
 - 22s - loss: 0.5403 - acc: 0.8030 - val_loss: 1.4363 - val_acc: 0.5661
Epoch 8/150
 - 21s - loss: 0.5013 - acc: 0.8149 - val_loss: 1.4424 - val_acc: 0.5740
Epoch 9/150
 - 21s - loss: 0.4534 - acc: 0.8346 - val_loss: 1.4075 - val_acc: 0.5804
Epoch 10/150
 - 21s - loss: 0.3697 - acc: 0.8598 - val_loss: 1.4527 - val_acc: 0.5861
Epoch 11/150
 - 21s - loss: 0.3275 - acc: 0.8781 - val_loss: 1.4040 - val_acc: 0.5919
Epoch 12/150
 - 21s - loss: 0.3028 - acc: 0.8874 - val_loss: 1.4419 - val_acc: 0.6026
Epoch 13/150
 - 21s - loss: 0.2677 - acc: 0.9029 - val_loss: 1.5353 - val_acc: 0.6011
Epoch 14/150
 - 21s - loss: 0.2878 - acc: 0.8956 - val_loss: 1.5838 - val_acc: 0.5947
Epoch 15/150
 - 21s - loss: 0.2731 - acc: 0.9054 - val_loss: 1.5812 - val_acc: 0.6197
Epoch 16/150
 - 21s - loss: 0.2419 - acc: 0.9144 - val_loss: 1.6014 - val_acc: 0.5976
Epoch 17/150
 - 21s - loss: 0.2368 - acc: 0.9133 - val_loss: 1.6424 - val_acc: 0.6040
Epoch 18/150
 - 21s - loss: 0.2222 - acc: 0.9194 - val_loss: 1.7276 - val_acc: 0.6104
Epoch 19/150
 - 21s - loss: 0.2127 - acc: 0.9296 - val_loss: 1.5324 - val_acc: 0.6497
Epoch 20/150
 - 21s - loss: 0.1712 - acc: 0.9401 - val_loss: 1.6034 - val_acc: 0.6440
Epoch 21/150
 - 21s - loss: 0.1605 - acc: 0.9412 - val_loss: 1.5325 - val_acc: 0.6469
Epoch 22/150
 - 21s - loss: 0.1882 - acc: 0.9349 - val_loss: 1.5504 - val_acc: 0.6447
Epoch 23/150
 - 21s - loss: 0.1150 - acc: 0.9591 - val_loss: 1.8382 - val_acc: 0.6226
Epoch 24/150
 - 21s - loss: 0.1381 - acc: 0.9557 - val_loss: 1.5608 - val_acc: 0.6598
Epoch 25/150
 - 21s - loss: 0.1590 - acc: 0.9444 - val_loss: 1.5767 - val_acc: 0.6247
Epoch 26/150
 - 21s - loss: 0.1339 - acc: 0.9551 - val_loss: 1.7380 - val_acc: 0.6383
Epoch 27/150
 - 21s - loss: 0.1869 - acc: 0.9374 - val_loss: 1.7134 - val_acc: 0.6333
slurmstepd: error: *** JOB 9406386 ON pg-gpu21 CANCELLED AT 2020-01-29T14:22:11 DUE TO TIME LIMIT ***


###############################################################################
Peregrine Cluster
Job 9406386 for user 's2934833'
Finished at: Wed Jan 29 14:22:12 CET 2020

Job details:
============

Name                : MCBNMessidor2
User                : s2934833
Partition           : gpu
Nodes               : pg-gpu21
Cores               : 12
State               : CANCELLED,TIMEOUT
Submit              : 2020-01-29T13:48:28
Start               : 2020-01-29T14:11:43
End                 : 2020-01-29T14:22:12
Reserved walltime   : 00:10:00
Used walltime       : 00:10:29
Used CPU time       : 00:04:42 (efficiency:  3.74%)
% User (Computation): 78.47%
% System (I/O)      : 21.53%
Mem reserved        : 62.50G/node
Max Mem used        : 8.62G (pg-gpu21)
Max Disk Write      : 184.32K (pg-gpu21)
Max Disk Read       : 6.44M (pg-gpu21)


Acknowledgements:
=================

Please see this page if you want to acknowledge Peregrine in your publications:

https://redmine.hpc.rug.nl/redmine/projects/peregrine/wiki/ScientificOutput

################################################################################
