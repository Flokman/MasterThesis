
The following have been reloaded with a version change:
  1) FFTW/3.3.7-gompic-2018a => FFTW/3.3.7-gompi-2018a
  2) OpenMPI/2.1.2-gcccuda-2018a => OpenMPI/2.1.2-GCC-6.4.0-2.28
  3) Python/3.6.4-fosscuda-2018a => Python/3.6.4-foss-2018a
  4) ScaLAPACK/2.0.2-gompic-2018a-OpenBLAS-0.2.20 => ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20

2020-01-21 16:31:49.409055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-21 16:31:49.409716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 31.87GiB freeMemory: 29.56GiB
2020-01-21 16:31:49.409782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 16:31:50.022929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 16:31:50.023071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 16:31:50.023090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 16:31:50.023243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28669 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-01-21 16:31:50.024601: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 64, num_classes = 5, epochs = 500, MCDO_amount_of_predictions = 500, MCDO_batch_size = 250, test_img_idx = 274, train_test_split = 0.8, to_shuffle = True, augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463], label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 0.0001, add_dropout_inside = False, train_all_layers = True, weights_to_use = None
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
dropout (Dropout)            (None, 4096)              0         
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
predictions (Dense)          (None, 5)                 20485     
=================================================================
Total params: 165,738,309
Trainable params: 165,738,309
Non-trainable params: 0
_________________________________________________________________
Start fitting monte carlo dropout model
Train on 5593 samples, validate on 1399 samples
Epoch 1/500
 - 56s - loss: 1.6070 - acc: 0.2199 - val_loss: 1.6089 - val_acc: 0.2144
Epoch 2/500
 - 46s - loss: 1.6090 - acc: 0.2072 - val_loss: 1.6069 - val_acc: 0.2116
Epoch 3/500
 - 46s - loss: 1.6092 - acc: 0.2240 - val_loss: 1.6060 - val_acc: 0.2373
Epoch 4/500
 - 46s - loss: 1.5322 - acc: 0.2939 - val_loss: 1.4632 - val_acc: 0.3310
Epoch 5/500
 - 48s - loss: 1.4014 - acc: 0.3805 - val_loss: 1.3668 - val_acc: 0.4031
Epoch 6/500
 - 46s - loss: 1.3295 - acc: 0.4166 - val_loss: 1.3351 - val_acc: 0.4103
Epoch 7/500
 - 47s - loss: 1.2630 - acc: 0.4509 - val_loss: 1.2913 - val_acc: 0.4196
Epoch 8/500
 - 46s - loss: 1.2243 - acc: 0.4604 - val_loss: 1.2487 - val_acc: 0.4646
Epoch 9/500
 - 46s - loss: 1.1705 - acc: 0.5072 - val_loss: 1.2297 - val_acc: 0.4853
Epoch 10/500
 - 46s - loss: 1.1035 - acc: 0.5292 - val_loss: 1.1777 - val_acc: 0.4932
Epoch 11/500
 - 46s - loss: 1.0372 - acc: 0.5736 - val_loss: 1.0890 - val_acc: 0.5482
Epoch 12/500
 - 46s - loss: 0.9360 - acc: 0.6202 - val_loss: 1.1258 - val_acc: 0.5611
Epoch 13/500
 - 46s - loss: 0.8366 - acc: 0.6678 - val_loss: 1.0567 - val_acc: 0.5826
Epoch 14/500
 - 46s - loss: 0.7125 - acc: 0.7225 - val_loss: 1.0241 - val_acc: 0.6119
Epoch 15/500
 - 46s - loss: 0.6073 - acc: 0.7620 - val_loss: 0.9789 - val_acc: 0.6369
Epoch 16/500
 - 46s - loss: 0.4991 - acc: 0.8062 - val_loss: 0.9551 - val_acc: 0.6562
Epoch 17/500
 - 46s - loss: 0.3850 - acc: 0.8536 - val_loss: 1.2402 - val_acc: 0.6319
Epoch 18/500
 - 46s - loss: 0.3254 - acc: 0.8804 - val_loss: 1.3564 - val_acc: 0.6576
Epoch 19/500
 - 46s - loss: 0.2480 - acc: 0.9081 - val_loss: 1.2576 - val_acc: 0.6698
Epoch 20/500
 - 46s - loss: 0.2135 - acc: 0.9237 - val_loss: 1.2348 - val_acc: 0.6783
Epoch 21/500
 - 46s - loss: 0.1662 - acc: 0.9396 - val_loss: 1.3561 - val_acc: 0.6741
Epoch 22/500
 - 46s - loss: 0.1211 - acc: 0.9562 - val_loss: 1.2901 - val_acc: 0.6812
Epoch 23/500
 - 47s - loss: 0.1151 - acc: 0.9589 - val_loss: 1.3837 - val_acc: 0.6833
Epoch 24/500
 - 46s - loss: 0.1260 - acc: 0.9573 - val_loss: 1.7055 - val_acc: 0.6526
Epoch 25/500
 - 46s - loss: 0.0855 - acc: 0.9737 - val_loss: 1.3887 - val_acc: 0.6769
