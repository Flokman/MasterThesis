
The following have been reloaded with a version change:
  1) FFTW/3.3.7-gompic-2018a => FFTW/3.3.7-gompi-2018a
  2) OpenMPI/2.1.2-gcccuda-2018a => OpenMPI/2.1.2-GCC-6.4.0-2.28
  3) Python/3.6.4-fosscuda-2018a => Python/3.6.4-foss-2018a
  4) ScaLAPACK/2.0.2-gompic-2018a-OpenBLAS-0.2.20 => ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20

2020-01-21 16:10:02.178819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-01-21 16:10:02.179539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 31.87GiB freeMemory: 29.56GiB
2020-01-21 16:10:02.179605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-01-21 16:10:02.751065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-01-21 16:10:02.751196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-01-21 16:10:02.751216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-01-21 16:10:02.751374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28669 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-01-21 16:10:02.752930: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 128, num_classes = 5, epochs = 500, MCDO_amount_of_predictions = 500, MCDO_batch_size = 250, test_img_idx = 1121, train_test_split = 0.8, to_shuffle = True, augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463], label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 0.0001, add_dropout_inside = False, train_all_layers = True, weights_to_use = None
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
dropout (Dropout)            (None, 4096)              0         
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
predictions (Dense)          (None, 5)                 20485     
=================================================================
Total params: 165,738,309
Trainable params: 165,738,309
Non-trainable params: 0
_________________________________________________________________
Start fitting monte carlo dropout model
Train on 5593 samples, validate on 1399 samples
Epoch 1/500
 - 63s - loss: 1.6066 - acc: 0.2058 - val_loss: 1.6004 - val_acc: 0.2473
Epoch 2/500
 - 43s - loss: 1.5950 - acc: 0.2455 - val_loss: 1.5588 - val_acc: 0.2580
Epoch 3/500
 - 43s - loss: 1.5087 - acc: 0.3376 - val_loss: 1.4164 - val_acc: 0.3788
Epoch 4/500
 - 43s - loss: 1.3592 - acc: 0.4059 - val_loss: 1.3711 - val_acc: 0.4167
Epoch 5/500
 - 43s - loss: 1.3091 - acc: 0.4330 - val_loss: 1.3056 - val_acc: 0.4482
Epoch 6/500
 - 43s - loss: 1.2641 - acc: 0.4574 - val_loss: 1.2644 - val_acc: 0.4510
Epoch 7/500
 - 43s - loss: 1.2066 - acc: 0.4872 - val_loss: 1.2111 - val_acc: 0.4746
Epoch 8/500
 - 43s - loss: 1.1714 - acc: 0.5090 - val_loss: 1.2285 - val_acc: 0.4632
Epoch 9/500
 - 44s - loss: 1.0926 - acc: 0.5512 - val_loss: 1.2039 - val_acc: 0.5046
Epoch 10/500
 - 44s - loss: 1.0153 - acc: 0.5834 - val_loss: 1.1090 - val_acc: 0.5482
Epoch 11/500
 - 44s - loss: 0.9431 - acc: 0.6265 - val_loss: 1.0620 - val_acc: 0.5668
Epoch 12/500
 - 44s - loss: 0.8427 - acc: 0.6601 - val_loss: 1.0236 - val_acc: 0.5904
Epoch 13/500
 - 44s - loss: 0.7620 - acc: 0.6962 - val_loss: 1.0173 - val_acc: 0.5890
Epoch 14/500
 - 44s - loss: 0.6471 - acc: 0.7520 - val_loss: 1.0486 - val_acc: 0.6254
Epoch 15/500
 - 44s - loss: 0.5536 - acc: 0.7876 - val_loss: 0.9844 - val_acc: 0.6376
Epoch 16/500
 - 44s - loss: 0.4384 - acc: 0.8339 - val_loss: 0.9777 - val_acc: 0.6562
Epoch 17/500
 - 44s - loss: 0.3941 - acc: 0.8648 - val_loss: 1.2351 - val_acc: 0.6705
Epoch 18/500
 - 44s - loss: 0.2787 - acc: 0.8999 - val_loss: 1.2631 - val_acc: 0.6783
Epoch 19/500
 - 44s - loss: 0.2609 - acc: 0.9038 - val_loss: 1.1571 - val_acc: 0.6769
Epoch 20/500
 - 44s - loss: 0.2012 - acc: 0.9285 - val_loss: 1.6648 - val_acc: 0.6619
Epoch 21/500
 - 44s - loss: 0.1705 - acc: 0.9372 - val_loss: 1.3355 - val_acc: 0.6934
Epoch 22/500
 - 44s - loss: 0.1764 - acc: 0.9392 - val_loss: 1.2886 - val_acc: 0.6912
Epoch 23/500
 - 44s - loss: 0.1371 - acc: 0.9524 - val_loss: 1.4013 - val_acc: 0.6819
Epoch 24/500
 - 44s - loss: 0.1172 - acc: 0.9576 - val_loss: 1.4299 - val_acc: 0.6783
Epoch 25/500
 - 44s - loss: 0.1335 - acc: 0.9592 - val_loss: 1.6327 - val_acc: 0.6583
Epoch 26/500
 - 44s - loss: 0.1080 - acc: 0.9664 - val_loss: 1.5592 - val_acc: 0.7084
Epoch 27/500
 - 44s - loss: 0.0621 - acc: 0.9794 - val_loss: 1.5511 - val_acc: 0.6848
Epoch 28/500
 - 44s - loss: 0.0912 - acc: 0.9746 - val_loss: 1.3499 - val_acc: 0.6926
Epoch 29/500
 - 44s - loss: 0.0654 - acc: 0.9802 - val_loss: 1.5912 - val_acc: 0.6948
Epoch 30/500
 - 45s - loss: 0.0513 - acc: 0.9869 - val_loss: 1.4442 - val_acc: 0.7084
Epoch 31/500
 - 44s - loss: 0.0369 - acc: 0.9889 - val_loss: 1.6187 - val_acc: 0.7091
Epoch 32/500
 - 44s - loss: 0.0435 - acc: 0.9878 - val_loss: 1.5073 - val_acc: 0.7155
Epoch 33/500
 - 44s - loss: 0.0427 - acc: 0.9857 - val_loss: 1.8321 - val_acc: 0.6984
Epoch 34/500
 - 44s - loss: 0.0594 - acc: 0.9827 - val_loss: 1.6041 - val_acc: 0.6826
Epoch 35/500
 - 45s - loss: 0.0544 - acc: 0.9843 - val_loss: 1.6753 - val_acc: 0.6876
Epoch 36/500
 - 44s - loss: 0.0419 - acc: 0.9877 - val_loss: 1.4661 - val_acc: 0.7112
Epoch 37/500
 - 44s - loss: 0.0358 - acc: 0.9891 - val_loss: 1.5442 - val_acc: 0.7005
Epoch 38/500
 - 43s - loss: 0.0546 - acc: 0.9839 - val_loss: 1.5506 - val_acc: 0.7026
Epoch 39/500
 - 43s - loss: 0.0333 - acc: 0.9903 - val_loss: 1.6198 - val_acc: 0.6955
Epoch 40/500
 - 43s - loss: 0.0287 - acc: 0.9902 - val_loss: 1.6227 - val_acc: 0.6984
Epoch 41/500
 - 43s - loss: 0.0338 - acc: 0.9896 - val_loss: 1.6845 - val_acc: 0.7041
Epoch 42/500
 - 43s - loss: 0.0271 - acc: 0.9900 - val_loss: 1.5431 - val_acc: 0.7084
Epoch 43/500
 - 44s - loss: 0.0245 - acc: 0.9920 - val_loss: 1.5860 - val_acc: 0.7177
Epoch 44/500
 - 43s - loss: 0.0273 - acc: 0.9905 - val_loss: 1.4846 - val_acc: 0.7069
Epoch 45/500
 - 43s - loss: 0.0628 - acc: 0.9825 - val_loss: 1.5216 - val_acc: 0.7019
Epoch 46/500
 - 43s - loss: 0.0262 - acc: 0.9903 - val_loss: 1.6439 - val_acc: 0.7169
Epoch 47/500
 - 43s - loss: 0.0250 - acc: 0.9925 - val_loss: 1.6018 - val_acc: 0.7084
Epoch 48/500
 - 43s - loss: 0.0234 - acc: 0.9914 - val_loss: 1.5638 - val_acc: 0.7227
Epoch 49/500
 - 43s - loss: 0.0261 - acc: 0.9903 - val_loss: 1.4631 - val_acc: 0.7241
Epoch 50/500
 - 43s - loss: 0.0201 - acc: 0.9916 - val_loss: 1.4893 - val_acc: 0.7105
Epoch 51/500
 - 43s - loss: 0.0216 - acc: 0.9909 - val_loss: 1.7389 - val_acc: 0.7048
Epoch 52/500
 - 43s - loss: 0.0192 - acc: 0.9925 - val_loss: 1.4206 - val_acc: 0.7177
Epoch 53/500
 - 43s - loss: 0.0153 - acc: 0.9939 - val_loss: 1.5963 - val_acc: 0.7119
Epoch 54/500
 - 43s - loss: 0.0166 - acc: 0.9923 - val_loss: 1.3167 - val_acc: 0.7255
Epoch 55/500
 - 43s - loss: 0.0143 - acc: 0.9928 - val_loss: 1.5965 - val_acc: 0.7148
Epoch 56/500
 - 43s - loss: 0.0173 - acc: 0.9923 - val_loss: 1.3317 - val_acc: 0.7341
