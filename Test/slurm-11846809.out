
The following have been reloaded with a version change:
  1) FFTW/3.3.8-gompic-2019b => FFTW/3.3.8-gompi-2019b
  2) OpenMPI/3.1.4-gcccuda-2019b => OpenMPI/3.1.4-GCC-8.3.0
  3) ScaLAPACK/2.0.2-gompic-2019b => ScaLAPACK/2.0.2-gompi-2019b
  4) SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4 => SciPy-bundle/2019.10-foss-2019b-Python-3.7.4


The following have been reloaded with a version change:
  1) SciPy-bundle/2019.10-foss-2019b-Python-3.7.4 => SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4

--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[63403,1],0] (PID 20785)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------
2020-05-31 20:33:04.985290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1

START Ensemble
Dataset_name: CIFAR10, N_folders: 1, total mebers: 40
shape preditctions:  (42, 8750, 10)
0/42
1/42
2/42
3/42
4/42
5/42
6/42
7/42
8/42
9/42
10/42
11/42
12/42
13/42
14/42
15/42
16/42
17/42
18/42
19/42
20/42
21/42
22/42
23/42
24/42
25/42
26/42
27/42
28/42
29/42
30/42
31/42
32/42
33/42
34/42
35/42
36/42
37/42
38/42
39/42
40/42
41/42
Correct: 6908, wrong: 1842, accuracy: 78.94857142857143%

Mean probability on true label of CIFAR10 test dataset when correctly predicted = 84.64%
Process Process-1:
Traceback (most recent call last):
  File "/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
    self.run()
  File "/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/multiprocessing/process.py", line 99, in run
    self._target(*self._args, **self._kwargs)
  File "use_Arrays.py", line 49, in with_profiling
    ret = fn(*args, **kwargs)
  File "use_Arrays.py", line 1009, in Ensemble
    method_main()
  File "use_Arrays.py", line 996, in method_main
    test_on_own_funcV2(METHODNAME, Ensemble_own_predictions, y_test)
  File "use_Arrays.py", line 544, in test_on_own_funcV2
    print("Mean uncertainty on true label of {} test dataset when correctly predicted = {:.2%}".format(DATANAME, mean(correct_unc)))
  File "/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/statistics.py", line 311, in mean
    T, total, count = _sum(data)
  File "/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/statistics.py", line 147, in _sum
    for n,d in map(_exact_ratio, values):
  File "/software/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/statistics.py", line 229, in _exact_ratio
    raise TypeError(msg.format(type(x).__name__))
TypeError: can't convert type 'ndarray' to numerator/denominator

#################################
Function main called 1 times. 
Execution time max: 405.854, average: 405.854
#################################


###############################################################################
Peregrine Cluster
Job 11846809 for user 's2934833'
Finished at: Sun May 31 20:39:55 CEST 2020

Job details:
============

Name                : CombArrays
User                : s2934833
Partition           : gpu
Nodes               : pg-gpu04
Cores               : 12
State               : COMPLETED
Submit              : 2020-05-31T20:32:47
Start               : 2020-05-31T20:33:00
End                 : 2020-05-31T20:39:55
Reserved walltime   : 04:00:00
Used walltime       : 00:06:55
Used CPU time       : 00:06:51 (efficiency:  8.27%)
% User (Computation): 99.20%
% System (I/O)      :  0.80%
Mem reserved        : 8000M/node
Max Mem used        : 463.86M (pg-gpu04)
Max Disk Write      : 194.56K (pg-gpu04)
Max Disk Read       : 7.13M (pg-gpu04)
Average GPU usage   : 0.0% (pg-gpu04)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
