Random seed for replication: 146
Random seed for replication: 414
dataset_name = /Polar_PNG_256.hdf5, batch_size = 16, num_classes = 3, epochs = 150,
        MCBN_PREDICTIONS = 250, Mini_batch_size = 15, test_img_idx = 4,
        train_test_split = 0.8, to_shuffle = True, augmentation = False, train_label_count = [42, 22, 120],
        test_label_count = [14, 7, 33], label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 0.01,
        add_bn_inside = True, train_all_layers = False, weights_to_use = imagenet,
        es_patience = 50, train_val_split = 0.9
x_train shape: (184, 256, 256, 3)
184 train samples
54 test samples
block1_conv1
1
block1_conv2
3
block2_conv1
6
block2_conv2
8
block3_conv1
11
block3_conv2
13
block3_conv3
15
block4_conv1
18
block4_conv2
20
block4_conv3
22
block5_conv1
25
block5_conv2
27
block5_conv3
29
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f7b1bf00a20> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b23ecccc0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b0ec6fdd8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b12fb5da0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b0ebe98d0> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f7b12fb5a20> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b12fec2b0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b0eb6f160> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0ec94b70> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b005cd240> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f7b0ec94dd8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0ec9c5c0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b005042b0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0eca8908> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b0045d9b0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0eca8ba8> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b00361358> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f7b0ecb1fd0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0ec3be10> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b00202978> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0ec3beb8> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7b000c7c50> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0ec42fd0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7aac05f2b0> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f7b0ec4ac88> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0ec4a278> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7a846cafd0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0ec56d68> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7a845747f0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f7b0ec56d30> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7a8433feb8> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f7b0ec5cd68> True
<tensorflow.python.keras.layers.core.Flatten object at 0x7f7a841a7f60> True
<tensorflow.python.keras.layers.core.Dense object at 0x7f7a841a7fd0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7a6074c898> True
<tensorflow.python.keras.layers.core.Dense object at 0x7f7a6074c518> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f7a60758208> True
<tensorflow.python.keras.layers.core.Dense object at 0x7f7a6070c048> True
Model: "model_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 256, 256, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
batch_normalization (BatchNo (None, 256, 256, 64)      256       
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
batch_normalization_1 (Batch (None, 256, 256, 64)      256       
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
batch_normalization_2 (Batch (None, 128, 128, 128)     512       
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
batch_normalization_3 (Batch (None, 128, 128, 128)     512       
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
batch_normalization_4 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
batch_normalization_5 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
batch_normalization_6 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
batch_normalization_7 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
batch_normalization_8 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
batch_normalization_9 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_10 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_11 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
batch_normalization_13 (Batc (None, 4096)              16384     
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
batch_normalization_14 (Batc (None, 4096)              16384     
_________________________________________________________________
predictions (Dense)          (None, 3)                 12291     
=================================================================
Total params: 165,779,779
Trainable params: 165,754,947
Non-trainable params: 24,832
_________________________________________________________________
Start fitting monte carlo batch_normalization model
Train for 11 steps, validate for 2 steps
Epoch 1/150
11/11 - 9s - loss: 17.9894 - accuracy: 0.4303 - val_loss: 10.3311 - val_accuracy: 0.4211
Epoch 2/150
11/11 - 4s - loss: 13.0395 - accuracy: 0.4424 - val_loss: 1.3331 - val_accuracy: 0.6842
Epoch 3/150
11/11 - 4s - loss: 7.2585 - accuracy: 0.4303 - val_loss: 1.1937 - val_accuracy: 0.5789
Epoch 4/150
11/11 - 4s - loss: 5.3333 - accuracy: 0.5273 - val_loss: 3.1699 - val_accuracy: 0.5263
Epoch 5/150
11/11 - 4s - loss: 3.9228 - accuracy: 0.5576 - val_loss: 4.6337 - val_accuracy: 0.5263
Epoch 6/150
11/11 - 3s - loss: 4.5390 - accuracy: 0.4667 - val_loss: 3.0222 - val_accuracy: 0.5263
Epoch 7/150
11/11 - 4s - loss: 4.3823 - accuracy: 0.6061 - val_loss: 3.9294 - val_accuracy: 0.4737
Epoch 8/150
11/11 - 4s - loss: 2.5674 - accuracy: 0.6424 - val_loss: 6.6738 - val_accuracy: 0.3684
Epoch 9/150
11/11 - 4s - loss: 4.0196 - accuracy: 0.5394 - val_loss: 5.1403 - val_accuracy: 0.4211
Epoch 10/150
11/11 - 4s - loss: 2.5044 - accuracy: 0.6485 - val_loss: 5.0647 - val_accuracy: 0.5263
Epoch 11/150
11/11 - 4s - loss: 1.8578 - accuracy: 0.7273 - val_loss: 3.3968 - val_accuracy: 0.5263
Epoch 12/150
11/11 - 4s - loss: 1.8873 - accuracy: 0.7697 - val_loss: 4.8297 - val_accuracy: 0.5789
Epoch 13/150
11/11 - 4s - loss: 1.6636 - accuracy: 0.7212 - val_loss: 4.9401 - val_accuracy: 0.5789
Epoch 14/150
11/11 - 4s - loss: 0.9542 - accuracy: 0.8303 - val_loss: 5.9571 - val_accuracy: 0.4737
Epoch 15/150
11/11 - 4s - loss: 1.2316 - accuracy: 0.7879 - val_loss: 4.1839 - val_accuracy: 0.6842
Epoch 16/150
11/11 - 4s - loss: 2.1197 - accuracy: 0.7758 - val_loss: 6.6659 - val_accuracy: 0.5263
Epoch 17/150
11/11 - 4s - loss: 2.1922 - accuracy: 0.8182 - val_loss: 5.8719 - val_accuracy: 0.5789
Epoch 18/150
11/11 - 4s - loss: 2.2154 - accuracy: 0.7576 - val_loss: 2.5624 - val_accuracy: 0.6842
Epoch 19/150
11/11 - 4s - loss: 1.4342 - accuracy: 0.8182 - val_loss: 5.2282 - val_accuracy: 0.5789
Epoch 20/150
11/11 - 4s - loss: 1.1903 - accuracy: 0.8303 - val_loss: 5.3734 - val_accuracy: 0.5263
Epoch 21/150
11/11 - 4s - loss: 1.0077 - accuracy: 0.8909 - val_loss: 4.4583 - val_accuracy: 0.5263
Epoch 22/150
11/11 - 4s - loss: 0.6376 - accuracy: 0.8727 - val_loss: 5.1609 - val_accuracy: 0.5263
Epoch 23/150
11/11 - 4s - loss: 0.8939 - accuracy: 0.8909 - val_loss: 3.6116 - val_accuracy: 0.6316
Epoch 24/150
11/11 - 4s - loss: 0.4789 - accuracy: 0.8909 - val_loss: 5.0057 - val_accuracy: 0.5789
Epoch 25/150
11/11 - 4s - loss: 2.2103 - accuracy: 0.7515 - val_loss: 8.3015 - val_accuracy: 0.6316
Epoch 26/150
11/11 - 4s - loss: 1.0702 - accuracy: 0.7879 - val_loss: 7.4555 - val_accuracy: 0.5789
Epoch 27/150
11/11 - 4s - loss: 0.5265 - accuracy: 0.9091 - val_loss: 5.8043 - val_accuracy: 0.4737
Epoch 28/150
11/11 - 4s - loss: 0.4746 - accuracy: 0.8848 - val_loss: 7.1941 - val_accuracy: 0.5789
Epoch 29/150
11/11 - 4s - loss: 0.8930 - accuracy: 0.8667 - val_loss: 6.3137 - val_accuracy: 0.5789
Epoch 30/150
11/11 - 4s - loss: 0.5485 - accuracy: 0.9030 - val_loss: 3.4915 - val_accuracy: 0.7368
Epoch 31/150
11/11 - 4s - loss: 0.7255 - accuracy: 0.8848 - val_loss: 2.3658 - val_accuracy: 0.7368
Epoch 32/150
11/11 - 4s - loss: 0.4650 - accuracy: 0.8970 - val_loss: 5.5739 - val_accuracy: 0.5789
Epoch 33/150
11/11 - 4s - loss: 0.6864 - accuracy: 0.8970 - val_loss: 3.8643 - val_accuracy: 0.6316
Epoch 34/150
11/11 - 4s - loss: 0.4413 - accuracy: 0.8970 - val_loss: 9.2520 - val_accuracy: 0.4737
Epoch 35/150
11/11 - 4s - loss: 1.1941 - accuracy: 0.9030 - val_loss: 5.4493 - val_accuracy: 0.5789
Epoch 36/150
11/11 - 4s - loss: 0.3167 - accuracy: 0.9515 - val_loss: 2.7084 - val_accuracy: 0.7368
Epoch 37/150
11/11 - 4s - loss: 0.4309 - accuracy: 0.9394 - val_loss: 4.2305 - val_accuracy: 0.7368
Epoch 38/150
11/11 - 4s - loss: 0.5691 - accuracy: 0.9030 - val_loss: 2.5381 - val_accuracy: 0.6316
Epoch 39/150
11/11 - 4s - loss: 0.2761 - accuracy: 0.9333 - val_loss: 2.8035 - val_accuracy: 0.7368
Epoch 40/150
11/11 - 4s - loss: 0.2336 - accuracy: 0.9515 - val_loss: 1.9634 - val_accuracy: 0.7368
Epoch 41/150
11/11 - 4s - loss: 0.1228 - accuracy: 0.9636 - val_loss: 1.7867 - val_accuracy: 0.7895
Epoch 42/150
11/11 - 4s - loss: 0.2920 - accuracy: 0.9455 - val_loss: 3.2021 - val_accuracy: 0.4737
Epoch 43/150
11/11 - 4s - loss: 0.1964 - accuracy: 0.9636 - val_loss: 2.3710 - val_accuracy: 0.7368
Epoch 44/150
11/11 - 4s - loss: 0.2643 - accuracy: 0.9576 - val_loss: 3.1446 - val_accuracy: 0.6316
Epoch 45/150
11/11 - 4s - loss: 0.3561 - accuracy: 0.9273 - val_loss: 3.3277 - val_accuracy: 0.6316
Epoch 46/150
11/11 - 4s - loss: 0.5314 - accuracy: 0.9515 - val_loss: 3.3599 - val_accuracy: 0.6842
Epoch 47/150
11/11 - 4s - loss: 0.9094 - accuracy: 0.8909 - val_loss: 2.3183 - val_accuracy: 0.8421
Epoch 48/150
11/11 - 4s - loss: 0.8121 - accuracy: 0.9273 - val_loss: 5.4029 - val_accuracy: 0.6316
Epoch 49/150
11/11 - 4s - loss: 0.6681 - accuracy: 0.9273 - val_loss: 2.5076 - val_accuracy: 0.6842
Epoch 50/150
11/11 - 4s - loss: 0.3461 - accuracy: 0.9333 - val_loss: 3.8347 - val_accuracy: 0.6316
Epoch 51/150
11/11 - 4s - loss: 0.7580 - accuracy: 0.8848 - val_loss: 5.0423 - val_accuracy: 0.5263
Epoch 52/150
11/11 - 4s - loss: 0.6328 - accuracy: 0.8970 - val_loss: 2.6133 - val_accuracy: 0.6842
Epoch 53/150
11/11 - 4s - loss: 0.4158 - accuracy: 0.9273 - val_loss: 1.9233 - val_accuracy: 0.7895
Epoch 00053: early stopping
input_1 False
block1_conv1 False
batch_normalization True
block1_conv2 False
batch_normalization_1 True
block1_pool False
block2_conv1 False
batch_normalization_2 True
block2_conv2 False
batch_normalization_3 True
block2_pool False
block3_conv1 False
batch_normalization_4 True
block3_conv2 False
batch_normalization_5 True
block3_conv3 False
batch_normalization_6 True
block3_pool False
block4_conv1 False
batch_normalization_7 True
block4_conv2 False
batch_normalization_8 True
block4_conv3 False
batch_normalization_9 True
block4_pool False
block5_conv1 False
batch_normalization_10 True
block5_conv2 False
batch_normalization_11 True
block5_conv3 False
batch_normalization_12 True
block5_pool False
flatten False
fc1 False
batch_normalization_13 True
fc2 False
batch_normalization_14 True
predictions False
  0/250 [..............................] - ETA: 0s  2/250 [..............................] - ETA: 10:28 12/250 [>.............................] - ETA: 3:21  22/250 [=>............................] - ETA: 2:37 32/250 [==>...........................] - ETA: 2:17 42/250 [====>.........................] - ETA: 2:05 52/250 [=====>........................] - ETA: 1:55 62/250 [======>.......................] - ETA: 1:47 72/250 [=======>......................] - ETA: 1:40 82/250 [========>.....................] - ETA: 1:33 92/250 [==========>...................] - ETA: 1:26102/250 [===========>..................] - ETA: 1:20112/250 [============>.................] - ETA: 1:14122/250 [=============>................] - ETA: 1:09132/250 [==============>...............] - ETA: 1:03142/250 [================>.............] - ETA: 57s 152/250 [=================>............] - ETA: 52s162/250 [==================>...........] - ETA: 46s172/250 [===================>..........] - ETA: 41s182/250 [====================>.........] - ETA: 36s192/250 [======================>.......] - ETA: 30s202/250 [=======================>......] - ETA: 25s212/250 [========================>.....] - ETA: 19s222/250 [=========================>....] - ETA: 14s232/250 [==========================>...] - ETA: 9s 242/250 [============================>.] - ETA: 4sMCBN accuracy: 55.8%
MCBN-ensemble accuracy: 57.4%
tf.Tensor(
[[ 5  0  9]
 [ 2  0  5]
 [ 6  1 26]], shape=(3, 3), dtype=int32)
posterior mean: 0
true label: 0

class: 0; proba: 83.5%; var: 32.66% 
class: 1; proba: 0.0%; var: 0.00% 
class: 2; proba: 16.5%; var: 32.66% 
