Random seed for replication: 846
Random seed for replication: 461
augmentation of train set done
augmentation test set done
dataset_name = /Polar_PNG_256.hdf5, batch_size = 16, num_classes = 3, epochs = 150,
        MCBN_PREDICTIONS = 250, Mini_batch_size = 15, test_img_idx = 11,
        train_test_split = 0.8, to_shuffle = True, augmentation = True, train_label_count = [42, 22, 120],
        test_label_count = [14, 7, 33], label_normalizer = False, save_augmentation_to_hdf5 = False, learn rate = 0.001,
        add_bn_inside = True, train_all_layers = False, weights_to_use = imagenet,
        es_patience = 50, train_val_split = 0.9
x_train shape: (184, 256, 256, 3)
184 train samples
54 test samples
block1_conv1
1
block1_conv2
3
block2_conv1
6
block2_conv2
8
block3_conv1
11
block3_conv2
13
block3_conv3
15
block4_conv1
18
block4_conv2
20
block4_conv3
22
block5_conv1
25
block5_conv2
27
block5_conv3
29
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7ff116a75a20> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff11ea42c88> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff1097e6dd8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff10db2bda0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff10975e8d0> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff10db2ba20> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff10db622b0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff1096e4160> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff109809b70> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff10416c240> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff109809dd8> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff1098125c0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff1040a32b0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff10981d908> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f87de9b0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff10981dba8> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f86e1358> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff109827fd0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff1097b0e10> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f8583978> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff1097b0eb8> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f8447c50> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff1097b7fd0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f83142b0> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff1097bfc88> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff1097bf278> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f818efd0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff1097cbd68> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f07f47f0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff1097cbd30> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f05bfeb8> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff1097d1d68> True
<tensorflow.python.keras.layers.core.Flatten object at 0x7ff0f0428f60> True
<tensorflow.python.keras.layers.core.Dense object at 0x7ff0f0428fd0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f020c898> True
<tensorflow.python.keras.layers.core.Dense object at 0x7ff0f020c518> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7ff0f0219208> True
<tensorflow.python.keras.layers.core.Dense object at 0x7ff0f01cd048> True
Model: "model_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 256, 256, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
batch_normalization (BatchNo (None, 256, 256, 64)      256       
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
batch_normalization_1 (Batch (None, 256, 256, 64)      256       
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
batch_normalization_2 (Batch (None, 128, 128, 128)     512       
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
batch_normalization_3 (Batch (None, 128, 128, 128)     512       
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
batch_normalization_4 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
batch_normalization_5 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
batch_normalization_6 (Batch (None, 64, 64, 256)       1024      
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
batch_normalization_7 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
batch_normalization_8 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
batch_normalization_9 (Batch (None, 32, 32, 512)       2048      
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_10 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_11 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 16, 16, 512)       2048      
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
batch_normalization_13 (Batc (None, 4096)              16384     
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
batch_normalization_14 (Batc (None, 4096)              16384     
_________________________________________________________________
predictions (Dense)          (None, 3)                 12291     
=================================================================
Total params: 165,779,779
Trainable params: 165,754,947
Non-trainable params: 24,832
_________________________________________________________________
Start fitting monte carlo batch_normalization model
Train for 11 steps, validate for 2 steps
Epoch 1/150
11/11 - 9s - loss: 10.3480 - accuracy: 0.3879 - val_loss: 4.4307 - val_accuracy: 0.2632
Epoch 2/150
11/11 - 4s - loss: 3.3168 - accuracy: 0.4424 - val_loss: 3.9615 - val_accuracy: 0.3684
Epoch 3/150
11/11 - 4s - loss: 2.0682 - accuracy: 0.4182 - val_loss: 5.4686 - val_accuracy: 0.2632
Epoch 4/150
11/11 - 4s - loss: 1.4222 - accuracy: 0.5333 - val_loss: 6.5816 - val_accuracy: 0.4211
Epoch 5/150
11/11 - 4s - loss: 0.9356 - accuracy: 0.6121 - val_loss: 7.6998 - val_accuracy: 0.3158
Epoch 6/150
11/11 - 4s - loss: 1.1476 - accuracy: 0.6545 - val_loss: 6.3928 - val_accuracy: 0.4737
Epoch 7/150
11/11 - 4s - loss: 0.5379 - accuracy: 0.7576 - val_loss: 10.8385 - val_accuracy: 0.2632
Epoch 8/150
11/11 - 4s - loss: 0.3434 - accuracy: 0.8545 - val_loss: 15.3587 - val_accuracy: 0.2632
Epoch 9/150
11/11 - 4s - loss: 0.2807 - accuracy: 0.8970 - val_loss: 12.1964 - val_accuracy: 0.3684
Epoch 10/150
11/11 - 4s - loss: 0.2124 - accuracy: 0.9212 - val_loss: 11.5284 - val_accuracy: 0.3684
Epoch 11/150
11/11 - 4s - loss: 0.1525 - accuracy: 0.9515 - val_loss: 7.0678 - val_accuracy: 0.4737
Epoch 12/150
11/11 - 4s - loss: 0.0905 - accuracy: 0.9818 - val_loss: 5.0274 - val_accuracy: 0.5263
Epoch 13/150
11/11 - 4s - loss: 0.0529 - accuracy: 0.9939 - val_loss: 3.5311 - val_accuracy: 0.4737
Epoch 14/150
11/11 - 4s - loss: 0.0366 - accuracy: 1.0000 - val_loss: 5.5749 - val_accuracy: 0.5263
Epoch 15/150
11/11 - 4s - loss: 0.0220 - accuracy: 1.0000 - val_loss: 5.6776 - val_accuracy: 0.6316
Epoch 16/150
11/11 - 4s - loss: 0.0159 - accuracy: 1.0000 - val_loss: 5.7582 - val_accuracy: 0.6316
Epoch 17/150
11/11 - 4s - loss: 0.0113 - accuracy: 1.0000 - val_loss: 5.7296 - val_accuracy: 0.5789
Epoch 18/150
11/11 - 4s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 5.6298 - val_accuracy: 0.5789
Epoch 19/150
11/11 - 4s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 5.4016 - val_accuracy: 0.5789
Epoch 20/150
11/11 - 4s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 4.9267 - val_accuracy: 0.5789
Epoch 21/150
11/11 - 4s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 4.1790 - val_accuracy: 0.5263
Epoch 22/150
11/11 - 4s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.8921 - val_accuracy: 0.5263
Epoch 23/150
11/11 - 4s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.5326 - val_accuracy: 0.5263
Epoch 24/150
11/11 - 4s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.3620 - val_accuracy: 0.5789
Epoch 25/150
11/11 - 4s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.1500 - val_accuracy: 0.6842
Epoch 26/150
11/11 - 4s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.9602 - val_accuracy: 0.6316
Epoch 27/150
11/11 - 4s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.8170 - val_accuracy: 0.6316
Epoch 28/150
11/11 - 4s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.6700 - val_accuracy: 0.6842
Epoch 29/150
11/11 - 4s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.5431 - val_accuracy: 0.6842
Epoch 30/150
11/11 - 4s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.3443 - val_accuracy: 0.6316
Epoch 31/150
11/11 - 4s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.2451 - val_accuracy: 0.6316
Epoch 32/150
11/11 - 4s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.0151 - val_accuracy: 0.6842
Epoch 33/150
11/11 - 4s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.8474 - val_accuracy: 0.6316
Epoch 34/150
11/11 - 4s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.6694 - val_accuracy: 0.6316
Epoch 35/150
11/11 - 4s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.4985 - val_accuracy: 0.6842
Epoch 36/150
11/11 - 4s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.3895 - val_accuracy: 0.6842
Epoch 37/150
11/11 - 4s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.2944 - val_accuracy: 0.6842
Epoch 38/150
11/11 - 4s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.2966 - val_accuracy: 0.6842
Epoch 39/150
11/11 - 4s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.2898 - val_accuracy: 0.7368
Epoch 40/150
11/11 - 4s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.2944 - val_accuracy: 0.7368
Epoch 41/150
11/11 - 4s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.3753 - val_accuracy: 0.7368
Epoch 42/150
11/11 - 4s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.4721 - val_accuracy: 0.7368
Epoch 43/150
11/11 - 4s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.5857 - val_accuracy: 0.7368
Epoch 44/150
11/11 - 4s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.6696 - val_accuracy: 0.7368
Epoch 45/150
11/11 - 4s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.7216 - val_accuracy: 0.7368
Epoch 46/150
11/11 - 4s - loss: 9.8251e-04 - accuracy: 1.0000 - val_loss: 1.8526 - val_accuracy: 0.7368
Epoch 47/150
11/11 - 4s - loss: 9.4476e-04 - accuracy: 1.0000 - val_loss: 1.8993 - val_accuracy: 0.7368
Epoch 48/150
11/11 - 4s - loss: 8.9547e-04 - accuracy: 1.0000 - val_loss: 1.9542 - val_accuracy: 0.7368
Epoch 49/150
11/11 - 4s - loss: 8.6022e-04 - accuracy: 1.0000 - val_loss: 2.0295 - val_accuracy: 0.7368
Epoch 50/150
11/11 - 4s - loss: 8.3087e-04 - accuracy: 1.0000 - val_loss: 2.1132 - val_accuracy: 0.7368
Epoch 51/150
11/11 - 4s - loss: 7.9119e-04 - accuracy: 1.0000 - val_loss: 2.1319 - val_accuracy: 0.7368
Epoch 52/150
11/11 - 4s - loss: 7.5994e-04 - accuracy: 1.0000 - val_loss: 2.1309 - val_accuracy: 0.7368
Epoch 53/150
11/11 - 4s - loss: 7.3302e-04 - accuracy: 1.0000 - val_loss: 2.1890 - val_accuracy: 0.7368
Epoch 54/150
11/11 - 4s - loss: 7.0429e-04 - accuracy: 1.0000 - val_loss: 2.1975 - val_accuracy: 0.7368
Epoch 55/150
11/11 - 4s - loss: 6.7795e-04 - accuracy: 1.0000 - val_loss: 2.2226 - val_accuracy: 0.7368
Epoch 56/150
11/11 - 4s - loss: 6.5250e-04 - accuracy: 1.0000 - val_loss: 2.2645 - val_accuracy: 0.7895
Epoch 57/150
11/11 - 4s - loss: 6.3167e-04 - accuracy: 1.0000 - val_loss: 2.2584 - val_accuracy: 0.7895
Epoch 58/150
11/11 - 4s - loss: 6.0948e-04 - accuracy: 1.0000 - val_loss: 2.2552 - val_accuracy: 0.7895
Epoch 59/150
11/11 - 4s - loss: 5.8637e-04 - accuracy: 1.0000 - val_loss: 2.2492 - val_accuracy: 0.7895
Epoch 60/150
11/11 - 4s - loss: 5.6718e-04 - accuracy: 1.0000 - val_loss: 2.2292 - val_accuracy: 0.7895
Epoch 61/150
11/11 - 4s - loss: 5.4700e-04 - accuracy: 1.0000 - val_loss: 2.2259 - val_accuracy: 0.7895
Epoch 62/150
11/11 - 4s - loss: 5.3159e-04 - accuracy: 1.0000 - val_loss: 2.2598 - val_accuracy: 0.7895
Epoch 63/150
11/11 - 4s - loss: 5.1335e-04 - accuracy: 1.0000 - val_loss: 2.2448 - val_accuracy: 0.7895
Epoch 64/150
11/11 - 4s - loss: 4.9710e-04 - accuracy: 1.0000 - val_loss: 2.2421 - val_accuracy: 0.7895
Epoch 65/150
11/11 - 4s - loss: 4.8232e-04 - accuracy: 1.0000 - val_loss: 2.2517 - val_accuracy: 0.7895
Epoch 66/150
11/11 - 4s - loss: 4.6866e-04 - accuracy: 1.0000 - val_loss: 2.2570 - val_accuracy: 0.7895
Epoch 67/150
11/11 - 4s - loss: 4.5244e-04 - accuracy: 1.0000 - val_loss: 2.2594 - val_accuracy: 0.7895
Epoch 68/150
11/11 - 4s - loss: 4.3888e-04 - accuracy: 1.0000 - val_loss: 2.2570 - val_accuracy: 0.7895
Epoch 69/150
11/11 - 4s - loss: 4.2641e-04 - accuracy: 1.0000 - val_loss: 2.2494 - val_accuracy: 0.7895
Epoch 70/150
11/11 - 4s - loss: 4.1532e-04 - accuracy: 1.0000 - val_loss: 2.2481 - val_accuracy: 0.7895
Epoch 71/150
11/11 - 4s - loss: 4.0194e-04 - accuracy: 1.0000 - val_loss: 2.2461 - val_accuracy: 0.7895
Epoch 72/150
11/11 - 4s - loss: 3.9410e-04 - accuracy: 1.0000 - val_loss: 2.2349 - val_accuracy: 0.7895
Epoch 73/150
11/11 - 4s - loss: 3.8007e-04 - accuracy: 1.0000 - val_loss: 2.2486 - val_accuracy: 0.7895
Epoch 74/150
11/11 - 4s - loss: 3.6948e-04 - accuracy: 1.0000 - val_loss: 2.2646 - val_accuracy: 0.7895
Epoch 75/150
11/11 - 4s - loss: 3.6006e-04 - accuracy: 1.0000 - val_loss: 2.2493 - val_accuracy: 0.7895
Epoch 76/150
11/11 - 4s - loss: 3.5085e-04 - accuracy: 1.0000 - val_loss: 2.2519 - val_accuracy: 0.7895
Epoch 77/150
11/11 - 4s - loss: 3.4180e-04 - accuracy: 1.0000 - val_loss: 2.2462 - val_accuracy: 0.7895
Epoch 78/150
11/11 - 4s - loss: 3.3433e-04 - accuracy: 1.0000 - val_loss: 2.2430 - val_accuracy: 0.7895
Epoch 79/150
11/11 - 4s - loss: 3.2492e-04 - accuracy: 1.0000 - val_loss: 2.2407 - val_accuracy: 0.8421
Epoch 80/150
11/11 - 4s - loss: 3.1718e-04 - accuracy: 1.0000 - val_loss: 2.2378 - val_accuracy: 0.7895
Epoch 81/150
11/11 - 4s - loss: 3.0862e-04 - accuracy: 1.0000 - val_loss: 2.2393 - val_accuracy: 0.8421
Epoch 82/150
11/11 - 4s - loss: 3.0141e-04 - accuracy: 1.0000 - val_loss: 2.2442 - val_accuracy: 0.8421
Epoch 83/150
11/11 - 4s - loss: 2.9442e-04 - accuracy: 1.0000 - val_loss: 2.2353 - val_accuracy: 0.7895
Epoch 84/150
11/11 - 4s - loss: 2.8641e-04 - accuracy: 1.0000 - val_loss: 2.2335 - val_accuracy: 0.8421
Epoch 85/150
11/11 - 4s - loss: 2.8071e-04 - accuracy: 1.0000 - val_loss: 2.2511 - val_accuracy: 0.8421
Epoch 86/150
11/11 - 4s - loss: 2.7345e-04 - accuracy: 1.0000 - val_loss: 2.2559 - val_accuracy: 0.8421
Epoch 87/150
11/11 - 4s - loss: 2.6706e-04 - accuracy: 1.0000 - val_loss: 2.2439 - val_accuracy: 0.8421
Epoch 88/150
11/11 - 4s - loss: 2.6112e-04 - accuracy: 1.0000 - val_loss: 2.2451 - val_accuracy: 0.8421
Epoch 89/150
11/11 - 4s - loss: 2.5512e-04 - accuracy: 1.0000 - val_loss: 2.2429 - val_accuracy: 0.8421
Epoch 00089: early stopping
input_1 False
block1_conv1 False
batch_normalization True
block1_conv2 False
batch_normalization_1 True
block1_pool False
block2_conv1 False
batch_normalization_2 True
block2_conv2 False
batch_normalization_3 True
block2_pool False
block3_conv1 False
batch_normalization_4 True
block3_conv2 False
batch_normalization_5 True
block3_conv3 False
batch_normalization_6 True
block3_pool False
block4_conv1 False
batch_normalization_7 True
block4_conv2 False
batch_normalization_8 True
block4_conv3 False
batch_normalization_9 True
block4_pool False
block5_conv1 False
batch_normalization_10 True
block5_conv2 False
batch_normalization_11 True
block5_conv3 False
batch_normalization_12 True
block5_pool False
flatten False
fc1 False
batch_normalization_13 True
fc2 False
batch_normalization_14 True
predictions False
  0/250 [..............................] - ETA: 0s  2/250 [..............................] - ETA: 10:34 12/250 [>.............................] - ETA: 3:22  22/250 [=>............................] - ETA: 2:38 32/250 [==>...........................] - ETA: 2:19 42/250 [====>.........................] - ETA: 2:06 52/250 [=====>........................] - ETA: 1:56 62/250 [======>.......................] - ETA: 1:48 72/250 [=======>......................] - ETA: 1:40 82/250 [========>.....................] - ETA: 1:33 92/250 [==========>...................] - ETA: 1:27102/250 [===========>..................] - ETA: 1:21112/250 [============>.................] - ETA: 1:15122/250 [=============>................] - ETA: 1:09132/250 [==============>...............] - ETA: 1:03142/250 [================>.............] - ETA: 58s 152/250 [=================>............] - ETA: 52s162/250 [==================>...........] - ETA: 47s172/250 [===================>..........] - ETA: 41s182/250 [====================>.........] - ETA: 36s192/250 [======================>.......] - ETA: 30s202/250 [=======================>......] - ETA: 25s212/250 [========================>.....] - ETA: 20s222/250 [=========================>....] - ETA: 14s232/250 [==========================>...] - ETA: 9s 242/250 [============================>.] - ETA: 4sMCBN accuracy: 44.9%
MCBN-ensemble accuracy: 46.3%
tf.Tensor(
[[ 4  1  9]
 [ 1  0  6]
 [ 9  3 21]], shape=(3, 3), dtype=int32)
posterior mean: 0
true label: 2

class: 0; proba: 100.0%; var: 0.00% 
class: 1; proba: 0.0%; var: 0.00% 
class: 2; proba: 0.0%; var: 0.00% 
