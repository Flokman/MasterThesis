
The following have been reloaded with a version change:
  1) FFTW/3.3.8-gompic-2019b => FFTW/3.3.8-gompi-2019b
  2) OpenMPI/3.1.4-gcccuda-2019b => OpenMPI/3.1.4-GCC-8.3.0
  3) ScaLAPACK/2.0.2-gompic-2019b => ScaLAPACK/2.0.2-gompi-2019b
  4) SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4 => SciPy-bundle/2019.10-foss-2019b-Python-3.7.4

2020-03-16 11:10:03.762708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-16 11:10:18.151956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-03-16 11:10:18.161636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:18.162399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GRID V100D-32Q computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.88GiB deviceMemoryBandwidth: 836.37GiB/s
2020-03-16 11:10:18.162476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-16 11:10:18.179613: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-16 11:10:18.189385: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-16 11:10:18.196400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-16 11:10:18.205903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-16 11:10:18.213391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-16 11:10:18.225452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-16 11:10:18.225863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:18.226788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:18.227334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-03-16 11:10:18.236173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:18.236832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: GRID V100D-32Q computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.88GiB deviceMemoryBandwidth: 836.37GiB/s
2020-03-16 11:10:18.236908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-16 11:10:18.236975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-16 11:10:18.236997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-03-16 11:10:18.237017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-03-16 11:10:18.237037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-03-16 11:10:18.237073: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-03-16 11:10:18.237097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-16 11:10:18.237249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:18.238044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:18.239041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-03-16 11:10:18.248277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-03-16 11:10:19.055921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-03-16 11:10:19.056052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-03-16 11:10:19.056069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-03-16 11:10:19.056492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:19.057415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:19.058130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-03-16 11:10:19.058606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28652 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-03-16 11:10:19.059387: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
2020-03-16 11:10:33.099406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-16 11:10:33.474856: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-16 11:10:35.777810: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.
2020-03-16 11:10:35.777958: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1259] Profiler found 1 GPUs
2020-03-16 11:10:35.786775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1
2020-03-16 11:10:35.887723: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_VIRTUALIZED_DEVICE_NOT_SUPPORTED
2020-03-16 11:10:35.888493: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_VIRTUALIZED_DEVICE_NOT_SUPPORTED
2020-03-16 11:10:35.912095: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER
2020-03-16 11:10:35.912242: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.
dataset_name = CIFAR10, batch_size = 32, num_classes = 10, epochs = 150,
        MCBN_PREDICTIONS = 250, Mini_batch_size = 128, test_img_idx = 3809,
        train_test_split = 0.8, to_shuffle = True, augmentation = False,
        label_normalizer = True, save_augmentation_to_hdf5 = True, learn rate = 0.0001,
        add_bn_inside = True, train_all_layers = False, weights_to_use = imagenet,
        es_patience = 50, train_val_split = 0.9
x_train shape: (50000, 32, 32, 3)
50000 train samples
10000 test samples
block1_conv1
1
block1_conv2
3
block2_conv1
6
block2_conv2
8
block3_conv1
11
block3_conv2
13
block3_conv3
15
block4_conv1
18
block4_conv2
20
block4_conv3
22
block5_conv1
25
block5_conv2
27
block5_conv3
29
<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f4823d99510> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4823c80fd0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f4818476750> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4825810150> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f481846aed0> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f48184d9ed0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f48184d9c90> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f48182b3710> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f48184ce750> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f48181cbad0> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f48184fd510> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4818504090> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f4800331710> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f481848af90> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f4800067910> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4818494d50> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b85d8890> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f481849ebd0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f48184a5bd0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b82a3d50> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f48184affd0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b8123350> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f48184af7d0> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b1c0bf90> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f48184c1890> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f481844b110> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b1b334d0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4818450f50> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b17082d0> True
<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f4818454d10> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b107e3d0> True
<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f48184647d0> True
<tensorflow.python.keras.layers.core.Flatten object at 0x7f47b0e67850> True
<tensorflow.python.keras.layers.core.Dense object at 0x7f47b0d24a90> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b0b95090> True
<tensorflow.python.keras.layers.core.Dense object at 0x7f47b0ba3150> True
<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f47b0b98110> True
<tensorflow.python.keras.layers.core.Dense object at 0x7f47b0b74e90> True
Model: "model_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      
_________________________________________________________________
batch_normalization (BatchNo (None, 32, 32, 64)        256       
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 64)        256       
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     
_________________________________________________________________
batch_normalization_2 (Batch (None, 16, 16, 128)       512       
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 128)       512       
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    
_________________________________________________________________
batch_normalization_4 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    
_________________________________________________________________
batch_normalization_5 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    
_________________________________________________________________
batch_normalization_6 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   
_________________________________________________________________
batch_normalization_7 (Batch (None, 4, 4, 512)         2048      
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   
_________________________________________________________________
batch_normalization_8 (Batch (None, 4, 4, 512)         2048      
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   
_________________________________________________________________
batch_normalization_9 (Batch (None, 4, 4, 512)         2048      
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   
_________________________________________________________________
batch_normalization_10 (Batc (None, 2, 2, 512)         2048      
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   
_________________________________________________________________
batch_normalization_11 (Batc (None, 2, 2, 512)         2048      
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   
_________________________________________________________________
batch_normalization_12 (Batc (None, 2, 2, 512)         2048      
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 512)               0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              2101248   
_________________________________________________________________
batch_normalization_13 (Batc (None, 4096)              16384     
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
batch_normalization_14 (Batc (None, 4096)              16384     
_________________________________________________________________
predictions (Dense)          (None, 10)                40970     
=================================================================
Total params: 33,687,882
Trainable params: 33,663,050
Non-trainable params: 24,832
_________________________________________________________________
Start fitting monte carlo batch_normalization model
Train for 1407 steps, validate for 157 steps
Epoch 1/150
1407/1407 - 33s - loss: 1.1607 - accuracy: 0.6661 - val_loss: 0.7134 - val_accuracy: 0.7622
Epoch 2/150
1407/1407 - 23s - loss: 0.6026 - accuracy: 0.8010 - val_loss: 0.5732 - val_accuracy: 0.8122
Epoch 3/150
1407/1407 - 23s - loss: 0.4390 - accuracy: 0.8562 - val_loss: 0.5804 - val_accuracy: 0.8250
Epoch 4/150
1407/1407 - 23s - loss: 0.3361 - accuracy: 0.8885 - val_loss: 0.5830 - val_accuracy: 0.8216
Epoch 5/150
1407/1407 - 23s - loss: 0.2514 - accuracy: 0.9174 - val_loss: 0.5676 - val_accuracy: 0.8482
Epoch 6/150
1407/1407 - 23s - loss: 0.1883 - accuracy: 0.9370 - val_loss: 0.6330 - val_accuracy: 0.8394
Epoch 7/150
1407/1407 - 23s - loss: 0.1510 - accuracy: 0.9508 - val_loss: 0.6174 - val_accuracy: 0.8536
Epoch 8/150
1407/1407 - 23s - loss: 0.1237 - accuracy: 0.9607 - val_loss: 0.6776 - val_accuracy: 0.8532
Epoch 9/150
1407/1407 - 23s - loss: 0.0985 - accuracy: 0.9686 - val_loss: 0.6706 - val_accuracy: 0.8562
Epoch 10/150
1407/1407 - 23s - loss: 0.0839 - accuracy: 0.9744 - val_loss: 0.6747 - val_accuracy: 0.8550
Epoch 11/150
1407/1407 - 23s - loss: 0.0787 - accuracy: 0.9746 - val_loss: 0.6405 - val_accuracy: 0.8598
Epoch 12/150
1407/1407 - 23s - loss: 0.0700 - accuracy: 0.9776 - val_loss: 0.7199 - val_accuracy: 0.8594
Epoch 13/150
1407/1407 - 23s - loss: 0.0594 - accuracy: 0.9812 - val_loss: 0.6691 - val_accuracy: 0.8716
Epoch 14/150
1407/1407 - 23s - loss: 0.0564 - accuracy: 0.9833 - val_loss: 0.6697 - val_accuracy: 0.8654
Epoch 15/150
1407/1407 - 23s - loss: 0.0512 - accuracy: 0.9838 - val_loss: 0.7297 - val_accuracy: 0.8644
Epoch 16/150
1407/1407 - 23s - loss: 0.0499 - accuracy: 0.9842 - val_loss: 0.7684 - val_accuracy: 0.8740
Epoch 17/150
1407/1407 - 23s - loss: 0.0447 - accuracy: 0.9867 - val_loss: 0.7169 - val_accuracy: 0.8662
Epoch 18/150
1407/1407 - 23s - loss: 0.0448 - accuracy: 0.9863 - val_loss: 0.7862 - val_accuracy: 0.8588
Epoch 19/150
1407/1407 - 23s - loss: 0.0390 - accuracy: 0.9880 - val_loss: 0.7379 - val_accuracy: 0.8610
Epoch 20/150
1407/1407 - 23s - loss: 0.0363 - accuracy: 0.9880 - val_loss: 0.8573 - val_accuracy: 0.8538
Epoch 21/150
1407/1407 - 23s - loss: 0.0481 - accuracy: 0.9853 - val_loss: 0.7347 - val_accuracy: 0.8702
Epoch 22/150
1407/1407 - 23s - loss: 0.0335 - accuracy: 0.9898 - val_loss: 0.7723 - val_accuracy: 0.8682
Epoch 23/150
1407/1407 - 23s - loss: 0.0274 - accuracy: 0.9915 - val_loss: 0.8012 - val_accuracy: 0.8650
Epoch 24/150
1407/1407 - 23s - loss: 0.0293 - accuracy: 0.9911 - val_loss: 0.8208 - val_accuracy: 0.8694
Epoch 25/150
1407/1407 - 23s - loss: 0.0322 - accuracy: 0.9902 - val_loss: 0.7821 - val_accuracy: 0.8648
Epoch 26/150
1407/1407 - 23s - loss: 0.0258 - accuracy: 0.9921 - val_loss: 0.8322 - val_accuracy: 0.8642
Epoch 27/150
1407/1407 - 23s - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.7508 - val_accuracy: 0.8714
Epoch 28/150
1407/1407 - 23s - loss: 0.0236 - accuracy: 0.9930 - val_loss: 0.8608 - val_accuracy: 0.8726
Epoch 29/150
1407/1407 - 23s - loss: 0.0241 - accuracy: 0.9925 - val_loss: 0.8247 - val_accuracy: 0.8674
Epoch 30/150
1407/1407 - 23s - loss: 0.0240 - accuracy: 0.9926 - val_loss: 0.8683 - val_accuracy: 0.8710
Epoch 31/150
1407/1407 - 23s - loss: 0.0235 - accuracy: 0.9928 - val_loss: 0.9446 - val_accuracy: 0.8592
Epoch 32/150
1407/1407 - 23s - loss: 0.0266 - accuracy: 0.9919 - val_loss: 0.8021 - val_accuracy: 0.8740
Epoch 33/150
1407/1407 - 23s - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.8596 - val_accuracy: 0.8738
Epoch 34/150
1407/1407 - 23s - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.8579 - val_accuracy: 0.8674
Epoch 35/150
1407/1407 - 23s - loss: 0.0324 - accuracy: 0.9919 - val_loss: 0.7782 - val_accuracy: 0.8736
Epoch 36/150
1407/1407 - 23s - loss: 0.0143 - accuracy: 0.9953 - val_loss: 0.9320 - val_accuracy: 0.8678
Epoch 37/150
1407/1407 - 23s - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.9777 - val_accuracy: 0.8692
Epoch 38/150
1407/1407 - 23s - loss: 0.0240 - accuracy: 0.9927 - val_loss: 0.8621 - val_accuracy: 0.8610
Epoch 39/150
1407/1407 - 23s - loss: 0.0182 - accuracy: 0.9942 - val_loss: 0.8230 - val_accuracy: 0.8638
Epoch 40/150
1407/1407 - 23s - loss: 0.0159 - accuracy: 0.9951 - val_loss: 0.8468 - val_accuracy: 0.8660
Epoch 41/150
1407/1407 - 23s - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.8481 - val_accuracy: 0.8768
Epoch 42/150
1407/1407 - 23s - loss: 0.0152 - accuracy: 0.9956 - val_loss: 0.8811 - val_accuracy: 0.8734
Epoch 43/150
1407/1407 - 23s - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.9547 - val_accuracy: 0.8746
Epoch 44/150
1407/1407 - 23s - loss: 0.0186 - accuracy: 0.9950 - val_loss: 0.8315 - val_accuracy: 0.8636
Epoch 45/150
1407/1407 - 23s - loss: 0.0137 - accuracy: 0.9958 - val_loss: 1.0169 - val_accuracy: 0.8684
Epoch 46/150
1407/1407 - 23s - loss: 0.0163 - accuracy: 0.9950 - val_loss: 0.9944 - val_accuracy: 0.8622
Epoch 47/150
1407/1407 - 23s - loss: 0.0172 - accuracy: 0.9952 - val_loss: 0.9438 - val_accuracy: 0.8706
Epoch 48/150
1407/1407 - 23s - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.9067 - val_accuracy: 0.8728
Epoch 49/150
1407/1407 - 23s - loss: 0.0145 - accuracy: 0.9957 - val_loss: 0.8921 - val_accuracy: 0.8768
Epoch 50/150
1407/1407 - 23s - loss: 0.0140 - accuracy: 0.9956 - val_loss: 0.9143 - val_accuracy: 0.8788
Epoch 51/150
1407/1407 - 23s - loss: 0.0132 - accuracy: 0.9958 - val_loss: 0.9472 - val_accuracy: 0.8752
Epoch 52/150
1407/1407 - 23s - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.9577 - val_accuracy: 0.8726
Epoch 53/150
1407/1407 - 23s - loss: 0.0132 - accuracy: 0.9959 - val_loss: 0.9247 - val_accuracy: 0.8676
Epoch 54/150
1407/1407 - 23s - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.9581 - val_accuracy: 0.8744
Epoch 55/150
1407/1407 - 23s - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.9567 - val_accuracy: 0.8746
Epoch 00055: early stopping
input_1 False
block1_conv1 False
batch_normalization True
block1_conv2 False
batch_normalization_1 True
block1_pool False
block2_conv1 False
batch_normalization_2 True
block2_conv2 False
batch_normalization_3 True
block2_pool False
block3_conv1 False
batch_normalization_4 True
block3_conv2 False
batch_normalization_5 True
block3_conv3 False
batch_normalization_6 True
block3_pool False
block4_conv1 False
batch_normalization_7 True
block4_conv2 False
batch_normalization_8 True
block4_conv3 False
batch_normalization_9 True
block4_pool False
block5_conv1 False
batch_normalization_10 True
block5_conv2 False
batch_normalization_11 True
block5_conv3 False
batch_normalization_12 True
block5_pool False
flatten False
fc1 False
batch_normalization_13 True
fc2 False
batch_normalization_14 True
predictions False
  0/250 [..............................] - ETA: 0s  1/250 [..............................] - ETA: 32:04 10/250 [>.............................] - ETA: 5:13  19/250 [=>............................] - ETA: 3:44 27/250 [==>...........................] - ETA: 3:13 36/250 [===>..........................] - ETA: 2:51 45/250 [====>.........................] - ETA: 2:35 54/250 [=====>........................] - ETA: 2:23 63/250 [======>.......................] - ETA: 2:13 72/250 [=======>......................] - ETA: 2:04 81/250 [========>.....................] - ETA: 1:55 90/250 [=========>....................] - ETA: 1:48 99/250 [==========>...................] - ETA: 1:41108/250 [===========>..................] - ETA: 1:34117/250 [=============>................] - ETA: 1:27126/250 [==============>...............] - ETA: 1:20135/250 [===============>..............] - ETA: 1:14144/250 [================>.............] - ETA: 1:08153/250 [=================>............] - ETA: 1:02162/250 [==================>...........] - ETA: 56s 171/250 [===================>..........] - ETA: 50s180/250 [====================>.........] - ETA: 44s189/250 [=====================>........] - ETA: 38s198/250 [======================>.......] - ETA: 32s207/250 [=======================>......] - ETA: 27s216/250 [========================>.....] - ETA: 21s225/250 [==========================>...] - ETA: 15s234/250 [===========================>..] - ETA: 10s243/250 [============================>.] - ETA: 4s MCBN accuracy: 82.0%
MCBN-ensemble accuracy: 84.1%
tf.Tensor(
[[875   6  18  15   9   3   7  10  37  20]
 [ 24 870   2   9   0   0   4   2  28  61]
 [ 43   2 781  41  34  40  31  19   6   3]
 [ 23   5  40 649  27 110  66  48  20  12]
 [ 21   1  31  32 817  29  22  36   7   4]
 [  9   3  13 113  19 774  21  39   7   2]
 [  9   5  13  20  11  14 914   4   7   3]
 [ 17   1  12  12  14  39   3 889   6   7]
 [ 42   1   4   7   3   4   2   1 923  13]
 [ 13  34   4   6   1   2   5   3  14 918]], shape=(10, 10), dtype=int32)
posterior mean: 4
true label: 4

class: 0; proba: 0.0%; var: 0.00% 
class: 1; proba: 2.2%; var: 13.67% 
class: 2; proba: 0.2%; var: 0.99% 
class: 3; proba: 0.0%; var: 0.05% 
class: 4; proba: 93.1%; var: 24.32% 
class: 5; proba: 0.0%; var: 0.00% 
class: 6; proba: 0.0%; var: 0.32% 
class: 7; proba: 4.4%; var: 18.68% 
class: 8; proba: 0.0%; var: 0.00% 
class: 9; proba: 0.1%; var: 0.53% 


###############################################################################
Peregrine Cluster
Job 10127854 for user 's2934833'
Finished at: Mon Mar 16 11:34:18 CET 2020

Job details:
============

Name                : MCBNCIFAR10
User                : s2934833
Partition           : gpu
Nodes               : pg-gpu26
Cores               : 12
State               : COMPLETED
Submit              : 2020-03-16T10:31:00
Start               : 2020-03-16T11:09:54
End                 : 2020-03-16T11:34:18
Reserved walltime   : 02:00:00
Used walltime       : 00:24:24
Used CPU time       : 00:26:39 (efficiency:  9.10%)
% User (Computation): 82.42%
% System (I/O)      : 17.58%
Mem reserved        : 62.50G/node
Max Mem used        : 4.29G (pg-gpu26)
Max Disk Write      : 153.60K (pg-gpu26)
Max Disk Read       : 5.35M (pg-gpu26)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
