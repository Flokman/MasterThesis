
The following have been reloaded with a version change:
  1) FFTW/3.3.7-gompic-2018a => FFTW/3.3.7-gompi-2018a
  2) OpenMPI/2.1.2-gcccuda-2018a => OpenMPI/2.1.2-GCC-6.4.0-2.28
  3) Python/3.6.4-fosscuda-2018a => Python/3.6.4-foss-2018a
  4) ScaLAPACK/2.0.2-gompic-2018a-OpenBLAS-0.2.20 => ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20

2020-02-18 13:33:06.765716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-18 13:33:06.766270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 31.87GiB freeMemory: 29.56GiB
2020-02-18 13:33:06.766321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-02-18 13:33:07.781062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-18 13:33:07.781170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-02-18 13:33:07.781190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-02-18 13:33:07.781321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28669 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-02-18 13:33:07.783212: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Random seed for replication: 113
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 32, num_classes = 5, epochs = 500,
          test_img_idx = 87, train_test_split = 0.8, to_shuffle = True,
          augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463], label_normalizer = True,
          save_augmentation_to_hdf5 = True, learn rate = 1e-05, train_all_layers = True,
          weights_to_use = None, es_patience = 30, train_val_split = 0.9,
          N_ENSEMBLE_MEMBERS = 10
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
predictions (Dense)          (None, 5)                 20485     
=================================================================
Total params: 165,738,309
Trainable params: 165,738,309
Non-trainable params: 0
_________________________________________________________________
Start fitting monte carlo dropout model
Random seed for replication: 716
Epoch 1/500
 - 48s - loss: 1.5850 - acc: 0.2330 - val_loss: 1.4990 - val_acc: 0.3107
Epoch 2/500
 - 39s - loss: 1.4332 - acc: 0.3680 - val_loss: 1.4042 - val_acc: 0.3679
Epoch 3/500
 - 42s - loss: 1.3713 - acc: 0.4049 - val_loss: 1.3658 - val_acc: 0.3929
Epoch 4/500
 - 43s - loss: 1.3109 - acc: 0.4316 - val_loss: 1.2721 - val_acc: 0.4482
Epoch 5/500
 - 42s - loss: 1.2674 - acc: 0.4580 - val_loss: 1.2671 - val_acc: 0.4429
Epoch 6/500
 - 42s - loss: 1.2179 - acc: 0.4887 - val_loss: 1.2226 - val_acc: 0.4857
Epoch 7/500
 - 41s - loss: 1.1737 - acc: 0.5145 - val_loss: 1.1622 - val_acc: 0.5286
Epoch 8/500
 - 41s - loss: 1.1140 - acc: 0.5574 - val_loss: 1.1263 - val_acc: 0.5571
Epoch 9/500
 - 41s - loss: 1.0451 - acc: 0.5765 - val_loss: 1.0787 - val_acc: 0.5446
Epoch 10/500
 - 41s - loss: 0.9782 - acc: 0.6106 - val_loss: 1.0085 - val_acc: 0.5946
Epoch 11/500
 - 40s - loss: 0.8915 - acc: 0.6520 - val_loss: 0.9502 - val_acc: 0.6268
Epoch 12/500
 - 41s - loss: 0.8097 - acc: 0.6839 - val_loss: 0.9053 - val_acc: 0.6411
Epoch 13/500
 - 41s - loss: 0.6962 - acc: 0.7302 - val_loss: 0.9151 - val_acc: 0.6536
Epoch 14/500
 - 41s - loss: 0.6092 - acc: 0.7628 - val_loss: 0.9471 - val_acc: 0.6321
Epoch 15/500
 - 41s - loss: 0.5299 - acc: 0.8014 - val_loss: 0.9862 - val_acc: 0.6696
Epoch 16/500
 - 40s - loss: 0.4411 - acc: 0.8419 - val_loss: 0.9330 - val_acc: 0.6839
Epoch 17/500
 - 40s - loss: 0.4018 - acc: 0.8495 - val_loss: 1.1257 - val_acc: 0.6625
Epoch 18/500
 - 40s - loss: 0.3136 - acc: 0.8872 - val_loss: 1.0886 - val_acc: 0.6750
Epoch 19/500
 - 40s - loss: 0.2631 - acc: 0.9039 - val_loss: 1.0874 - val_acc: 0.6875
Epoch 20/500
 - 40s - loss: 0.2314 - acc: 0.9183 - val_loss: 0.9486 - val_acc: 0.7143
Epoch 21/500
 - 40s - loss: 0.1681 - acc: 0.9430 - val_loss: 1.0240 - val_acc: 0.7179
Epoch 22/500
 - 40s - loss: 0.1959 - acc: 0.9392 - val_loss: 1.1003 - val_acc: 0.6857
Epoch 23/500
 - 40s - loss: 0.2245 - acc: 0.9311 - val_loss: 1.1008 - val_acc: 0.6786
Epoch 24/500
 - 40s - loss: 0.1403 - acc: 0.9523 - val_loss: 1.1885 - val_acc: 0.7143
Epoch 25/500
 - 40s - loss: 0.1062 - acc: 0.9664 - val_loss: 1.2914 - val_acc: 0.7214
Epoch 26/500
 - 41s - loss: 0.1321 - acc: 0.9585 - val_loss: 1.3509 - val_acc: 0.7179
Epoch 27/500
 - 40s - loss: 0.1260 - acc: 0.9611 - val_loss: 1.2433 - val_acc: 0.6964
Epoch 28/500
 - 40s - loss: 0.1047 - acc: 0.9668 - val_loss: 1.2689 - val_acc: 0.7143
Epoch 29/500
 - 40s - loss: 0.0950 - acc: 0.9716 - val_loss: 1.2300 - val_acc: 0.7036
Epoch 30/500
 - 40s - loss: 0.0728 - acc: 0.9775 - val_loss: 1.2598 - val_acc: 0.7429
Epoch 31/500
 - 40s - loss: 0.0575 - acc: 0.9828 - val_loss: 1.4452 - val_acc: 0.7179
Epoch 32/500
 - 40s - loss: 0.0655 - acc: 0.9826 - val_loss: 1.2853 - val_acc: 0.7375
Epoch 33/500
 - 40s - loss: 0.0567 - acc: 0.9840 - val_loss: 1.3282 - val_acc: 0.7375
Epoch 34/500
 - 40s - loss: 0.0522 - acc: 0.9836 - val_loss: 1.3319 - val_acc: 0.7446
Epoch 35/500
 - 40s - loss: 0.0904 - acc: 0.9763 - val_loss: 1.6158 - val_acc: 0.6732
Epoch 36/500
 - 40s - loss: 0.1545 - acc: 0.9529 - val_loss: 1.2646 - val_acc: 0.7089
Epoch 37/500
 - 40s - loss: 0.0668 - acc: 0.9783 - val_loss: 1.2580 - val_acc: 0.7339
Epoch 38/500
 - 40s - loss: 0.0479 - acc: 0.9852 - val_loss: 1.1978 - val_acc: 0.7464
Epoch 39/500
 - 40s - loss: 0.0505 - acc: 0.9854 - val_loss: 1.5427 - val_acc: 0.7196
Epoch 40/500
 - 40s - loss: 0.0753 - acc: 0.9773 - val_loss: 1.1602 - val_acc: 0.7375
Epoch 41/500
 - 40s - loss: 0.0803 - acc: 0.9766 - val_loss: 1.2658 - val_acc: 0.7107
Epoch 42/500
 - 40s - loss: 0.0395 - acc: 0.9883 - val_loss: 1.2290 - val_acc: 0.7464
Epoch 00042: early stopping
Random seed for replication: 414
Epoch 1/500
 - 40s - loss: 1.5831 - acc: 0.2485 - val_loss: 1.4943 - val_acc: 0.3071
Epoch 2/500
 - 40s - loss: 1.4542 - acc: 0.3489 - val_loss: 1.4620 - val_acc: 0.3250
Epoch 3/500
 - 42s - loss: 1.4181 - acc: 0.3648 - val_loss: 1.4183 - val_acc: 0.3482
Epoch 4/500
 - 43s - loss: 1.3869 - acc: 0.3907 - val_loss: 1.4025 - val_acc: 0.3500
Epoch 5/500
 - 42s - loss: 1.3471 - acc: 0.4178 - val_loss: 1.3217 - val_acc: 0.3964
Epoch 6/500
 - 41s - loss: 1.2970 - acc: 0.4391 - val_loss: 1.3094 - val_acc: 0.4179
Epoch 7/500
 - 41s - loss: 1.2358 - acc: 0.4677 - val_loss: 1.2196 - val_acc: 0.4857
Epoch 8/500
 - 41s - loss: 1.1742 - acc: 0.5093 - val_loss: 1.1915 - val_acc: 0.4893
Epoch 9/500
 - 41s - loss: 1.1126 - acc: 0.5459 - val_loss: 1.1511 - val_acc: 0.5000
Epoch 10/500
 - 41s - loss: 1.0597 - acc: 0.5699 - val_loss: 1.1387 - val_acc: 0.5268
Epoch 11/500
 - 41s - loss: 0.9702 - acc: 0.6076 - val_loss: 1.1663 - val_acc: 0.5304
Epoch 12/500
 - 41s - loss: 0.8776 - acc: 0.6475 - val_loss: 1.0445 - val_acc: 0.5607
Epoch 13/500
 - 41s - loss: 0.7902 - acc: 0.6841 - val_loss: 1.0310 - val_acc: 0.5857
Epoch 14/500
 - 41s - loss: 0.6842 - acc: 0.7318 - val_loss: 1.0626 - val_acc: 0.6000
Epoch 15/500
 - 40s - loss: 0.5833 - acc: 0.7696 - val_loss: 1.1206 - val_acc: 0.6000
Epoch 16/500
 - 41s - loss: 0.4842 - acc: 0.8093 - val_loss: 1.0083 - val_acc: 0.6321
Epoch 17/500
 - 40s - loss: 0.3859 - acc: 0.8474 - val_loss: 1.1857 - val_acc: 0.5946
Epoch 18/500
 - 40s - loss: 0.3329 - acc: 0.8759 - val_loss: 1.1318 - val_acc: 0.6625
Epoch 19/500
 - 40s - loss: 0.2438 - acc: 0.9116 - val_loss: 1.2865 - val_acc: 0.6786
Epoch 20/500
 - 40s - loss: 0.2373 - acc: 0.9121 - val_loss: 1.2277 - val_acc: 0.6661
Epoch 21/500
 - 40s - loss: 0.1987 - acc: 0.9344 - val_loss: 1.3410 - val_acc: 0.6839
Epoch 22/500
 - 40s - loss: 0.1574 - acc: 0.9449 - val_loss: 1.4027 - val_acc: 0.6857
Epoch 23/500
 - 40s - loss: 0.1491 - acc: 0.9517 - val_loss: 1.4927 - val_acc: 0.6464
Epoch 24/500
 - 40s - loss: 0.1139 - acc: 0.9616 - val_loss: 1.6629 - val_acc: 0.6571
Epoch 25/500
 - 40s - loss: 0.1153 - acc: 0.9638 - val_loss: 1.4144 - val_acc: 0.6821
Epoch 26/500
 - 40s - loss: 0.0925 - acc: 0.9670 - val_loss: 1.5135 - val_acc: 0.6750
Epoch 27/500
 - 40s - loss: 0.1114 - acc: 0.9656 - val_loss: 1.4806 - val_acc: 0.6554
Epoch 28/500
 - 40s - loss: 0.1169 - acc: 0.9620 - val_loss: 1.5818 - val_acc: 0.6821
Epoch 29/500
 - 40s - loss: 0.1289 - acc: 0.9595 - val_loss: 1.6045 - val_acc: 0.6500
Epoch 30/500
 - 41s - loss: 0.0758 - acc: 0.9741 - val_loss: 1.5637 - val_acc: 0.6893
Epoch 31/500
 - 41s - loss: 0.0775 - acc: 0.9792 - val_loss: 1.5522 - val_acc: 0.6946
Epoch 32/500
 - 40s - loss: 0.0577 - acc: 0.9820 - val_loss: 1.5731 - val_acc: 0.6911
Epoch 33/500
 - 40s - loss: 0.0496 - acc: 0.9867 - val_loss: 1.5830 - val_acc: 0.6964
Epoch 34/500
 - 40s - loss: 0.0451 - acc: 0.9866 - val_loss: 1.5490 - val_acc: 0.6911
Epoch 35/500
 - 40s - loss: 0.0492 - acc: 0.9848 - val_loss: 1.7115 - val_acc: 0.6911
Epoch 36/500
 - 40s - loss: 0.0474 - acc: 0.9875 - val_loss: 1.4922 - val_acc: 0.7125
Epoch 37/500
 - 40s - loss: 0.0381 - acc: 0.9881 - val_loss: 1.6934 - val_acc: 0.7036
Epoch 38/500
 - 40s - loss: 0.0433 - acc: 0.9883 - val_loss: 1.4482 - val_acc: 0.7196
Epoch 39/500
 - 40s - loss: 0.0381 - acc: 0.9879 - val_loss: 1.5514 - val_acc: 0.7071
Epoch 40/500
 - 40s - loss: 0.0499 - acc: 0.9836 - val_loss: 1.6887 - val_acc: 0.6643
Epoch 41/500
 - 40s - loss: 0.1639 - acc: 0.9490 - val_loss: 1.4583 - val_acc: 0.6786
Epoch 42/500
 - 40s - loss: 0.0517 - acc: 0.9842 - val_loss: 1.7479 - val_acc: 0.6804
Epoch 43/500
 - 40s - loss: 0.0479 - acc: 0.9848 - val_loss: 1.6683 - val_acc: 0.6786
Epoch 44/500
 - 41s - loss: 0.0374 - acc: 0.9881 - val_loss: 1.5601 - val_acc: 0.7071
Epoch 45/500
 - 41s - loss: 0.0322 - acc: 0.9889 - val_loss: 1.8208 - val_acc: 0.6446
Epoch 46/500
 - 40s - loss: 0.0476 - acc: 0.9842 - val_loss: 1.7447 - val_acc: 0.6661
Epoch 00046: early stopping
Random seed for replication: 16
Epoch 1/500
 - 40s - loss: 1.5894 - acc: 0.2269 - val_loss: 1.5302 - val_acc: 0.3196
Epoch 2/500
 - 40s - loss: 1.4682 - acc: 0.3305 - val_loss: 1.4292 - val_acc: 0.3375
Epoch 3/500
 - 42s - loss: 1.4009 - acc: 0.3763 - val_loss: 1.3619 - val_acc: 0.3857
Epoch 4/500
 - 43s - loss: 1.3439 - acc: 0.4185 - val_loss: 1.3169 - val_acc: 0.4286
Epoch 5/500
 - 43s - loss: 1.2856 - acc: 0.4525 - val_loss: 1.2655 - val_acc: 0.4589
Epoch 6/500
 - 42s - loss: 1.2412 - acc: 0.4759 - val_loss: 1.2718 - val_acc: 0.4536
Epoch 7/500
 - 42s - loss: 1.1745 - acc: 0.5121 - val_loss: 1.2225 - val_acc: 0.4679
Epoch 8/500
 - 42s - loss: 1.1094 - acc: 0.5420 - val_loss: 1.1722 - val_acc: 0.5250
Epoch 9/500
 - 41s - loss: 1.0402 - acc: 0.5675 - val_loss: 1.0950 - val_acc: 0.5411
Epoch 10/500
 - 41s - loss: 0.9597 - acc: 0.6076 - val_loss: 1.0679 - val_acc: 0.5696
Epoch 11/500
 - 41s - loss: 0.8627 - acc: 0.6574 - val_loss: 0.9823 - val_acc: 0.5929
Epoch 12/500
 - 41s - loss: 0.7415 - acc: 0.7050 - val_loss: 1.0188 - val_acc: 0.6125
Epoch 13/500
 - 41s - loss: 0.6429 - acc: 0.7447 - val_loss: 0.8897 - val_acc: 0.6518
Epoch 14/500
 - 41s - loss: 0.5232 - acc: 0.7992 - val_loss: 1.0348 - val_acc: 0.6554
Epoch 15/500
 - 41s - loss: 0.4428 - acc: 0.8323 - val_loss: 0.9541 - val_acc: 0.6339
Epoch 16/500
 - 41s - loss: 0.3540 - acc: 0.8742 - val_loss: 0.8778 - val_acc: 0.6929
Epoch 17/500
 - 41s - loss: 0.2786 - acc: 0.8970 - val_loss: 1.0549 - val_acc: 0.6786
Epoch 18/500
 - 41s - loss: 0.2166 - acc: 0.9231 - val_loss: 0.9825 - val_acc: 0.7196
Epoch 19/500
 - 41s - loss: 0.1851 - acc: 0.9379 - val_loss: 1.0898 - val_acc: 0.6982
Epoch 20/500
 - 40s - loss: 0.1544 - acc: 0.9460 - val_loss: 1.1700 - val_acc: 0.6911
Epoch 21/500
 - 40s - loss: 0.1231 - acc: 0.9604 - val_loss: 1.2405 - val_acc: 0.7143
Epoch 22/500
 - 40s - loss: 0.1044 - acc: 0.9668 - val_loss: 1.3151 - val_acc: 0.7143
Epoch 23/500
 - 40s - loss: 0.0893 - acc: 0.9729 - val_loss: 1.2780 - val_acc: 0.7179
Epoch 24/500
 - 40s - loss: 0.1499 - acc: 0.9603 - val_loss: 1.3876 - val_acc: 0.7179
Epoch 25/500
 - 40s - loss: 0.1551 - acc: 0.9532 - val_loss: 1.1241 - val_acc: 0.7250
Epoch 26/500
 - 40s - loss: 0.0761 - acc: 0.9804 - val_loss: 1.2435 - val_acc: 0.7232
Epoch 27/500
 - 41s - loss: 0.0526 - acc: 0.9836 - val_loss: 1.2238 - val_acc: 0.7196
Epoch 28/500
 - 40s - loss: 0.0487 - acc: 0.9854 - val_loss: 1.4015 - val_acc: 0.7036
Epoch 29/500
 - 40s - loss: 0.0564 - acc: 0.9867 - val_loss: 1.2956 - val_acc: 0.7357
Epoch 30/500
 - 40s - loss: 0.0463 - acc: 0.9891 - val_loss: 1.3021 - val_acc: 0.7411
Epoch 31/500
 - 40s - loss: 0.0508 - acc: 0.9867 - val_loss: 1.2441 - val_acc: 0.7161
Epoch 32/500
 - 40s - loss: 0.0405 - acc: 0.9887 - val_loss: 1.2144 - val_acc: 0.7304
Epoch 33/500
 - 40s - loss: 0.0456 - acc: 0.9873 - val_loss: 1.3165 - val_acc: 0.7250
Epoch 34/500
 - 40s - loss: 0.0486 - acc: 0.9866 - val_loss: 1.2158 - val_acc: 0.7232
Epoch 35/500
 - 40s - loss: 0.0436 - acc: 0.9881 - val_loss: 1.2445 - val_acc: 0.7393
Epoch 36/500
 - 40s - loss: 0.0391 - acc: 0.9897 - val_loss: 1.2977 - val_acc: 0.7321
Epoch 37/500
 - 40s - loss: 0.0377 - acc: 0.9889 - val_loss: 1.2379 - val_acc: 0.7196
Epoch 38/500
 - 41s - loss: 0.0349 - acc: 0.9887 - val_loss: 1.2808 - val_acc: 0.7357
Epoch 39/500
 - 40s - loss: 0.0457 - acc: 0.9853 - val_loss: 1.4526 - val_acc: 0.6982
Epoch 40/500
 - 40s - loss: 0.1016 - acc: 0.9719 - val_loss: 1.4285 - val_acc: 0.6732
Epoch 41/500
 - 41s - loss: 0.1018 - acc: 0.9678 - val_loss: 1.7550 - val_acc: 0.6571
Epoch 42/500
 - 41s - loss: 0.0642 - acc: 0.9786 - val_loss: 1.1695 - val_acc: 0.7304
Epoch 43/500
 - 41s - loss: 0.0414 - acc: 0.9895 - val_loss: 1.2907 - val_acc: 0.7179
Epoch 44/500
 - 42s - loss: 0.0355 - acc: 0.9887 - val_loss: 1.2199 - val_acc: 0.7107
Epoch 45/500
 - 41s - loss: 0.0318 - acc: 0.9905 - val_loss: 1.3054 - val_acc: 0.7375
Epoch 46/500
 - 41s - loss: 0.0418 - acc: 0.9871 - val_loss: 1.4027 - val_acc: 0.6982
Epoch 00046: early stopping
Random seed for replication: 572
Epoch 1/500
 - 40s - loss: 1.5968 - acc: 0.2207 - val_loss: 1.5130 - val_acc: 0.3161
Epoch 2/500
 - 40s - loss: 1.4560 - acc: 0.3459 - val_loss: 1.3994 - val_acc: 0.3589
Epoch 3/500
 - 42s - loss: 1.3806 - acc: 0.3869 - val_loss: 1.3829 - val_acc: 0.4179
Epoch 4/500
 - 43s - loss: 1.3386 - acc: 0.4188 - val_loss: 1.3202 - val_acc: 0.4196
Epoch 5/500
 - 43s - loss: 1.2936 - acc: 0.4515 - val_loss: 1.2652 - val_acc: 0.4339
Epoch 6/500
 - 42s - loss: 1.2465 - acc: 0.4635 - val_loss: 1.2595 - val_acc: 0.4643
Epoch 7/500
 - 42s - loss: 1.2003 - acc: 0.4977 - val_loss: 1.1829 - val_acc: 0.5000
Epoch 8/500
 - 41s - loss: 1.1313 - acc: 0.5288 - val_loss: 1.1255 - val_acc: 0.5393
Epoch 9/500
 - 41s - loss: 1.0478 - acc: 0.5710 - val_loss: 1.0771 - val_acc: 0.5571
Epoch 10/500
 - 41s - loss: 0.9634 - acc: 0.6133 - val_loss: 1.0254 - val_acc: 0.5804
Epoch 11/500
 - 41s - loss: 0.8669 - acc: 0.6524 - val_loss: 0.9415 - val_acc: 0.6196
Epoch 12/500
 - 41s - loss: 0.7650 - acc: 0.6997 - val_loss: 0.9332 - val_acc: 0.6500
Epoch 13/500
 - 41s - loss: 0.6582 - acc: 0.7420 - val_loss: 0.8995 - val_acc: 0.6750
Epoch 14/500
 - 41s - loss: 0.5650 - acc: 0.7801 - val_loss: 0.8431 - val_acc: 0.6964
Epoch 15/500
 - 41s - loss: 0.4665 - acc: 0.8183 - val_loss: 0.8202 - val_acc: 0.6982
Epoch 16/500
 - 41s - loss: 0.3967 - acc: 0.8492 - val_loss: 0.9134 - val_acc: 0.7018
Epoch 17/500
 - 41s - loss: 0.2996 - acc: 0.8941 - val_loss: 0.9555 - val_acc: 0.7089
Epoch 18/500
 - 41s - loss: 0.2358 - acc: 0.9135 - val_loss: 1.0448 - val_acc: 0.7196
Epoch 19/500
 - 41s - loss: 0.2115 - acc: 0.9245 - val_loss: 1.1215 - val_acc: 0.7179
Epoch 20/500
 - 40s - loss: 0.1801 - acc: 0.9347 - val_loss: 1.1318 - val_acc: 0.7518
Epoch 21/500
 - 40s - loss: 0.1725 - acc: 0.9464 - val_loss: 1.3035 - val_acc: 0.7125
Epoch 22/500
 - 41s - loss: 0.1297 - acc: 0.9579 - val_loss: 1.2652 - val_acc: 0.7357
Epoch 23/500
 - 41s - loss: 0.1183 - acc: 0.9602 - val_loss: 1.2566 - val_acc: 0.7464
Epoch 24/500
 - 41s - loss: 0.0979 - acc: 0.9707 - val_loss: 1.2650 - val_acc: 0.7464
Epoch 25/500
 - 41s - loss: 0.1119 - acc: 0.9680 - val_loss: 1.1455 - val_acc: 0.7464
Epoch 26/500
 - 41s - loss: 0.0832 - acc: 0.9776 - val_loss: 1.1572 - val_acc: 0.7482
Epoch 27/500
 - 40s - loss: 0.0727 - acc: 0.9798 - val_loss: 1.1706 - val_acc: 0.7786
Epoch 28/500
 - 41s - loss: 0.0608 - acc: 0.9850 - val_loss: 1.1977 - val_acc: 0.7714
Epoch 29/500
 - 41s - loss: 0.0586 - acc: 0.9840 - val_loss: 1.2213 - val_acc: 0.7554
Epoch 30/500
 - 40s - loss: 0.0487 - acc: 0.9846 - val_loss: 1.2278 - val_acc: 0.7571
Epoch 31/500
 - 40s - loss: 0.0899 - acc: 0.9747 - val_loss: 1.3042 - val_acc: 0.7643
Epoch 32/500
 - 40s - loss: 0.1072 - acc: 0.9711 - val_loss: 1.2102 - val_acc: 0.7339
Epoch 33/500
 - 41s - loss: 0.0789 - acc: 0.9763 - val_loss: 1.2139 - val_acc: 0.7768
Epoch 34/500
 - 41s - loss: 0.0543 - acc: 0.9824 - val_loss: 1.3335 - val_acc: 0.7589
Epoch 35/500
 - 41s - loss: 0.0463 - acc: 0.9862 - val_loss: 1.3170 - val_acc: 0.7518
Epoch 36/500
 - 41s - loss: 0.0637 - acc: 0.9816 - val_loss: 1.2642 - val_acc: 0.7643
Epoch 37/500
 - 41s - loss: 0.0488 - acc: 0.9860 - val_loss: 1.2134 - val_acc: 0.7804
Epoch 38/500
 - 41s - loss: 0.0459 - acc: 0.9875 - val_loss: 1.3392 - val_acc: 0.7411
Epoch 39/500
 - 41s - loss: 0.0536 - acc: 0.9834 - val_loss: 1.1585 - val_acc: 0.7857
Epoch 40/500
 - 41s - loss: 0.0857 - acc: 0.9764 - val_loss: 1.1590 - val_acc: 0.7214
