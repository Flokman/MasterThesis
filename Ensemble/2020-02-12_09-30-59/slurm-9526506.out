
The following have been reloaded with a version change:
  1) FFTW/3.3.7-gompic-2018a => FFTW/3.3.7-gompi-2018a
  2) OpenMPI/2.1.2-gcccuda-2018a => OpenMPI/2.1.2-GCC-6.4.0-2.28
  3) Python/3.6.4-fosscuda-2018a => Python/3.6.4-foss-2018a
  4) ScaLAPACK/2.0.2-gompic-2018a-OpenBLAS-0.2.20 => ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20

2020-02-12 09:30:59.409985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-12 09:30:59.412780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 31.88GiB freeMemory: 29.56GiB
2020-02-12 09:30:59.412843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-02-12 09:30:59.840365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-12 09:30:59.840451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-02-12 09:30:59.840469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-02-12 09:30:59.840587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28671 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-02-12 09:30:59.841607: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Random seed for replication: 400
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 32, num_classes = 5, epochs = 500,
          test_img_idx = 1018, train_test_split = 0.8, to_shuffle = True,
          augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463], label_normalizer = True,
          save_augmentation_to_hdf5 = True, learn rate = 1e-05, train_all_layers = True,
          weights_to_use = None, es_patience = 10, train_val_split = 0.9
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
predictions (Dense)          (None, 5)                 20485     
=================================================================
Total params: 165,738,309
Trainable params: 165,738,309
Non-trainable params: 0
_________________________________________________________________
Start fitting monte carlo dropout model
Random seed for replication: 639
Epoch 1/500
 - 47s - loss: 1.5805 - acc: 0.2467 - val_loss: 1.5024 - val_acc: 0.2946
Epoch 2/500
 - 39s - loss: 1.4436 - acc: 0.3493 - val_loss: 1.4221 - val_acc: 0.3964
Epoch 3/500
 - 42s - loss: 1.3848 - acc: 0.3903 - val_loss: 1.3718 - val_acc: 0.4018
Epoch 4/500
 - 42s - loss: 1.3288 - acc: 0.4190 - val_loss: 1.3302 - val_acc: 0.4089
Epoch 5/500
 - 42s - loss: 1.2916 - acc: 0.4435 - val_loss: 1.3146 - val_acc: 0.4357
Epoch 6/500
 - 42s - loss: 1.2305 - acc: 0.4854 - val_loss: 1.2480 - val_acc: 0.4964
Epoch 7/500
 - 41s - loss: 1.1639 - acc: 0.5128 - val_loss: 1.2123 - val_acc: 0.5089
Epoch 8/500
 - 41s - loss: 1.1011 - acc: 0.5502 - val_loss: 1.2025 - val_acc: 0.5446
Epoch 9/500
 - 41s - loss: 1.0451 - acc: 0.5735 - val_loss: 1.1483 - val_acc: 0.5411
Epoch 10/500
 - 41s - loss: 0.9679 - acc: 0.6171 - val_loss: 1.1309 - val_acc: 0.5643
Epoch 11/500
 - 41s - loss: 0.8921 - acc: 0.6414 - val_loss: 1.0602 - val_acc: 0.5982
Epoch 12/500
 - 40s - loss: 0.8051 - acc: 0.6859 - val_loss: 1.0106 - val_acc: 0.6232
Epoch 13/500
 - 40s - loss: 0.7025 - acc: 0.7186 - val_loss: 1.0448 - val_acc: 0.6107
Epoch 14/500
 - 41s - loss: 0.6333 - acc: 0.7554 - val_loss: 0.9405 - val_acc: 0.6696
Epoch 15/500
 - 40s - loss: 0.5306 - acc: 0.7904 - val_loss: 1.0087 - val_acc: 0.6393
Epoch 16/500
 - 40s - loss: 0.4443 - acc: 0.8266 - val_loss: 0.9639 - val_acc: 0.6571
Epoch 17/500
 - 40s - loss: 0.3840 - acc: 0.8598 - val_loss: 1.0208 - val_acc: 0.6804
Epoch 18/500
 - 40s - loss: 0.2946 - acc: 0.8929 - val_loss: 1.0424 - val_acc: 0.6661
Epoch 19/500
 - 40s - loss: 0.2557 - acc: 0.9040 - val_loss: 1.1895 - val_acc: 0.6679
Epoch 20/500
 - 40s - loss: 0.2269 - acc: 0.9163 - val_loss: 1.0213 - val_acc: 0.7161
Epoch 21/500
 - 42s - loss: 0.1787 - acc: 0.9393 - val_loss: 1.1248 - val_acc: 0.6804
Epoch 22/500
 - 40s - loss: 0.1525 - acc: 0.9482 - val_loss: 1.1590 - val_acc: 0.7339
Epoch 23/500
 - 40s - loss: 0.1371 - acc: 0.9536 - val_loss: 1.2441 - val_acc: 0.7089
Epoch 24/500
 - 40s - loss: 0.1177 - acc: 0.9606 - val_loss: 1.2276 - val_acc: 0.7232
Epoch 00024: early stopping
Random seed for replication: 972
Epoch 1/500
 - 40s - loss: 1.5839 - acc: 0.2314 - val_loss: 1.5005 - val_acc: 0.2964
Epoch 2/500
 - 39s - loss: 1.4545 - acc: 0.3364 - val_loss: 1.4243 - val_acc: 0.3679
Epoch 3/500
 - 42s - loss: 1.4111 - acc: 0.3624 - val_loss: 1.4326 - val_acc: 0.3518
Epoch 4/500
 - 43s - loss: 1.3904 - acc: 0.3754 - val_loss: 1.3772 - val_acc: 0.3946
Epoch 5/500
 - 42s - loss: 1.3587 - acc: 0.4041 - val_loss: 1.3835 - val_acc: 0.3714
Epoch 6/500
 - 41s - loss: 1.3222 - acc: 0.4230 - val_loss: 1.3443 - val_acc: 0.3893
Epoch 7/500
 - 41s - loss: 1.2655 - acc: 0.4501 - val_loss: 1.2663 - val_acc: 0.4321
Epoch 8/500
 - 41s - loss: 1.2135 - acc: 0.4781 - val_loss: 1.2767 - val_acc: 0.4411
Epoch 9/500
 - 41s - loss: 1.1698 - acc: 0.4914 - val_loss: 1.2555 - val_acc: 0.4411
Epoch 10/500
 - 41s - loss: 1.1234 - acc: 0.5259 - val_loss: 1.2319 - val_acc: 0.4964
Epoch 11/500
 - 40s - loss: 1.0682 - acc: 0.5589 - val_loss: 1.1576 - val_acc: 0.5357
Epoch 12/500
 - 40s - loss: 0.9979 - acc: 0.5898 - val_loss: 1.1192 - val_acc: 0.5304
Epoch 13/500
 - 40s - loss: 0.9098 - acc: 0.6334 - val_loss: 1.1941 - val_acc: 0.5054
Epoch 14/500
 - 40s - loss: 0.8066 - acc: 0.6786 - val_loss: 1.1245 - val_acc: 0.5571
Epoch 15/500
 - 40s - loss: 0.7267 - acc: 0.7065 - val_loss: 1.0280 - val_acc: 0.5911
Epoch 16/500
 - 40s - loss: 0.6283 - acc: 0.7563 - val_loss: 1.0378 - val_acc: 0.6179
Epoch 17/500
 - 40s - loss: 0.5138 - acc: 0.8034 - val_loss: 1.1325 - val_acc: 0.6143
Epoch 18/500
 - 40s - loss: 0.4390 - acc: 0.8297 - val_loss: 1.1409 - val_acc: 0.6446
Epoch 19/500
 - 40s - loss: 0.3648 - acc: 0.8613 - val_loss: 1.2428 - val_acc: 0.6250
Epoch 20/500
 - 40s - loss: 0.3220 - acc: 0.8779 - val_loss: 1.1966 - val_acc: 0.6500
Epoch 21/500
 - 41s - loss: 0.2603 - acc: 0.9064 - val_loss: 1.2317 - val_acc: 0.6518
Epoch 22/500
 - 40s - loss: 0.2274 - acc: 0.9201 - val_loss: 1.4792 - val_acc: 0.6214
Epoch 23/500
 - 40s - loss: 0.1838 - acc: 0.9322 - val_loss: 1.6223 - val_acc: 0.6304
Epoch 24/500
 - 40s - loss: 0.1947 - acc: 0.9286 - val_loss: 1.4111 - val_acc: 0.6589
Epoch 25/500
 - 40s - loss: 0.1443 - acc: 0.9490 - val_loss: 1.5478 - val_acc: 0.6589
Epoch 00025: early stopping
Random seed for replication: 360
Epoch 1/500
 - 39s - loss: 1.5885 - acc: 0.2287 - val_loss: 1.4638 - val_acc: 0.3571
Epoch 2/500
 - 39s - loss: 1.4612 - acc: 0.3311 - val_loss: 1.3873 - val_acc: 0.3679
Epoch 3/500
 - 42s - loss: 1.4137 - acc: 0.3622 - val_loss: 1.3507 - val_acc: 0.3786
Epoch 4/500
 - 43s - loss: 1.3972 - acc: 0.3757 - val_loss: 1.3730 - val_acc: 0.3732
Epoch 5/500
 - 41s - loss: 1.3724 - acc: 0.3915 - val_loss: 1.3392 - val_acc: 0.4268
Epoch 6/500
 - 40s - loss: 1.3397 - acc: 0.4128 - val_loss: 1.3251 - val_acc: 0.4071
Epoch 7/500
 - 40s - loss: 1.2969 - acc: 0.4378 - val_loss: 1.2764 - val_acc: 0.4661
Epoch 8/500
 - 41s - loss: 1.2461 - acc: 0.4663 - val_loss: 1.2492 - val_acc: 0.4482
Epoch 9/500
 - 41s - loss: 1.2014 - acc: 0.4965 - val_loss: 1.2107 - val_acc: 0.5018
Epoch 10/500
 - 41s - loss: 1.1295 - acc: 0.5346 - val_loss: 1.1901 - val_acc: 0.4982
Epoch 11/500
 - 41s - loss: 1.0572 - acc: 0.5673 - val_loss: 1.1202 - val_acc: 0.5554
Epoch 12/500
 - 41s - loss: 0.9692 - acc: 0.6076 - val_loss: 1.1459 - val_acc: 0.5339
Epoch 13/500
 - 41s - loss: 0.8639 - acc: 0.6586 - val_loss: 0.9890 - val_acc: 0.5964
Epoch 14/500
 - 41s - loss: 0.7550 - acc: 0.7036 - val_loss: 0.9305 - val_acc: 0.6286
Epoch 15/500
 - 41s - loss: 0.6228 - acc: 0.7597 - val_loss: 1.0320 - val_acc: 0.6411
Epoch 16/500
 - 41s - loss: 0.5298 - acc: 0.7963 - val_loss: 0.9541 - val_acc: 0.6393
Epoch 17/500
 - 41s - loss: 0.4171 - acc: 0.8462 - val_loss: 1.0646 - val_acc: 0.6714
Epoch 18/500
 - 40s - loss: 0.3457 - acc: 0.8739 - val_loss: 1.0691 - val_acc: 0.6607
Epoch 19/500
 - 40s - loss: 0.2982 - acc: 0.8952 - val_loss: 1.0386 - val_acc: 0.6929
Epoch 20/500
 - 40s - loss: 0.2149 - acc: 0.9220 - val_loss: 1.1780 - val_acc: 0.6911
Epoch 21/500
 - 40s - loss: 0.1947 - acc: 0.9329 - val_loss: 1.2433 - val_acc: 0.6893
Epoch 22/500
 - 40s - loss: 0.1507 - acc: 0.9427 - val_loss: 1.4654 - val_acc: 0.6679
Epoch 23/500
 - 40s - loss: 0.1440 - acc: 0.9509 - val_loss: 1.2732 - val_acc: 0.7036
Epoch 24/500
 - 40s - loss: 0.1112 - acc: 0.9641 - val_loss: 1.3452 - val_acc: 0.7000
Epoch 00024: early stopping
Random seed for replication: 640
Epoch 1/500
 - 39s - loss: 1.5698 - acc: 0.2481 - val_loss: 1.4901 - val_acc: 0.3161
Epoch 2/500
 - 39s - loss: 1.4398 - acc: 0.3531 - val_loss: 1.4280 - val_acc: 0.3661
Epoch 3/500
 - 42s - loss: 1.3831 - acc: 0.3915 - val_loss: 1.3765 - val_acc: 0.4125
Epoch 4/500
 - 43s - loss: 1.3270 - acc: 0.4322 - val_loss: 1.3710 - val_acc: 0.4250
Epoch 5/500
 - 42s - loss: 1.2923 - acc: 0.4490 - val_loss: 1.3038 - val_acc: 0.4500
Epoch 6/500
 - 42s - loss: 1.2339 - acc: 0.4854 - val_loss: 1.2606 - val_acc: 0.4750
Epoch 7/500
 - 41s - loss: 1.1777 - acc: 0.5137 - val_loss: 1.2126 - val_acc: 0.4929
Epoch 8/500
 - 41s - loss: 1.1152 - acc: 0.5386 - val_loss: 1.2011 - val_acc: 0.4946
Epoch 9/500
 - 41s - loss: 1.0408 - acc: 0.5824 - val_loss: 1.1055 - val_acc: 0.5500
Epoch 10/500
 - 41s - loss: 0.9528 - acc: 0.6223 - val_loss: 1.0775 - val_acc: 0.5679
Epoch 11/500
 - 40s - loss: 0.8601 - acc: 0.6580 - val_loss: 1.0364 - val_acc: 0.5964
Epoch 12/500
 - 40s - loss: 0.7544 - acc: 0.6974 - val_loss: 0.9912 - val_acc: 0.6125
Epoch 13/500
 - 40s - loss: 0.6524 - acc: 0.7402 - val_loss: 1.0797 - val_acc: 0.6161
Epoch 14/500
 - 40s - loss: 0.5662 - acc: 0.7727 - val_loss: 0.9072 - val_acc: 0.6625
Epoch 15/500
 - 40s - loss: 0.4674 - acc: 0.8204 - val_loss: 0.9288 - val_acc: 0.6768
Epoch 16/500
 - 40s - loss: 0.4017 - acc: 0.8473 - val_loss: 0.9267 - val_acc: 0.7143
Epoch 17/500
 - 40s - loss: 0.3190 - acc: 0.8776 - val_loss: 0.9582 - val_acc: 0.7071
Epoch 18/500
 - 40s - loss: 0.2712 - acc: 0.8977 - val_loss: 1.1171 - val_acc: 0.6804
Epoch 19/500
 - 40s - loss: 0.2354 - acc: 0.9182 - val_loss: 1.1323 - val_acc: 0.6964
Epoch 20/500
 - 40s - loss: 0.1906 - acc: 0.9324 - val_loss: 1.2009 - val_acc: 0.6857
Epoch 21/500
 - 40s - loss: 0.1808 - acc: 0.9349 - val_loss: 1.1873 - val_acc: 0.7161
Epoch 22/500
 - 40s - loss: 0.1307 - acc: 0.9551 - val_loss: 1.1892 - val_acc: 0.7214
Epoch 23/500
 - 40s - loss: 0.1537 - acc: 0.9464 - val_loss: 1.1626 - val_acc: 0.7125
Epoch 24/500
 - 40s - loss: 0.1098 - acc: 0.9634 - val_loss: 1.2292 - val_acc: 0.7196
Epoch 00024: early stopping
Random seed for replication: 254
Epoch 1/500
 - 39s - loss: 1.5861 - acc: 0.2335 - val_loss: 1.5276 - val_acc: 0.3054
Epoch 2/500
 - 39s - loss: 1.4469 - acc: 0.3519 - val_loss: 1.4308 - val_acc: 0.3875
Epoch 3/500
 - 42s - loss: 1.3931 - acc: 0.3670 - val_loss: 1.3698 - val_acc: 0.4375
Epoch 4/500
 - 43s - loss: 1.3464 - acc: 0.4047 - val_loss: 1.3620 - val_acc: 0.4107
Epoch 5/500
 - 42s - loss: 1.3074 - acc: 0.4380 - val_loss: 1.2944 - val_acc: 0.4821
Epoch 6/500
 - 42s - loss: 1.2647 - acc: 0.4613 - val_loss: 1.2446 - val_acc: 0.4946
Epoch 7/500
 - 42s - loss: 1.2177 - acc: 0.4826 - val_loss: 1.2827 - val_acc: 0.4536
Epoch 8/500
 - 41s - loss: 1.1642 - acc: 0.5153 - val_loss: 1.1768 - val_acc: 0.5036
Epoch 9/500
 - 41s - loss: 1.0885 - acc: 0.5542 - val_loss: 1.1501 - val_acc: 0.5339
Epoch 10/500
 - 41s - loss: 1.0037 - acc: 0.5927 - val_loss: 1.0767 - val_acc: 0.5661
Epoch 11/500
 - 41s - loss: 0.9341 - acc: 0.6278 - val_loss: 1.0475 - val_acc: 0.5571
Epoch 12/500
 - 41s - loss: 0.8233 - acc: 0.6744 - val_loss: 1.0097 - val_acc: 0.6286
Epoch 13/500
 - 41s - loss: 0.7249 - acc: 0.7107 - val_loss: 0.9698 - val_acc: 0.6107
Epoch 14/500
 - 41s - loss: 0.6304 - acc: 0.7541 - val_loss: 0.9220 - val_acc: 0.6696
Epoch 15/500
 - 41s - loss: 0.5209 - acc: 0.8023 - val_loss: 0.9958 - val_acc: 0.6393
Epoch 16/500
 - 40s - loss: 0.4200 - acc: 0.8421 - val_loss: 0.7863 - val_acc: 0.7107
Epoch 17/500
 - 40s - loss: 0.3361 - acc: 0.8761 - val_loss: 0.8876 - val_acc: 0.7036
Epoch 18/500
 - 40s - loss: 0.2743 - acc: 0.9024 - val_loss: 0.9448 - val_acc: 0.7018
Epoch 19/500
 - 40s - loss: 0.2439 - acc: 0.9110 - val_loss: 0.9228 - val_acc: 0.7107
Epoch 20/500
 - 40s - loss: 0.1757 - acc: 0.9391 - val_loss: 1.0457 - val_acc: 0.6929
Epoch 21/500
 - 40s - loss: 0.1624 - acc: 0.9446 - val_loss: 0.9433 - val_acc: 0.7554
Epoch 22/500
 - 40s - loss: 0.1285 - acc: 0.9579 - val_loss: 1.0611 - val_acc: 0.7536
Epoch 23/500
 - 40s - loss: 0.1096 - acc: 0.9658 - val_loss: 1.1332 - val_acc: 0.7464
Epoch 24/500
 - 40s - loss: 0.1397 - acc: 0.9562 - val_loss: 1.1206 - val_acc: 0.7268
Epoch 25/500
 - 40s - loss: 0.0855 - acc: 0.9741 - val_loss: 1.2190 - val_acc: 0.7339
Epoch 26/500
 - 40s - loss: 0.0834 - acc: 0.9765 - val_loss: 1.1152 - val_acc: 0.7464
/software/software/h5py/2.7.1-fosscuda-2018a-Python-3.6.4/lib/python3.6/site-packages/h5py-2.7.1-py3.6-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Epoch 00026: early stopping
Highest acc of model in ensemble: 71.3%
Mean ensemble accuracy: 71.3%
Saved mcdo_model to disk
Saved mcdo_model to disk
Saved mcdo_model to disk
Saved mcdo_model to disk
Saved mcdo_model to disk
[[127  77  19   7   4]
 [ 16 188  42  11  18]
 [ 13  72 141  34  12]
 [  2  24  22 246   8]
 [  1   8   3   8 296]]
posterior mean: 2
true label: 2

class: 0; proba: 0.0%; var: 0.00% 
class: 1; proba: 0.0%; var: 0.00% 
class: 2; proba: 100.0%; var: 0.00% 
class: 3; proba: 0.0%; var: 0.00% 
class: 4; proba: 0.0%; var: 0.00% 


###############################################################################
Peregrine Cluster
Job 9526506 for user 's2934833'
Finished at: Wed Feb 12 10:55:52 CET 2020

Job details:
============

Name                : ensemble
User                : s2934833
Partition           : gpu
Nodes               : pg-gpu25
Cores               : 12
State               : COMPLETED
Submit              : 2020-02-12T09:30:17
Start               : 2020-02-12T09:30:42
End                 : 2020-02-12T10:55:52
Reserved walltime   : 10:00:00
Used walltime       : 01:25:10
Used CPU time       : 01:11:21 (efficiency:  6.98%)
% User (Computation): 69.99%
% System (I/O)      : 30.02%
Mem reserved        : 62.50G/node
Max Mem used        : 15.43G (pg-gpu25)
Max Disk Write      : 133.12K (pg-gpu25)
Max Disk Read       : 4.22M (pg-gpu25)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
