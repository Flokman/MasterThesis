
The following have been reloaded with a version change:
  1) FFTW/3.3.7-gompic-2018a => FFTW/3.3.7-gompi-2018a
  2) OpenMPI/2.1.2-gcccuda-2018a => OpenMPI/2.1.2-GCC-6.4.0-2.28
  3) Python/3.6.4-fosscuda-2018a => Python/3.6.4-foss-2018a
  4) ScaLAPACK/2.0.2-gompic-2018a-OpenBLAS-0.2.20 => ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20

2020-02-12 11:38:35.017005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-02-12 11:38:35.017517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GRID V100D-32Q major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:02:00.0
totalMemory: 31.87GiB freeMemory: 29.56GiB
2020-02-12 11:38:35.017564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-02-12 11:38:35.538758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-12 11:38:35.538846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-02-12 11:38:35.538864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-02-12 11:38:35.538975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 28669 MB memory) -> physical GPU (device: 0, name: GRID V100D-32Q, pci bus id: 0000:02:00.0, compute capability: 7.0)
2020-02-12 11:38:35.540267: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Random seed for replication: 948
dataset_name = /Messidor2_PNG_AUG_256.hdf5, batch_size = 32, num_classes = 5, epochs = 500,
          test_img_idx = 1293, train_test_split = 0.8, to_shuffle = True,
          augmentation = False, label_count = [1261, 1416, 1397, 1455, 1463], label_normalizer = True,
          save_augmentation_to_hdf5 = True, learn rate = 1e-05, train_all_layers = True,
          weights_to_use = None, es_patience = 30, train_val_split = 0.9,
          N_ENSEMBLE_MEMBERS = 20
x_train shape: (5593, 256, 256, 3)
5593 train samples
1399 test samples
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 256, 256, 3)       0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 32768)             0         
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              134221824 
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312  
_________________________________________________________________
predictions (Dense)          (None, 5)                 20485     
=================================================================
Total params: 165,738,309
Trainable params: 165,738,309
Non-trainable params: 0
_________________________________________________________________
Start fitting monte carlo dropout model
Random seed for replication: 923
Epoch 1/500
 - 46s - loss: 1.5824 - acc: 0.2346 - val_loss: 1.4818 - val_acc: 0.3500
Epoch 2/500
 - 39s - loss: 1.4449 - acc: 0.3408 - val_loss: 1.4468 - val_acc: 0.3411
Epoch 3/500
 - 42s - loss: 1.4004 - acc: 0.3734 - val_loss: 1.3518 - val_acc: 0.3714
Epoch 4/500
 - 43s - loss: 1.3483 - acc: 0.4086 - val_loss: 1.3452 - val_acc: 0.3964
Epoch 5/500
 - 42s - loss: 1.3162 - acc: 0.4331 - val_loss: 1.2745 - val_acc: 0.4321
Epoch 6/500
 - 41s - loss: 1.2690 - acc: 0.4572 - val_loss: 1.2598 - val_acc: 0.4357
Epoch 7/500
 - 41s - loss: 1.2091 - acc: 0.4964 - val_loss: 1.1804 - val_acc: 0.4964
Epoch 8/500
 - 41s - loss: 1.1460 - acc: 0.5334 - val_loss: 1.1693 - val_acc: 0.5375
Epoch 9/500
 - 41s - loss: 1.0785 - acc: 0.5526 - val_loss: 1.0984 - val_acc: 0.5732
Epoch 10/500
 - 41s - loss: 0.9964 - acc: 0.5970 - val_loss: 1.0308 - val_acc: 0.5821
Epoch 11/500
 - 41s - loss: 0.9192 - acc: 0.6281 - val_loss: 0.9699 - val_acc: 0.6268
Epoch 12/500
 - 41s - loss: 0.8090 - acc: 0.6796 - val_loss: 1.0461 - val_acc: 0.5804
Epoch 13/500
 - 41s - loss: 0.7236 - acc: 0.7059 - val_loss: 0.9291 - val_acc: 0.6661
Epoch 14/500
 - 41s - loss: 0.6293 - acc: 0.7531 - val_loss: 0.8913 - val_acc: 0.6714
Epoch 15/500
 - 41s - loss: 0.5328 - acc: 0.7919 - val_loss: 0.8483 - val_acc: 0.6911
Epoch 16/500
 - 41s - loss: 0.4319 - acc: 0.8384 - val_loss: 0.8686 - val_acc: 0.6982
Epoch 17/500
 - 40s - loss: 0.3784 - acc: 0.8574 - val_loss: 0.9532 - val_acc: 0.6911
Epoch 18/500
 - 40s - loss: 0.2903 - acc: 0.8897 - val_loss: 0.9837 - val_acc: 0.6929
Epoch 19/500
 - 40s - loss: 0.2369 - acc: 0.9131 - val_loss: 0.9509 - val_acc: 0.7375
Epoch 20/500
 - 40s - loss: 0.1840 - acc: 0.9351 - val_loss: 1.1590 - val_acc: 0.7000
Epoch 21/500
 - 40s - loss: 0.1967 - acc: 0.9300 - val_loss: 0.9096 - val_acc: 0.7232
Epoch 22/500
 - 40s - loss: 0.1436 - acc: 0.9563 - val_loss: 1.3015 - val_acc: 0.7089
Epoch 23/500
 - 40s - loss: 0.1762 - acc: 0.9415 - val_loss: 1.0620 - val_acc: 0.7393
Epoch 24/500
 - 40s - loss: 0.1186 - acc: 0.9622 - val_loss: 1.1198 - val_acc: 0.7393
Epoch 25/500
 - 41s - loss: 0.0984 - acc: 0.9709 - val_loss: 1.1995 - val_acc: 0.7250
Epoch 26/500
 - 41s - loss: 0.1312 - acc: 0.9547 - val_loss: 1.4083 - val_acc: 0.7000
Epoch 27/500
 - 41s - loss: 0.0830 - acc: 0.9747 - val_loss: 1.4215 - val_acc: 0.7125
Epoch 28/500
 - 40s - loss: 0.1028 - acc: 0.9701 - val_loss: 1.1931 - val_acc: 0.7429
Epoch 29/500
 - 41s - loss: 0.0594 - acc: 0.9826 - val_loss: 1.2333 - val_acc: 0.7500
Epoch 30/500
 - 40s - loss: 0.0661 - acc: 0.9792 - val_loss: 1.4151 - val_acc: 0.7286
Epoch 31/500
 - 40s - loss: 0.0619 - acc: 0.9836 - val_loss: 1.1849 - val_acc: 0.7571
Epoch 32/500
 - 40s - loss: 0.0521 - acc: 0.9852 - val_loss: 1.1454 - val_acc: 0.7643
Epoch 33/500
 - 40s - loss: 0.0557 - acc: 0.9860 - val_loss: 1.3694 - val_acc: 0.7500
Epoch 34/500
 - 40s - loss: 0.0519 - acc: 0.9866 - val_loss: 1.1907 - val_acc: 0.7464
Epoch 35/500
 - 41s - loss: 0.0506 - acc: 0.9846 - val_loss: 1.3328 - val_acc: 0.7393
Epoch 36/500
 - 40s - loss: 0.0745 - acc: 0.9784 - val_loss: 1.2408 - val_acc: 0.7375
Epoch 37/500
 - 40s - loss: 0.0740 - acc: 0.9794 - val_loss: 1.3756 - val_acc: 0.7196
Epoch 38/500
 - 41s - loss: 0.0817 - acc: 0.9741 - val_loss: 1.2092 - val_acc: 0.7571
Epoch 39/500
 - 41s - loss: 0.0536 - acc: 0.9854 - val_loss: 1.3582 - val_acc: 0.7232
Epoch 40/500
 - 41s - loss: 0.0761 - acc: 0.9776 - val_loss: 1.2969 - val_acc: 0.7393
Epoch 41/500
 - 41s - loss: 0.0448 - acc: 0.9854 - val_loss: 1.1949 - val_acc: 0.7696
Epoch 42/500
 - 41s - loss: 0.0412 - acc: 0.9877 - val_loss: 1.2703 - val_acc: 0.7625
Epoch 43/500
 - 41s - loss: 0.0320 - acc: 0.9913 - val_loss: 1.2222 - val_acc: 0.7571
Epoch 44/500
 - 41s - loss: 0.0344 - acc: 0.9903 - val_loss: 1.1104 - val_acc: 0.7643
Epoch 45/500
 - 40s - loss: 0.0309 - acc: 0.9917 - val_loss: 1.1437 - val_acc: 0.7643
Epoch 00045: early stopping
Saved ensemble_model to disk
Random seed for replication: 975
Epoch 1/500
 - 39s - loss: 1.5917 - acc: 0.2230 - val_loss: 1.5369 - val_acc: 0.2661
Epoch 2/500
 - 40s - loss: 1.4469 - acc: 0.3443 - val_loss: 1.4523 - val_acc: 0.3071
Epoch 3/500
 - 42s - loss: 1.4029 - acc: 0.3611 - val_loss: 1.3972 - val_acc: 0.2982
Epoch 4/500
 - 42s - loss: 1.3502 - acc: 0.4064 - val_loss: 1.3777 - val_acc: 0.3839
Epoch 5/500
 - 42s - loss: 1.3088 - acc: 0.4371 - val_loss: 1.3243 - val_acc: 0.4571
Epoch 6/500
 - 41s - loss: 1.2652 - acc: 0.4555 - val_loss: 1.2807 - val_acc: 0.4589
Epoch 7/500
 - 41s - loss: 1.2060 - acc: 0.4862 - val_loss: 1.2672 - val_acc: 0.4625
Epoch 8/500
 - 42s - loss: 1.1523 - acc: 0.5193 - val_loss: 1.2066 - val_acc: 0.5018
Epoch 9/500
 - 41s - loss: 1.0968 - acc: 0.5454 - val_loss: 1.2046 - val_acc: 0.4946
Epoch 10/500
 - 41s - loss: 1.0200 - acc: 0.5861 - val_loss: 1.1184 - val_acc: 0.5518
Epoch 11/500
 - 41s - loss: 0.9324 - acc: 0.6288 - val_loss: 1.1207 - val_acc: 0.5339
Epoch 12/500
 - 41s - loss: 0.8457 - acc: 0.6648 - val_loss: 1.1081 - val_acc: 0.5643
Epoch 13/500
 - 41s - loss: 0.7362 - acc: 0.7147 - val_loss: 1.0508 - val_acc: 0.5982
Epoch 14/500
 - 40s - loss: 0.6513 - acc: 0.7419 - val_loss: 1.0092 - val_acc: 0.6268
Epoch 15/500
 - 41s - loss: 0.5432 - acc: 0.7926 - val_loss: 1.0677 - val_acc: 0.6339
Epoch 16/500
 - 40s - loss: 0.4548 - acc: 0.8328 - val_loss: 1.0247 - val_acc: 0.6625
Epoch 17/500
 - 41s - loss: 0.3532 - acc: 0.8694 - val_loss: 1.1596 - val_acc: 0.6393
Epoch 18/500
 - 41s - loss: 0.2983 - acc: 0.8964 - val_loss: 1.1728 - val_acc: 0.6679
Epoch 19/500
 - 40s - loss: 0.2292 - acc: 0.9189 - val_loss: 1.0567 - val_acc: 0.6982
Epoch 20/500
 - 40s - loss: 0.1954 - acc: 0.9298 - val_loss: 1.2818 - val_acc: 0.7036
Epoch 21/500
 - 41s - loss: 0.2316 - acc: 0.9282 - val_loss: 1.1900 - val_acc: 0.7036
Epoch 22/500
 - 40s - loss: 0.1527 - acc: 0.9562 - val_loss: 1.3345 - val_acc: 0.6911
Epoch 23/500
 - 40s - loss: 0.1195 - acc: 0.9636 - val_loss: 1.3484 - val_acc: 0.6786
Epoch 24/500
 - 40s - loss: 0.0937 - acc: 0.9739 - val_loss: 1.4694 - val_acc: 0.6982
Epoch 25/500
 - 40s - loss: 0.1564 - acc: 0.9504 - val_loss: 1.3936 - val_acc: 0.6804
Epoch 26/500
 - 40s - loss: 0.1108 - acc: 0.9650 - val_loss: 1.2539 - val_acc: 0.7304
Epoch 27/500
 - 40s - loss: 0.0871 - acc: 0.9753 - val_loss: 1.4593 - val_acc: 0.7054
Epoch 28/500
 - 41s - loss: 0.0799 - acc: 0.9761 - val_loss: 1.4061 - val_acc: 0.7107
Epoch 29/500
 - 40s - loss: 0.1006 - acc: 0.9699 - val_loss: 1.5543 - val_acc: 0.7000
Epoch 30/500
 - 40s - loss: 0.0634 - acc: 0.9806 - val_loss: 1.5560 - val_acc: 0.7071
Epoch 31/500
 - 40s - loss: 0.0737 - acc: 0.9798 - val_loss: 1.4948 - val_acc: 0.7232
Epoch 32/500
 - 40s - loss: 0.0504 - acc: 0.9829 - val_loss: 1.6481 - val_acc: 0.7304
Epoch 33/500
 - 40s - loss: 0.0609 - acc: 0.9814 - val_loss: 1.4752 - val_acc: 0.7214
Epoch 34/500
 - 40s - loss: 0.0508 - acc: 0.9873 - val_loss: 1.5331 - val_acc: 0.7286
Epoch 35/500
 - 40s - loss: 0.0489 - acc: 0.9860 - val_loss: 1.5127 - val_acc: 0.7232
Epoch 36/500
 - 40s - loss: 0.0365 - acc: 0.9899 - val_loss: 1.3807 - val_acc: 0.7161
Epoch 37/500
 - 40s - loss: 0.0319 - acc: 0.9911 - val_loss: 1.7035 - val_acc: 0.6768
Epoch 38/500
 - 40s - loss: 0.0330 - acc: 0.9911 - val_loss: 1.5312 - val_acc: 0.7321
Epoch 39/500
 - 40s - loss: 0.0351 - acc: 0.9907 - val_loss: 1.5126 - val_acc: 0.7250
Epoch 40/500
 - 40s - loss: 0.1227 - acc: 0.9647 - val_loss: 1.4424 - val_acc: 0.6804
Epoch 41/500
 - 40s - loss: 0.0840 - acc: 0.9762 - val_loss: 1.3769 - val_acc: 0.7071
Epoch 42/500
 - 40s - loss: 0.0438 - acc: 0.9885 - val_loss: 1.3542 - val_acc: 0.7357
Epoch 43/500
 - 41s - loss: 0.0348 - acc: 0.9913 - val_loss: 1.2541 - val_acc: 0.7536
Epoch 44/500
 - 41s - loss: 0.0295 - acc: 0.9909 - val_loss: 1.3691 - val_acc: 0.7554
Epoch 00044: early stopping
Saved ensemble_model to disk
Random seed for replication: 795
Epoch 1/500
 - 39s - loss: 1.5857 - acc: 0.2286 - val_loss: 1.5103 - val_acc: 0.2893
Epoch 2/500
 - 39s - loss: 1.4594 - acc: 0.3330 - val_loss: 1.4668 - val_acc: 0.3143
Epoch 3/500
 - 42s - loss: 1.4206 - acc: 0.3565 - val_loss: 1.4189 - val_acc: 0.3661
Epoch 4/500
 - 42s - loss: 1.3976 - acc: 0.3755 - val_loss: 1.4368 - val_acc: 0.3518
Epoch 5/500
 - 41s - loss: 1.3563 - acc: 0.4118 - val_loss: 1.3637 - val_acc: 0.3929
Epoch 6/500
 - 41s - loss: 1.3119 - acc: 0.4324 - val_loss: 1.3293 - val_acc: 0.4196
Epoch 7/500
 - 41s - loss: 1.2712 - acc: 0.4575 - val_loss: 1.2792 - val_acc: 0.4464
Epoch 8/500
 - 41s - loss: 1.2009 - acc: 0.4957 - val_loss: 1.2157 - val_acc: 0.4964
Epoch 9/500
 - 41s - loss: 1.1575 - acc: 0.5133 - val_loss: 1.1881 - val_acc: 0.4982
Epoch 10/500
 - 41s - loss: 1.1004 - acc: 0.5405 - val_loss: 1.1528 - val_acc: 0.5232
Epoch 11/500
 - 41s - loss: 1.0076 - acc: 0.5876 - val_loss: 1.0694 - val_acc: 0.5321
Epoch 12/500
 - 41s - loss: 0.9342 - acc: 0.6248 - val_loss: 1.0450 - val_acc: 0.5768
Epoch 13/500
 - 41s - loss: 0.8181 - acc: 0.6786 - val_loss: 0.9395 - val_acc: 0.6286
Epoch 14/500
 - 40s - loss: 0.7127 - acc: 0.7214 - val_loss: 0.8860 - val_acc: 0.6482
Epoch 15/500
 - 40s - loss: 0.6021 - acc: 0.7681 - val_loss: 0.8338 - val_acc: 0.6786
Epoch 16/500
 - 40s - loss: 0.5069 - acc: 0.8053 - val_loss: 0.8330 - val_acc: 0.6714
Epoch 17/500
 - 40s - loss: 0.4222 - acc: 0.8418 - val_loss: 0.8000 - val_acc: 0.7107
Epoch 18/500
 - 40s - loss: 0.3254 - acc: 0.8817 - val_loss: 0.8684 - val_acc: 0.7196
Epoch 19/500
 - 41s - loss: 0.2757 - acc: 0.8995 - val_loss: 0.8456 - val_acc: 0.7196
Epoch 20/500
 - 40s - loss: 0.2155 - acc: 0.9216 - val_loss: 0.9311 - val_acc: 0.7018
Epoch 21/500
 - 40s - loss: 0.2046 - acc: 0.9315 - val_loss: 0.8456 - val_acc: 0.7536
Epoch 22/500
 - 40s - loss: 0.1560 - acc: 0.9449 - val_loss: 0.9782 - val_acc: 0.7429
Epoch 23/500
 - 40s - loss: 0.1220 - acc: 0.9595 - val_loss: 0.9537 - val_acc: 0.7643
Epoch 24/500
 - 40s - loss: 0.0974 - acc: 0.9693 - val_loss: 0.9708 - val_acc: 0.7679
Epoch 25/500
 - 40s - loss: 0.0829 - acc: 0.9747 - val_loss: 1.0555 - val_acc: 0.7554
Epoch 26/500
 - 40s - loss: 0.1294 - acc: 0.9601 - val_loss: 1.0290 - val_acc: 0.7429
Epoch 27/500
 - 40s - loss: 0.1521 - acc: 0.9540 - val_loss: 0.9357 - val_acc: 0.7607
Epoch 28/500
 - 40s - loss: 0.0667 - acc: 0.9836 - val_loss: 1.0856 - val_acc: 0.7464
Epoch 29/500
 - 41s - loss: 0.0765 - acc: 0.9780 - val_loss: 1.0381 - val_acc: 0.7589
Epoch 30/500
 - 41s - loss: 0.0677 - acc: 0.9780 - val_loss: 1.1652 - val_acc: 0.7571
Epoch 31/500
 - 40s - loss: 0.0538 - acc: 0.9858 - val_loss: 0.9608 - val_acc: 0.7714
Epoch 32/500
 - 40s - loss: 0.0538 - acc: 0.9866 - val_loss: 1.0602 - val_acc: 0.7536
Epoch 33/500
 - 40s - loss: 0.0796 - acc: 0.9780 - val_loss: 1.0342 - val_acc: 0.7500
Epoch 34/500
 - 40s - loss: 0.0616 - acc: 0.9844 - val_loss: 0.8662 - val_acc: 0.7732
Epoch 35/500
 - 40s - loss: 0.0474 - acc: 0.9875 - val_loss: 0.9209 - val_acc: 0.7821
Epoch 36/500
 - 40s - loss: 0.0368 - acc: 0.9917 - val_loss: 0.9870 - val_acc: 0.7679
Epoch 37/500
 - 40s - loss: 0.0445 - acc: 0.9875 - val_loss: 1.1342 - val_acc: 0.7464
Epoch 38/500
 - 40s - loss: 0.0613 - acc: 0.9800 - val_loss: 1.0138 - val_acc: 0.7571
Epoch 39/500
 - 40s - loss: 0.0665 - acc: 0.9802 - val_loss: 1.0206 - val_acc: 0.7732
Epoch 40/500
 - 40s - loss: 0.0539 - acc: 0.9828 - val_loss: 1.0728 - val_acc: 0.7857
Epoch 41/500
 - 40s - loss: 0.0524 - acc: 0.9836 - val_loss: 1.0276 - val_acc: 0.7911
Epoch 42/500
 - 40s - loss: 0.0470 - acc: 0.9858 - val_loss: 0.9360 - val_acc: 0.7554
Epoch 43/500
 - 40s - loss: 0.0369 - acc: 0.9893 - val_loss: 0.9673 - val_acc: 0.7929
Epoch 44/500
 - 40s - loss: 0.0356 - acc: 0.9907 - val_loss: 1.0007 - val_acc: 0.7768
Epoch 45/500
 - 40s - loss: 0.0305 - acc: 0.9921 - val_loss: 0.9054 - val_acc: 0.7786
Epoch 46/500
 - 40s - loss: 0.0319 - acc: 0.9909 - val_loss: 0.8973 - val_acc: 0.7839
Epoch 47/500
 - 40s - loss: 0.0256 - acc: 0.9925 - val_loss: 0.9057 - val_acc: 0.7857


###############################################################################
Peregrine Cluster
Job 9528784 for user 's2934833'
Finished at: Wed Feb 12 13:11:12 CET 2020

Job details:
============

Name                : ensemble
User                : s2934833
Partition           : gpu
Nodes               : pg-gpu11
Cores               : 12
State               : FAILED
Submit              : 2020-02-12T11:37:51
Start               : 2020-02-12T11:38:23
End                 : 2020-02-12T13:11:11
Reserved walltime   : 10:00:00
Used walltime       : 01:32:48
Used CPU time       : 01:17:38 (efficiency:  6.97%)
% User (Computation): 70.64%
% System (I/O)      : 29.37%
Mem reserved        : 62.50G/node
Max Mem used        : 12.42G (pg-gpu11)
Max Disk Write      : 133.12K (pg-gpu11)
Max Disk Read       : 4.22M (pg-gpu11)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/additional_information/scientific_output

################################################################################
